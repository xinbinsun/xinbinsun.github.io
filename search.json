[{"title":"Compilation Principle","date":"2023-06-21T10:28:40.000Z","url":"/2023/06/21/Compilation-principle/","tags":[["Undergraduate Courses","/tags/Undergraduate-Courses/"]],"categories":[["Undergraduate Courses","/categories/Undergraduate-Courses/"]],"content":"编译原理 总复习2023春季学期 QingDao University 一、名词解释翻译器 能够完成一种语言到另一种语言的保语义变换的软件称为翻译器，这两种语言分别称为该翻译器的源语言和目标语言 句柄句型的句柄是该句型中和一个产生式右部匹配的子串，并且，把它归约成该产生式左部的非终结符代表了最右推导过程的逆过程的一步 L属性定义如果每个产生式 A-&gt;X~1~，X~2~…X~n~ 的每条语义规则计算的属性是 A 的综合 属性；或者计算的是 X~j~ 的继承属性，他仅依赖： 1：该产生式中 X~j~ 左边符号 X~1~，X~2~…X~j-1~ 的属性； 2: A 的继承属性； 良行为的程序一个程序的运行不可能引起不会被捕获错误的出现，那么称该程序是良行为的。 三地址代码三地址代码是一般形式为 x =y op z 的指令序列,其中 x，y和z是名字、常数或编译器产生的临时变量,op 代表算符； 二、简答题6~8章 三、词法分析子集构造、极小化状态、最简DFA，不能用死状态 四、语法分析自上而下/自下而上 考一个 LL(1) or LR 消除左递归、左因子 follow集、first集 预测分析表——&gt;不能作为终结符出现 五、语义分析与类型检查六、开放（自然语言处理/编译原理）请阐述编译原理和自然语言处理在词法分析方面的差异编译原理和自然语言处理在词法分析方面有一些差异。 编译原理中的词法分析是编译过程的第一个阶段，目的是将输入的源代码分解为词法单元（tokens），并生成词法单元流。词法单元是源代码中的基本单元，如关键字、标识符、运算符和常量等。编译器使用词法分析器（lexer）根据语言的词法规则，将源代码转换为词法单元流，以便后续的语法分析和语义分析。 而自然语言处理（NLP）中的词法分析也被称为分词（tokenization），其目标是将自然语言文本分解为语言单位，如单词、短语或句子等。NLP的词法分析涉及处理各种自然语言特定的问题，如处理复杂的语法结构、词性标注、词形还原和命名实体识别等。 所以，编译原理的词法分析主要关注于编程语言的词法规则和生成词法单元流，而NLP的词法分析更关注于处理自然语言文本的特定问题，例如分词和语言单位的识别。尽管两者都涉及词法分析的概念，但在实践中的重点和方法会有所不同。 请阐述编译原理和自然语言处理在语法分析方面的差异编译原理和自然语言处理在语法分析方面也存在一些差异。 在编译原理中，语法分析是编译过程的第二个阶段，其目标是根据语法规则验证和分析词法单元流的结构，生成抽象语法树（Abstract Syntax Tree，AST）。编译器使用语法分析器（parser）来解析词法单元流，并构建表示程序结构的抽象语法树。语法分析器根据编程语言的语法规则，检查输入的词法单元流是否符合语法要求，并转换为易于处理的中间表示形式。 而在自然语言处理中，语法分析也被称为句法分析（parsing），其目标是分析自然语言句子的结构和语法关系。语法分析器在自然语言处理中用于确定句子的短语结构、依存关系和语法角色等。这有助于理解句子的语法含义和上下文关系。 尽管编译原理和自然语言处理都涉及语法分析的概念，但在实践中的重点和方法存在差异。编译原理的语法分析主要关注于编程语言的形式化语法规则和语法树的生成，而自然语言处理的语法分析更关注于自然语言的句法结构、依存关系和语法角色的识别。因此，两者在语法分析的目标和处理方法上有所不同。 请阐述编译原理和自然语言处理在语义分析与类型检查方面的差异编译原理和自然语言处理在语义分析与类型检查方面也存在一些差异。 在编译原理中，语义分析是编译过程的一个重要阶段，其目标是对源代码进行语义验证和语义处理。语义分析器会对抽象语法树进行遍历，检查变量的声明和使用是否合法，执行类型推导和类型检查，处理作用域和命名空间等。语义分析的目的是确保源代码在语义上是正确的，并生成中间代码或优化代码。 而在自然语言处理中，语义分析的目标是理解自然语言文本的语义含义和推理。语义分析器在NLP中用于解析句子的含义、上下文关联和语义角色标注等任务。这包括识别句子中的命名实体、消歧义歧义词义、执行逻辑推理和情感分析等。语义分析在NLP中有助于将自然语言转化为机器可理解的表示形式。 类型检查是编译原理中常见的任务之一，它涉及验证变量和表达式的类型是否匹配，并检测类型错误。编译器会检查类型的一致性、隐式类型转换和类型转换的合法性等。类型检查有助于捕获程序中的类型错误和提供类型安全性。 在自然语言处理中，类型检查的概念并不像编译原理中那样明确存在。自然语言处理更注重于对文本的语义理解和推理，而不像编译原理中那样明确涉及静态类型系统和类型检查。 因此，编译原理和自然语言处理在语义分析与类型检查方面存在差异，主要体现在目标和处理方法的不同。编译原理注重于程序语义的验证和类型检查，而自然语言处理注重于自然语言文本的语义解析和推理。"},{"title":"Distributed System","date":"2023-06-18T15:30:30.000Z","url":"/2023/06/18/distributed-system/","tags":[["Undergraduate Courses","/tags/Undergraduate-Courses/"]],"categories":[["Undergraduate Courses","/categories/Undergraduate-Courses/"]],"content":"分布式系统 总复习2023春季学期 QingDao University 总复习概述和体系结构模块分布式计算中透明性的含义在分布式计算中，透明性是指隐藏系统内部的复杂性，使得分布式系统对用户和应用程序表现为单一的、统一的实体，而不暴露其分布性和并发性。透明性是为了简化分布式系统的设计、开发和使用，使得用户和应用程序可以像使用单个计算资源一样使用分布式系统，而无需了解其内部的复杂性。 访问透明性（Access Transparency）：用户和应用程序可以透明地访问分布式系统中的资源，无需关心资源的物理位置和访问细节。这使得用户可以像访问本地资源一样访问远程资源。 位置透明性（Location Transparency）：用户和应用程序可以透明地访问分布式系统中的资源，而不需要了解资源的具体物理位置。系统会根据资源的可用性和负载情况进行适当的资源定位和路由。 迁移透明性（Migration Transparency）：系统可以在不中断用户和应用程序的情况下迁移资源或重新配置资源。用户和应用程序无需感知资源的迁移和重新配置过程。 并发透明性（Concurrency Transparency）：系统可以透明地处理并发操作，即使多个用户或应用程序同时访问和修改资源，也不会发生冲突或数据损坏。 故障透明性（Failure Transparency）：系统可以透明地处理组件故障或网络故障。用户和应用程序无需感知故障的发生，并且系统能够自动处理故障，并保证服务的可用性和可靠性。 分布式体系结构有哪些？ 客户端-服务器体系结构（Client-Server Architecture）：这是最常见的分布式体系结构之一，其中客户端应用程序通过网络连接到服务器，并向服务器发送请求以获取服务或数据。服务器接收请求并提供相应的服务或数据。这种体系结构可以实现资源集中管理和共享，同时提供高可用性和可扩展性。 对等（Peer-to-Peer）体系结构：在对等体系结构中，节点之间没有明确的客户端和服务器角色区分，所有节点都可以充当客户端和服务器。节点之间通过直接通信进行交互和资源共享，形成一个去中心化的网络。对等体系结构通常用于文件共享、点对点通信和分布式计算等场景。 三层体系结构（Three-Tier Architecture）：该体系结构将应用程序分为三个层次：客户端界面层、应用程序逻辑层和数据存储层。客户端界面层处理用户与系统的交互，应用程序逻辑层处理业务逻辑和请求处理，数据存储层负责数据的存储和访问。每个层次可以运行在不同的物理或逻辑节点上，实现分布式的功能和负载均衡。 微服务体系结构（Microservices Architecture）：微服务体系结构是一种将应用程序拆分为多个独立的小型服务的架构风格。每个微服务都是独立部署、可独立扩展和管理的，通过轻量级的通信机制进行交互。微服务体系结构可以提高开发速度、灵活性和可伸缩性，同时允许不同的团队独立开发和维护各自的服务。 区分软件体系结构和系统体系结构在分布式系统中，常见的逻辑体系结构包括以下几种： 客户端-服务器（Client-Server）体系结构：这是最常见的分布式系统体系结构，其中客户端和服务器之间进行通信和交互。客户端发送请求，服务器提供相应的服务并返回结果。 对等网络（Peer-to-Peer）体系结构：在对等网络中，系统中的节点可以相互通信和交互，没有明确的客户端和服务器之分。每个节点既可以提供服务，也可以请求服务。节点之间的通信是对等的，数据和功能可以在节点之间共享和分发。 发布-订阅（Publish-Subscribe）体系结构：在发布-订阅体系结构中，消息的发布者（发布者）将消息发布到一个或多个主题（或频道），而订阅者（订阅者）可以选择订阅感兴趣的主题并接收相应的消息。这种体系结构支持一对多的通信模式。 分层体系结构：分层体系结构将系统划分为多个层级，每个层级负责特定的功能和任务。上层的组件可以通过下层的组件提供的接口和服务进行通信和交互。这种体系结构简化了系统的设计和维护，提高了系统的可扩展性和可重用性。 微服务体系结构：微服务体系结构将系统划分为一组小型、自治的服务单元，每个服务单元专注于特定的业务功能。这些服务单元可以独立部署、扩展和管理，通过轻量级的通信机制进行交互。微服务体系结构支持系统的解耦和灵活性。 系统体系结构是指整个计算系统的结构和组织方式，包括硬件、软件和各种外部组件。下面是几种常见的系统体系结构： 单层体系结构（Single-Tier Architecture）：也称为单一体系结构或独立体系结构，系统的所有组件都部署在同一层级上。这种体系结构适用于简单的应用程序，其中用户界面、业务逻辑和数据存储等功能集中在一个单一的环境中。 客户端-服务器体系结构（Client-Server Architecture）：系统被分为客户端和服务器两个部分，客户端负责发送请求和接收响应，服务器负责处理请求并提供相应的服务。客户端和服务器之间通过网络进行通信，可以是两层、三层或多层的体系结构。 分布式体系结构（Distributed Architecture）：系统的组件部署在多个计算机节点上，这些节点通过网络进行通信和协作。分布式体系结构支持并行处理、资源共享和容错性，并允许系统在不同地理位置部署。 多层体系结构（Multilayer Architecture）：系统被分为多个层级，每个层级负责特定的功能和任务。常见的多层体系结构包括三层体系结构（表示层、业务逻辑层和数据访问层）和四层体系结构（表示层、业务逻辑层、应用层和数据访问层）等。多层体系结构提高了系统的可维护性、可扩展性和灵活性。 云体系结构（Cloud Architecture）：系统基于云计算平台构建，利用云服务提供的资源和功能。云体系结构可以包括公有云、私有云或混合云，以满足不同的需求和隐私要求。 C/S模式、P2P模式、边界服务器系统等实例的体系结构P2P（Peer-to-Peer）是一种分布式网络架构，其中计算机节点之间具有对等的地位，可以相互通信和协作，而无需集中的中央服务器。以下是从不同方面介绍P2P的内容： 架构特点： 对等通信：P2P网络中的节点相互平等，可以充当客户端和服务器角色，可以请求服务和提供服务。 分散化：P2P网络中没有中央控制节点，系统的决策和管理分布在各个节点上，提高了系统的可扩展性和容错性。 自组织性：P2P网络具有自组织的能力，新节点可以加入网络并参与协作，网络的拓扑结构可以动态调整和优化。 通信方式： 直接通信：P2P节点可以直接与其他节点进行通信，通过互联网或局域网建立点对点的连接。 中继通信：在某些情况下，由于网络拓扑或防火墙等限制，节点之间无法直接通信，这时可以借助其他节点进行中继传输。 应用领域： 文件共享：P2P文件共享网络允许用户直接分享和下载文件，如BitTorrent等。 即时通信：P2P即时通信允许用户实时交流和消息传递，如Skype、Bitmessage等。 分布式计算：P2P分布式计算允许多个节点共同参与计算任务，如分布式哈希表、分布式存储等。 区块链技术：区块链是一种基于P2P网络的分布式账本技术，如比特币、以太坊等。 优点： 去中心化：P2P架构避免了单点故障和集中控制，提高了系统的可靠性和可用性。 可扩展性：P2P网络具有良好的可扩展性，可以容纳大量的节点并处理大规模的数据和请求。 共享资源：P2P网络允许节点共享资源和服务，提高了资源的利用率和效率。 挑战和解决方案： 网络拓扑：P2P网络的拓扑结构对性能和可用性有影响，需要采取合适的算法和协议来优化拓扑结构。 安全和隐私：P2P网络中的节点可能存在安全和隐私问题，需要采取身份验证、加密和访问控制等措施保护系统安全。 数据一致性：由于节点之间的分散和异步操作，P2P网络中的数据一致性是一个挑战，需要采取合适的一致性协议和算法。 C/S（Client/Server）是一种计算机体系结构，其中客户端和服务器之间存在一种分工合作的关系。以下是从不同方面介绍C/S的内容： 架构特点： 分工合作：C/S架构将系统功能分为客户端和服务器两部分，客户端负责用户界面和用户交互，服务器负责处理请求和提供服务。 中心化：C/S架构中存在一个中心服务器，负责协调和管理客户端的请求和资源。 高可靠性：C/S架构通过服务器集中处理和管理数据和业务逻辑，提高了系统的可靠性和稳定性。 通信方式： 请求-响应模式：客户端向服务器发送请求，服务器处理请求并返回响应给客户端。 双向通信：客户端和服务器之间可以进行双向通信，可以发送请求和接收服务器推送的数据。 应用领域： 客户端应用程序：C/S架构适用于各种客户端应用程序，如桌面应用、移动应用等。客户端应用程序通过与服务器通信获取数据和服务。 数据库管理系统：数据库管理系统（DBMS）采用C/S架构，客户端通过连接到服务器来管理和操作数据库。 优点： 可扩展性：C/S架构允许在需要时增加服务器的数量，以应对用户量增加和系统负载的增加。 灵活性：C/S架构可以根据不同的需求和应用场景进行定制开发，满足特定的功能和业务需求。 安全性：通过服务器集中管理数据和业务逻辑，可以实施安全控制措施，提高系统的安全性。 挑战和解决方案： 性能：C/S架构的性能受限于服务器的处理能力和网络带宽，需要优化服务器端的资源管理和通信机制，以提高系统的性能。 协议和通信：客户端和服务器之间的通信需要定义和实现特定的协议和接口，确保数据的正确传输和解释。 单点故障：由于C/S架构中存在中心服务器，一旦服务器出现故障，整个系统可能受到影响。为了避免单点故障，可以使用负载均衡和容错机制。 边界服务器系统是一种常见的计算机系统架构，它在网络中扮演着重要的角色。以下是从各方面介绍边界服务器系统的内容： 定义与作用： 边界服务器系统是位于网络边界的一组服务器，位于内部网络和外部网络之间。 边界服务器系统充当了内部网络和外部网络之间的中转站和保护屏障，提供了访问控制、安全防护和流量管理等功能。 功能与特点： 访问控制：边界服务器系统可以对外部网络请求进行认证和授权，限制访问内部网络的权限和范围。 安全防护：边界服务器系统通过防火墙、入侵检测和防护系统等，保护内部网络免受恶意攻击和未经授权的访问。 负载均衡：边界服务器系统可以分担内部服务器的负载，通过负载均衡算法将请求分发到多个内部服务器上，提高系统的性能和可用性。 缓存和加速：边界服务器系统可以缓存静态内容和常用数据，减少对内部服务器的访问压力，并提供快速响应给客户端。 日志记录与分析：边界服务器系统可以记录和分析网络流量、安全事件和访问日志，用于监控和审计网络活动。 部署方式： 反向代理：边界服务器可以充当反向代理服务器，将客户端请求转发到合适的内部服务器上，并将响应返回给客户端。 VPN网关：边界服务器可以作为虚拟专用网络（VPN）的网关，处理来自外部网络的VPN连接请求，将其转发到内部网络中的目标主机。 防火墙与安全网关：边界服务器可以配置为防火墙和安全网关，通过规则和策略过滤和控制网络流量，保护内部网络的安全。 优势与挑战： 优势：边界服务器系统提供了安全保护、访问控制和性能优化等功能，可以提升网络安全性、可用性和性能。 挑战：边界服务器系统的部署和管理需要一定的专业知识和配置，且需要定期维护和更新以应对新的安全威胁。 中间件的基本概念及其实现中间件是指位于应用程序和操作系统之间的软件层，用于协调和支持分布式系统中的不同组件和服务之间的通信和交互。它提供了一系列功能和服务，使得应用程序开发者可以更加方便地构建、部署和管理分布式应用。 中间件的基本概念包括以下几个方面： 抽象和封装：中间件屏蔽了底层系统和网络的细节，提供了高层次的抽象接口和功能，使得应用程序可以独立于具体的操作系统和网络环境。 通信和协议：中间件提供了通信机制和协议，使得分布式系统中的不同组件可以进行可靠的通信和数据交换。常见的通信方式包括消息传递、远程过程调用（RPC）、消息队列等。 分布式事务：中间件可以支持分布式事务的管理和协调，确保在分布式环境下多个操作的一致性和原子性。 容错和可靠性：中间件可以提供容错和故障恢复机制，例如集群、复制和备份等，确保系统在发生故障时能够继续运行并保持可靠性。 安全和权限管理：中间件提供了安全机制和权限管理，用于保护分布式系统中的数据和资源不受未经授权的访问和恶意攻击。 中间件的实现可以通过各种方式和技术来完成，包括以下几种常见的实现方式： 消息队列中间件：通过消息队列实现异步通信和解耦，常见的消息队列中间件包括RabbitMQ、Apache Kafka等。 远程过程调用中间件：通过远程过程调用实现分布式函数调用和方法调用，常见的远程过程调用中间件包括gRPC、Apache Dubbo等。 数据库中间件：通过数据库中间件实现数据的分片、缓存和路由等功能，常见的数据库中间件包括MySQL Proxy、MyBatis等。 服务总线中间件：通过服务总线实现服务的发布、订阅和路由，常见的服务总线中间件包括Apache ServiceMix、MuleSoft等。 分布式缓存中间件：通过分布式缓存实现数据的高速缓存和共享，常见的分布式缓存中间件包括Redis、Memcached等。 总之，中间件在分布式系统中发挥着重要的作用，通过提供统一的接口和功能，简化了分布式应用的开发和部署，提高了系统的可靠性、可扩展性和性能。 进程模块线程的实现方法线程的实现方法有多种，下面介绍几种常见的线程实现方式： 原生线程库：大多数编程语言和操作系统都提供了原生的线程库，可以使用这些库来创建和管理线程。例如，C++中的&lt;thread&gt;库、Java中的java.lang.Thread类以及Python中的threading模块都提供了创建和操作线程的接口。 线程池：线程池是一种管理和复用线程的机制。通过创建一个线程池，可以预先创建一组线程，并将任务提交给线程池来执行。线程池会自动管理线程的生命周期，包括线程的创建、复用和回收，从而避免了频繁创建和销毁线程的开销。许多编程语言和框架都提供了线程池的实现，例如Java中的ExecutorService接口。 协程：协程是一种轻量级的线程替代方案，通过协程可以实现并发执行和任务切换，而无需创建和管理多个线程。协程可以在执行过程中主动让出CPU控制权，并在适当的时候恢复执行，从而提高并发性能和资源利用率。许多编程语言都提供了协程的支持，例如Python的asyncio库和Go语言的协程（goroutine）。 异步编程：异步编程是一种基于事件驱动的编程模型，通过使用异步回调、事件循环或者Promise等机制，可以在单线程中处理多个任务的并发执行。异步编程适用于I/O密集型任务，可以避免阻塞和线程创建的开销。许多编程语言和框架都提供了异步编程的支持，例如JavaScript中的Promise和async/await，以及Python中的asyncio。 单线程进程与多线程进程的区别单线程进程和多线程进程是指在操作系统中运行的两种不同类型的进程，它们的区别主要体现在以下几个方面： 执行能力：单线程进程只能同时执行一个任务或指令，而多线程进程可以同时执行多个任务或指令。多线程进程可以将任务划分为多个线程，在多个线程之间共享进程的资源和状态，从而实现并发执行。 并发性：多线程进程可以提高并发性，即在同一时间内执行多个任务。通过将任务分配给不同的线程，可以利用多核处理器或多个CPU来实现并行处理，提高系统的响应性和效率。而单线程进程只能按照顺序执行任务，无法并行处理。 资源共享和同步：在多线程进程中，不同的线程可以共享进程的资源和内存空间，因此可以更方便地进行数据共享和通信。然而，多个线程同时访问共享资源可能会引发并发访问的问题，需要采取同步机制来确保数据的一致性和线程的安全性。而单线程进程不需要考虑资源共享和同步的问题，因为只有一个线程在执行。 编程复杂性：多线程编程相对于单线程编程来说更复杂一些。在多线程环境下，需要考虑线程间的同步、互斥和通信，以避免竞态条件、死锁和资源泄漏等并发编程问题。而单线程编程相对较简单，无需考虑并发性和线程间的交互。 总的来说，多线程进程在并发性和资源利用上具有优势，可以实现更高的系统性能和响应性。但同时也带来了并发编程的复杂性和需要注意的问题。而单线程进程相对简单，适用于一些简单的任务或者无需并发处理的场景。选择使用单线程还是多线程进程取决于具体的应用需求和性能要求。 虚拟机体系结构、虚拟化技术虚拟机体系结构是一种将物理计算机资源抽象为虚拟计算机的技术架构。它通过虚拟化技术，使得多个虚拟机可以在同一台物理计算机上同时运行，每个虚拟机都具有自己的操作系统和应用程序，彼此之间相互隔离。 虚拟化技术是实现虚拟机体系结构的核心技术，它主要包括以下几种类型： 完全虚拟化（Full Virtualization）：完全虚拟化技术通过在物理计算机上创建一个称为虚拟机监视器（Hypervisor）的软件层，来模拟硬件资源并管理虚拟机。虚拟机监视器负责将虚拟机对硬件资源的请求转发到物理计算机上，并确保不同虚拟机之间的隔离性。常见的完全虚拟化解决方案有VMware、KVM等。 部分虚拟化（Para-virtualization）：部分虚拟化技术通过修改客户操作系统内核，使其与虚拟化层进行协同工作，以提高性能和效率。客户操作系统需要被特别修改才能运行在部分虚拟化的环境中。常见的部分虚拟化解决方案有Xen等。 操作系统级虚拟化（OS-level Virtualization）：操作系统级虚拟化技术利用操作系统内核的功能来实现虚拟化，而无需使用虚拟机监视器。在操作系统级虚拟化中，主机操作系统允许多个独立的虚拟环境（容器或虚拟化实例）共享同一套操作系统内核，每个虚拟环境都被视为一个隔离的操作系统实例。常见的操作系统级虚拟化解决方案有Docker、LXC等。 虚拟机体系结构和虚拟化技术使得计算机资源能够更好地利用和管理，提供了更灵活、可扩展和高效的计算环境。它们被广泛应用于服务器虚拟化、云计算、测试和开发环境等领域，为用户提供了更多的选择和便利。 设计服务器需要注意的问题，及服务器集群组织方式在设计服务器时，有几个重要的问题需要考虑： 性能：服务器的性能是关键因素之一。需要考虑处理器性能、内存容量、硬盘速度和网络带宽等方面，以满足预期的工作负载和用户需求。 可靠性：服务器的可靠性是确保系统连续运行和数据完整性的关键。需要考虑使用可靠的硬件组件、冗余机制和备份策略，以及监控和报警系统来及时发现和处理故障。 可扩展性：服务器的可扩展性是指能够根据需要动态调整和扩展系统的能力。需要考虑系统架构的弹性，如水平扩展和垂直扩展，以支持更多的用户和增加的工作负载。 安全性：服务器的安全性是非常重要的。需要采取安全措施来保护服务器和数据，如访问控制、身份验证、加密通信和漏洞修补等。 管理和监控：服务器需要进行有效的管理和监控，以确保系统的稳定性和高效性。需要考虑使用合适的管理工具和监控系统来跟踪服务器的性能、资源利用率和事件日志。 服务器集群组织方式主要有以下几种： 对等集群（Peer-to-Peer）：在对等集群中，每个服务器都具有相同的地位，可以相互通信和共享资源。对等集群通常用于分布式计算和文件共享等场景。 主从集群（Master-Slave）：在主从集群中，有一个主服务器（Master）和多个从服务器（Slave）。主服务器负责接收请求和处理业务逻辑，而从服务器负责辅助处理和备份。主从集群常用于负载均衡和高可用性要求较高的场景。 分布式集群（Distributed Cluster）：分布式集群是由多台服务器组成的集群，各个服务器相互协作完成任务。分布式集群可以通过分片（Sharding）和副本（Replication）等方式来分散数据和负载，提高系统的吞吐量和可扩展性。 负载均衡集群（Load Balancing Cluster）：负载均衡集群通过在多个服务器之间分配负载，以提高系统的性能和可靠性。负载均衡器（Load Balancer）可以根据不同的负载均衡策略将请求分发到不同的服务器上，实现负载均衡。 命名系统模块命名的基本概念、3种命名系统类型命名是指为实体、对象或概念赋予一个可识别和唯一的标识符或名称的过程。命名的基本概念包括： 标识符（Identifier）：用于唯一标识一个实体的符号或名称。标识符可以是字符串、数字、符号或其他形式的标记。 命名空间（Namespace）：命名空间是一个用于组织和管理标识符的容器。它提供了一种避免标识符冲突的机制，使得不同的实体可以在不同的命名空间中被唯一标识。 解析（Resolution）：解析是将标识符映射到相应的实体或对象的过程。解析可以通过查找命名空间中的映射关系、引用表或其他方式来完成。 根据命名系统的不同特点和应用场景，可以将命名系统分为以下三种类型： 层次命名系统（Hierarchical Naming System）：层次命名系统采用树状结构组织命名空间，通过父子关系将命名空间划分为不同的层次。每个层次可以包含子命名空间或实体标识符。常见的层次命名系统包括域名系统（DNS）和文件系统中的路径命名。 平坦命名系统（Flat Naming System）：平坦命名系统不采用层次结构，而是将所有标识符置于同一级别。标识符之间没有父子关系，而是独立存在。平坦命名系统可以更直接地访问标识符，但容易导致命名冲突。常见的平坦命名系统包括全局唯一标识符（GUID）和全局唯一名称（GUN）。 结构化命名系统（Structured Naming System）：结构化命名系统通过采用特定的命名规则和语法，将标识符划分为不同的组成部分，并赋予每个部分特定的含义。这种命名系统可以提供更丰富的语义信息，并支持更复杂的命名模式。常见的结构化命名系统包括统一资源定位符（URL）和统一资源标识符（URI）。 无层次命名中，实现定位实体的方法有哪些，各种方法的过程在无层次命名的环境中，可以使用以下几种方法来定位实体： 广播和多播（Broadcasting and Multicasting）：通过向网络中的所有节点广播或多播请求消息，让所有节点都能接收到该消息，并根据消息内容来定位实体。广播是将消息发送到网络的所有节点，而多播是将消息发送到预定的一组节点。 转发指针（Forwarding Pointers）：在无层次命名系统中，可以为每个实体分配一个独特的指针或引用，该指针包含实体的位置信息。当其他节点需要访问该实体时，通过使用指针来直接访问实体的位置。 宿主位置（Host Location）：在无层次命名系统中，可以为每个实体指定一个宿主位置，它可以是网络地址或物理地址等。当其他节点需要访问该实体时，可以使用宿主位置来直接定位实体所在的节点。 这些方法的过程如下： 广播和多播：节点发送广播或多播请求消息，该消息会被网络中的所有节点接收到。接收到消息的节点会根据消息的内容来判断是否需要响应该消息。如果需要响应，则通过消息中的标识符或其他信息来定位实体，并返回相应的响应消息给请求节点。 转发指针：节点通过指针或引用来定位实体。当节点需要访问实体时，会直接使用指针或引用来获取实体的位置信息。节点根据位置信息与实体所在的节点进行通信，以访问或操作该实体。 宿主位置：节点通过宿主位置来定位实体。当节点需要访问实体时，会使用宿主位置来直接寻址实体所在的节点。节点根据宿主位置与实体所在的节点进行通信，以访问或操作该实体。 结构化命名中的命名空间管理方法在结构化命名中，命名空间的管理方法可以包括以下几个方面： 命名空间的层次结构：结构化命名系统通过定义层次结构来组织和管理命名空间。不同的命名空间可以根据其层次关系进行划分，每个层次可以包含子命名空间或实体标识符。层次结构提供了一种组织和分类命名空间的方式，使得命名空间之间具有父子关系。 命名空间的命名规则和语法：结构化命名系统定义了命名空间的命名规则和语法，规定了如何命名命名空间和其中的实体标识符。命名规则和语法可以根据具体的命名约定和规范来定义，确保命名空间中的标识符具有一致性和可理解性。 命名空间的访问控制：结构化命名系统可以使用访问控制机制来管理命名空间的访问权限。通过定义访问策略和权限规则，可以控制哪些实体可以访问特定的命名空间，以及访问命名空间中的哪些实体。这有助于保护命名空间中的数据和资源，并确保合适的实体可以获取所需的信息。 命名空间的映射和解析：结构化命名系统提供了映射和解析机制，将命名空间中的标识符映射到相应的实体或对象。这可以通过查找命名空间的映射关系表、引用表或其他方式来完成。映射和解析机制使得能够根据标识符来定位和访问命名空间中的实体，实现了命名空间的功能。 综合上述方法，结构化命名系统能够提供一种可管理和可理解的命名空间结构，使得实体和标识符之间的关系清晰，并提供了相应的访问控制和映射解析机制。这有助于在复杂的系统环境中有效管理和使用命名空间。 DNS命名服务的基本原理和解析方法与过程DNS（Domain Name System）是互联网中用于将域名转换为 IP 地址的分布式命名服务。它提供了将易于记忆的域名映射到相应 IP 地址的功能。以下是DNS命名服务的基本原理和解析方法与过程的简要介绍： 基本原理： 域名层次结构：DNS采用了层次结构的域名命名空间，以区分不同的域名和子域名。域名由多个标签组成，每个标签之间用点号（.）分隔。 域名解析：DNS将域名解析为对应的 IP 地址，或者将 IP 地址解析为对应的域名。这是通过域名解析器（DNS Resolver）和DNS服务器（DNS Server）之间的交互来实现的。 解析方法与过程： 递归查询（Recursive Query）：当客户端的DNS解析器接收到域名查询请求时，它会向根域名服务器发送一个递归查询请求。根域名服务器会返回顶级域名服务器的地址给解析器。 迭代查询（Iterative Query）：解析器将查询转发给顶级域名服务器，顶级域名服务器再返回下一级域名服务器的地址给解析器。这个过程会一直重复，直到找到最终的授权域名服务器。 缓存（Caching）：为了提高解析效率，DNS解析器会在解析过程中缓存查询结果。这样，在后续相同域名的查询中，解析器可以直接使用缓存的结果，而无需进行完整的解析过程。 域名服务器层次结构：DNS服务器也采用层次结构，分为根域名服务器、顶级域名服务器、权威域名服务器等。每个域名服务器存储了一部分域名和相应的IP地址，它们之间通过查询和转发来相互协作完成域名解析的任务。 总结： DNS的基本原理是通过递归查询和迭代查询的方式来将域名解析为对应的IP地址，或将IP地址解析为对应的域名。这一过程涉及到DNS解析器、DNS服务器以及域名服务器的协同工作。在解析过程中，会进行缓存以提高效率。通过DNS命名服务，用户可以使用易于记忆的域名来访问互联网上的各种服务和资源。 通信Socket、RPC、RMI、MoM/MQ编程的基本方法 Socket编程：Socket编程是一种基于网络套接字的编程模式，用于实现网络通信。通过使用Socket API，开发者可以创建客户端和服务器应用程序，进行网络连接、数据发送和接收等操作。Socket编程提供了底层的网络通信接口，使得应用程序能够在不同主机之间进行数据交换。 RPC（Remote Procedure Call）编程：RPC是一种远程过程调用的编程模式，用于在分布式系统中进行跨网络的函数调用。通过RPC框架，客户端可以调用远程服务器上的函数，就像调用本地函数一样。RPC编程隐藏了底层的网络细节，提供了一种方便的方式来实现分布式应用程序。 RMI（Remote Method Invocation）编程：RMI是一种Java特定的RPC实现，用于在Java应用程序之间进行远程方法调用。通过RMI，Java对象的方法可以在不同的Java虚拟机（JVM）中进行调用，实现分布式的Java应用程序开发。RMI编程利用Java的远程对象和远程接口来定义和调用远程方法。 MoM（Message-oriented Middleware）/MQ（Message Queue）编程：MoM/MQ编程是一种基于消息传递的中间件编程模式，用于在分布式系统中进行异步通信。通过使用消息队列，应用程序可以通过发送和接收消息来实现解耦合的通信方式。MoM/MQ提供了消息的可靠传递、消息队列的管理和消息处理的机制，使得应用程序可以以异步的方式进行通信和处理消息。 远程过程调用远程过程调用（Remote Procedure Call，RPC）是一种分布式系统中的编程模式，用于实现跨网络的函数调用。它允许一个计算机程序在网络上调用另一个计算机程序的函数或方法，就像调用本地函数一样。 RPC的基本原理如下： 定义接口：首先，需要定义一个远程接口，其中包含了需要调用的远程函数的签名和参数。 代理生成：根据远程接口定义，生成客户端和服务器端的代理（Proxy）代码。客户端代理用于向服务器发送请求，服务器端代理用于接收请求并执行相应的函数。 通信传输：客户端代理将函数调用的请求进行封装，并通过网络发送到服务器端。常用的通信协议包括HTTP、TCP/IP、UDP等。 服务器端处理：服务器端接收到请求后，通过解析请求的参数和函数名，执行相应的函数，并将结果返回给客户端。 客户端接收结果：客户端代理接收到服务器返回的结果，将其解析并返回给调用方。 RPC的优点包括： 简化分布式系统开发：RPC隐藏了网络通信的细节，使得开发者可以像调用本地函数一样调用远程函数，简化了分布式系统的开发过程。 提高代码复用性：通过定义远程接口，不同的客户端可以共享相同的接口，从而提高了代码的复用性。 提高系统性能：RPC可以通过数据传输的优化和异步调用的方式提高系统的性能和吞吐量。 RPC也存在一些局限性，以下是一些常见的局限性： 网络延迟和可靠性：RPC依赖于网络通信，因此受网络延迟和可靠性的影响。如果网络延迟高或网络不可靠，会导致RPC调用的响应时间增加或调用失败。 跨平台和跨语言支持：RPC通常在特定的编程语言和平台上实现，并且客户端和服务器端需要使用相同的RPC框架和接口定义。这可能限制了跨平台和跨语言的互操作性。 序列化和反序列化开销：RPC需要将函数调用和返回值进行序列化和反序列化，以便在网络上传输。这涉及到数据的编码和解码，可能会引入一定的开销。 扩展性和可伸缩性：RPC在大规模系统中的扩展性和可伸缩性可能存在挑战。随着系统规模的增大，RPC的调用频率和负载可能会增加，需要合理设计和调优以保证性能和可扩展性。 依赖于网络连接：RPC调用需要建立有效的网络连接，如果网络连接不可用或中断，调用可能失败或超时。这也意味着RPC调用对网络稳定性和可靠性要求较高。 安全性和认证：RPC需要考虑数据的安全性和身份认证，以防止未经授权的访问和数据泄露。这需要在RPC框架中实现适当的安全措施和身份验证机制。 面向消息的通信、消息队列的通信、接口与管理机制面向消息的通信、消息队列的通信以及接口与管理机制是分布式系统中常见的通信模式和组件，它们有以下特点和功能： 面向消息的通信（Message-oriented Communication）：面向消息的通信是一种基于消息的异步通信模式，其中消息是在不同组件之间进行传递和交换的基本单位。在这种通信模式下，发送者将消息发送到消息传递系统中，并且不需要立即等待接收者的响应。接收者可以异步地接收和处理消息。这种通信模式提供了解耦合、可靠性、灵活性和可伸缩性等优势。 消息队列的通信：消息队列是一种用于在分布式系统中传递和存储消息的中间件。它通过提供队列机制来实现异步的、可靠的消息传递。消息发送者将消息发送到队列中，而消息接收者可以从队列中获取消息进行处理。消息队列提供了一种解耦合的通信方式，可以平衡系统之间的负载，提高系统的可靠性和可扩展性。 接口与管理机制：在分布式系统中，接口和管理机制起着关键的作用。接口定义了组件之间的通信规范和约定，它定义了消息的格式、数据结构、操作方法等。通过接口，不同的组件可以协调工作，进行消息的发送和接收。管理机制则负责管理和监控分布式系统中的各个组件和资源。它包括配置管理、错误处理、安全认证、负载均衡等功能，以确保系统的正常运行和性能优化。 流通信的基本含义和机制流通信是一种在计算机网络中进行数据传输的基本方式，它基于流式数据的传输和处理。在流通信中，数据被切分成小的数据块（也称为数据流或数据包），按顺序通过网络进行传输，并在接收端进行重新组装和处理。 流通信的基本含义是以连续的数据流的形式进行数据传输，而不是一次性地发送和接收固定大小的数据块。这种方式可以在数据流中按需传输和处理数据，而无需等待整个数据块的完整到达。 流通信的机制包括以下关键要素： 数据分割和封装：发送端将要传输的数据流切分为适当的数据块，并添加必要的控制信息，如标识符、校验和、序列号等，以便在接收端进行正确的数据重组和验证。 传输和接收：数据块通过网络传输到接收端。流通信可以基于不同的传输协议，如TCP（传输控制协议）或UDP（用户数据报协议）。 数据重组和处理：接收端根据接收到的数据块的控制信息，按序号进行数据重组，并进行相应的处理，如数据解码、错误校验、数据处理等。 可靠性和流量控制：流通信可以提供一定的可靠性和流量控制机制，以确保数据的正确传输和接收，并控制发送和接收之间的数据流速率，以避免数据拥塞和丢失。 流通信在许多网络应用中得到广泛应用，如视频流、音频流、实时数据传输等。它提供了实时性和灵活性，并且适用于需要连续数据传输和实时响应的场景。 叠加网络叠加网络（Overlay Network）是在底层网络之上构建的一种逻辑网络结构。它通过在现有的底层网络中创建一组节点之间的虚拟连接来实现通信。叠加网络可以用于增强现有网络的功能或提供额外的服务。 以下是叠加网络的一些关键特点和用途： 虚拟连接：叠加网络在底层网络中创建虚拟连接，这些连接可以是点对点的、点对多点的或多点对多点的。通过这些连接，节点可以进行通信和数据交换，而无需了解底层网络的细节。 路由和转发：叠加网络使用自己的路由和转发机制，将数据从源节点传递到目标节点。这些路由和转发算法可以根据叠加网络的设计目标进行优化，例如最短路径、负载均衡等。 功能增强：叠加网络可以在现有网络的基础上增加额外的功能和服务。例如，可以构建具有匿名性的叠加网络，用于保护用户的隐私；或者构建用于内容分发的叠加网络，以提高数据的可靠性和传输效率。 高度可扩展：叠加网络的拓扑结构可以灵活地进行调整和扩展，而不需要对底层网络进行修改。这使得叠加网络能够适应不同规模和需求的应用场景。 弹性和鲁棒性：叠加网络可以提供一定程度的弹性和鲁棒性。即使底层网络中的节点或连接出现故障，叠加网络可以通过重新路由或创建新的虚拟连接来保持通信的连续性。 叠加网络在互联网和分布式系统中得到广泛应用。例如，虚拟专用网络（VPN）使用叠加网络技术在公共网络上创建私有网络；点对点文件共享系统使用叠加网络来实现节点之间的直接文件传输。叠加网络提供了一种灵活且可定制的方式来构建具有特定功能和性能要求的网络结构。 应用层多播通信应用层多播通信是一种将数据从一个源节点传输到多个目标节点的通信方式。它可以在应用层上实现，而不依赖底层网络的多播支持。应用层多播通信可以通过以下方式实现： 源节点广播：源节点将数据广播到所有的目标节点。每个目标节点都可以接收到广播的数据，并进行相应的处理。这种方式简单直接，但会造成网络带宽的浪费，尤其在大规模的多播场景中。 接收者报名：源节点通过维护一个多播组成员列表，每个目标节点在加入多播组时向源节点报名。源节点将数据仅发送给已经报名的目标节点，减少了带宽的浪费。然而，这种方式需要维护和同步多播组成员列表，对源节点的管理和维护带来了额外的开销。 树形结构：源节点与多个目标节点之间构建一棵树形结构。源节点作为树的根节点，每个目标节点作为树的叶子节点。当源节点发送数据时，它通过树的分支将数据传递到每个目标节点。这种方式可以有效减少带宽的使用，但需要在每个目标节点和源节点之间建立和维护树形结构。 混合方式：多播通信也可以采用混合的方式，结合以上提到的方法。例如，可以使用源节点广播和接收者报名相结合的方式，根据网络规模和需求选择最合适的方法。 应用层多播通信适用于需要将数据同时发送给多个接收者的场景，如实时视频流、在线游戏、实时数据分发等。通过在应用层上实现多播通信，可以灵活地控制和管理多播过程，满足不同应用的需求，并提高网络的效率和可靠性。 Chord算法Chord算法是一种用于分布式哈希表的一致性哈希算法，用于在分布式系统中解决数据的分布和查找问题。Chord算法基于DHT（分布式哈希表）的概念，通过将数据和节点映射到一个圆环上，并使用一致性哈希算法来确定数据的存储位置和节点的路由。 下面是Chord算法的基本原理和步骤： 节点标识和哈希环：Chord算法中的节点通过一个唯一的标识符（如节点的IP地址或哈希值）表示，并将这些节点映射到一个虚拟的哈希环上。通常使用哈希函数将节点的标识符映射到哈希环上的一个位置。 节点加入：当一个新节点加入Chord网络时，它会选择一个合适的标识符，并将自己插入到哈希环中的适当位置。然后，它需要与其他节点进行通信以建立网络连接和获取相应的路由信息。 路由查找：为了定位存储在Chord网络中的数据，节点需要进行路由查找。节点通过比较目标数据的标识符与自己在哈希环上的位置，确定该数据应该存储在哪个节点上。如果当前节点不是数据所在的节点，它将通过一系列的路由表查找和跳跃操作将请求转发到正确的节点。 节点故障处理：在Chord网络中，节点可能会故障或离开。为了保持网络的连通性和一致性，Chord算法使用了一些机制来处理节点的故障。例如，当一个节点离开网络时，它的后继节点将接管其责任，并更新路由表以反映节点的变化。 Chord算法通过构建一致性哈希环和使用分布式路由表，实现了高效的数据分布和查找。它具有良好的可扩展性和容错性，能够处理节点的加入和离开，并在动态的分布式环境中保持数据的一致性和可用性。Chord算法被广泛应用于分布式系统中的数据存储和路由问题。 假设有一个分布式存储系统，使用Chord算法来管理数据的分布和查找。 在该系统中，有5个节点参与，它们分别是节点A、节点B、节点C、节点D和节点E。 首先，每个节点选择一个唯一的标识符，并将这些节点映射到一个虚拟的哈希环上。假设节点A的标识符为10，节点B的标识符为20，节点C的标识符为30，节点D的标识符为40，节点E的标识符为50。这些节点根据标识符的大小依次在哈希环上的适当位置插入。 接下来，节点加入网络。假设节点D是最新加入的节点。节点D会选择一个合适的标识符，并将自己插入到哈希环中的适当位置，如标识符为35，它会插入到节点C和节点D之间。 当需要查找特定数据时，假设数据的标识符为25。节点A接收到查找请求，通过比较目标数据的标识符与自己在哈希环上的位置，确定数据应该存储在节点B上。节点A将查找请求转发给节点B，节点B确认数据是否存在并返回相应结果。 如果节点C发生故障，节点D将接管其责任。节点D会更新自己的路由表，使其成为节点C的后继节点，从而保持网络的连通性和一致性。 这是一个简化的例子，展示了Chord算法在分布式存储系统中的应用。实际情况中，可能涉及更多节点和更复杂的路由表。通过Chord算法，节点可以根据数据的标识符快速定位数据所在的节点，实现高效的数据存储和检索。 socket编程平台和编译器 Linux平台PC GNU gcc HPUX平台 gcc 1）什么是 socket？socket是使用标准Unix 文件描述符 (file descriptor) 和其它程序通讯的方式。 什么？ 你也许听到一些Unix高手(hacker)这样说过：“呀，Unix中的一切就是文件！”那个家伙也许正在说到一个事实：Unix 程序在执行任何形式的 I/O 的时候，程序是在读或者写一个文件描述符。一个文件描述符只是一个和打开的文件相关联的整数。但是(注意后面的话)，这个文件可能是一个网络连接，FIFO，管道，终端，磁盘上的文件或者什么其它的东西。Unix 中所有的东西就是文件！所以，你想和Internet上别的程序通讯的时候，你将要使用到文件描述符。你必须理解刚才的话。现在你脑海中或许冒出这样的念头：“那么我从哪里得到网络通讯的文件描述符呢？”，这个问题无论如何我都要回答：你利用系统调用 socket()，它返回套接字描述符 (socket descriptor)，然后你再通过它来进行send() 和 recv()调用。 “但是…”，你可能有很大的疑惑，“如果它是个文件描述符，那么为什 么不用一般调用read()和write()来进行套接字通讯？”简单的答案是：“你可以使用！”。详细的答案是：“你可以，但是使用send()和recv()让你更好的控制数据传输。” 存在这样一个情况：在我们的世界上，有很多种套接字。有DARPA Internet 地址 (Internet 套接字)，本地节点的路径名 (Unix套接字)，CCITT X.25地址 (你可以将X.25 套接字完全忽略)。也许在你的Unix 机器上还有其它的。我们在这里只讲第一种：Internet 套接字。 2）Internet 套接字的两种类型什么意思？有两种类型的Internet 套接字？是的。不，我在撒谎。其实还有很多，但是我可不想吓着你。我们这里只讲两种。除了这些, 我打算另外介绍的 “Raw Sockets” 也是非常强大的，很值得查阅。那么这两种类型是什么呢？一种是”Stream Sockets”（流格式），另外一种是”Datagram Sockets”（数据包格式）。我们以后谈到它们的时候也会用到 “SOCK_STREAM” 和 “SOCK_DGRAM”。数据报套接字有时也叫“无连接套接字”(如果你确实要连接的时候可以用connect()。) 流式套接字是可靠的双向通讯的数据流。如果你向套接字按顺序输出“1，2”，那么它们将按顺序“1，2”到达另一边。它们是无错误的传递的，有自己的错误控制，在此不讨论。 有什么在使用流式套接字？你可能听说过 telnet，不是吗？它就使用流式套接字。你需要你所输入的字符按顺序到达，不是吗？同样，WWW浏览器使用的 HTTP 协议也使用它们来下载页面。实际上，当你通过端口80 telnet 到一个 WWW 站点，然后输入 “GET pagename” 的时候，你也可以得到 HTML 的内容。为什么流式套接字可以达到高质量的数据传输？这是因为它使用了“传输控制协议 (The Transmission Control Protocol)”，也叫 “TCP” (请参考 RFC-793 获得详细资料。)TCP 控制你的数据按顺序到达并且没有错误。你也许听到 “TCP” 是因为听到过 “TCP/IP”。这里的 IP 是指“Internet 协议”(请参考 RFC-791。) IP只是处理 Internet 路由而已。 那么数据报套接字呢？为什么它叫无连接呢？为什么它是不可靠的呢？有这样的一些事实：如果你发送一个数据报，它可能会到达，它可能次序颠倒了。如果它到达，那么在这个包的内部是无错误的。数据报也使用 IP 作路由，但是它不使用 TCP。它使用“用户数据报协议 (User Datagram Protocol)”，也叫 “UDP” (请参考 RFC-768。) 为什么它们是无连接的呢？主要是因为它并不象流式套接字那样维持一个连接。你只要建立一个包，构造一个有目标信息的IP 头，然后发出去。无需连接。它们通常使用于传输包-包信息。简单的应用程序有：tftp, bootp等等。 你也许会想：“假如数据丢失了这些程序如何正常工作？”我的朋友，每个程序在 UDP 上有自己的协议。例如，tftp 协议每发出的一个被接受到包，收到者必须发回一个包来说“我收到了！” (一个“命令正确应答”也叫“ACK” 包)。如果在一定时间内(例如5秒)，发送方没有收到应答，它将重新发送，直到得到 ACK。这一ACK过程在实现 SOCK_DGRAM 应用程序的时候非常重要。 3）网络理论既然我刚才提到了协议层，那么现在是讨论网络究竟如何工作和一些 关于 SOCK_DGRAM 包是如何建立的例子。当然，你也可以跳过这一段， 如果你认为已经熟悉的话。 现在是学习数据封装 (Data Encapsulation) 的时候了！它非常非常重要。它重要性重要到你在网络课程学习中无论如何也得也得掌握它（图1：数据封装）。主要 的内容是：一个包，先是被第一个协议(在这里是TFTP )在它的报头（也许 是报尾）包装(“封装”)，然后，整个数据(包括 TFTP 头)被另外一个协议 (在这里是 UDP )封装，然后下一个( IP )，一直重复下去，直到硬件(物理) 层( 这里是以太网 )。当另外一台机器接收到包，硬件先剥去以太网头，内核剥去IP和UDP 头，TFTP程序再剥去TFTP头，最后得到数据。 现在我们终于讲到声名狼藉的网络分层模型 (Layered Network Model)。这种网络模型在描述网络系统上相对其它模型有很多优点。例如， 你可以写一个套接字程序而不用关心数据的物理传输(串行口，以太网，连 接单元接口 (AUI) 还是其它介质)，因为底层的程序会为你处理它们。实际 的网络硬件和拓扑对于程序员来说是透明的。 不说其它废话了，我现在列出整个层次模型。如果你要参加网络考试， 可一定要记住： 应用层 (Application) 表示层 (Presentation) 会话层 (Session) 传输层(Transport) 网络层(Network) 数据链路层(Data Link) 物理层(Physical) 物理层是硬件(串口，以太网等等)。应用层是和硬件层相隔最远的—它 是用户和网络交互的地方。 这个模型如此通用，如果你想，你可以把它作为修车指南。把它对应 到 Unix，结果是： 应用层(Application Layer) (telnet, ftp,等等) 传输层(Host-to-Host Transport Layer) (TCP, UDP) Internet层(Internet Layer) (IP和路由) 网络访问层 (Network Access Layer) (网络层，数据链路层和物理层) 现在，你可能看到这些层次如何协调来封装原始的数据了。 看看建立一个简单的数据包有多少工作？哎呀，你将不得不使用 “cat” 来建立数据包头！这仅仅是个玩笑。对于流式套接字你要作的是 send() 发 送数据。对于数据报式套接字，你按照你选择的方式封装数据然后使用 sendto()。内核将为你建立传输层和 Internet 层，硬件完成网络访问层。 这就是现代科技。 现在结束我们的网络理论速成班。哦，忘记告诉你关于路由的事情了。 但是我不准备谈它，如果你真的关心，那么参考 IP RFC。 4）结构体终于谈到编程了。在这章，我将谈到被套接字用到的各种数据类型。 因为它们中的一些内容很重要了。 首先是简单的一个：socket描述符。它是下面的类型： int 仅仅是一个常见的 int。 从现在起，事情变得不可思议了，而你所需做的就是继续看下去。注 意这样的事实：有两种字节排列顺序：重要的字节 (有时叫 “octet”，即八 位位组) 在前面，或者不重要的字节在前面。前一种叫“网络字节顺序 (Network Byte Order)”。有些机器在内部是按照这个顺序储存数据，而另外 一些则不然。当我说某数据必须按照 NBO 顺序，那么你要调用函数(例如 htons() )来将它从本机字节顺序 (Host Byte Order) 转换过来。如果我没有 提到 NBO， 那么就让它保持本机字节顺序。 我的第一个结构(在这个技术手册TM中)—struct sockaddr.。这个结构 为许多类型的套接字储存套接字地址信息： sa_family 能够是各种各样的类型，但是在这篇文章中都是 “AF_INET”。 sa_data包含套接字中的目标地址和端口信息。这好像有点 不明智。 为了处理struct sockaddr，程序员创造了一个并列的结构： struct sockaddr_in (“in” 代表 “Internet”。) 用这个数据结构可以轻松处理套接字地址的基本元素。注意 sin_zero (它被加入到这个结构，并且长度和 struct sockaddr 一样) 应该使用函数 bzero() 或 memset() 来全部置零。 同时，这一重要的字节，一个指向 sockaddr_in结构体的指针也可以被指向结构体sockaddr并且代替它。这样的话即使 socket() 想要的是 struct sockaddr *，你仍然可以使用 struct sockaddr_in，并且在最后转换。同时，注意 sin_family 和 struct sockaddr 中的 sa_family 一致并能够设置为 “AF_INET”。最后，sin_port和 sin_addr 必须是网络字节顺序 (Network Byte Order)！ 你也许会反对道：”但是，怎么让整个数据结构 struct in_addr sin_addr 按照网络字节顺序呢?” 要知道这个问题的答案，我们就要仔细的看一看这 个数据结构： struct in_addr, 有这样一个联合 (unions)： 它曾经是个最坏的联合，但是现在那些日子过去了。如果你声明 “ina” 是数据结构 struct sockaddr_in 的实例，那么 “ina.sin_addr.s_addr” 就储 存4字节的 IP 地址(使用网络字节顺序)。如果你不幸的系统使用的还是恐 怖的联合 struct in_addr ，你还是可以放心4字节的 IP 地址并且和上面 我说的一样(这是因为使用了“#define”。) 5）本机转换我们现在到了新的章节。我们曾经讲了很多网络到本机字节顺序的转 换，现在可以实践了！ 你能够转换两种类型： short (两个字节)和 long (四个字节)。这个函 数对于变量类型 unsigned 也适用。假设你想将 short 从本机字节顺序转 换为网络字节顺序。用 “h” 表示 “本机 (host)”，接着是 “to”，然后用 “n” 表 示 “网络 (network)”，最后用 “s” 表示 “short”： h-to-n-s, 或者 htons() (“Host to Network Short”)。 太简单了… ，如果不是太傻的话，你一定想到了由”n”，”h”，”s”，和 “l”形成的正确 组合，例如这里肯定没有stolh() (“Short to Long Host”) 函数，不仅在这里 没有，所有场合都没有。但是这里有： htons()—“Host to Network Short” htonl()—“Host to Network Long” ntohs()—“Network to Host Short” ntohl()—“Network to Host Long” 现在，你可能想你已经知道它们了。你也可能想：“如果我想改变 char 的顺序要怎么办呢?” 但是你也许马上就想到，“用不着考虑的”。你也许 会想到：我的 68000 机器已经使用了网络字节顺序，我没有必要去调用 htonl() 转换 IP 地址。你可能是对的，但是当你移植你的程序到别的机器 上的时候，你的程序将失败。可移植性！这里是 Unix 世界！记住：在你 将数据放到网络上的时候，确信它们是网络字节顺序的。 最后一点：为什么在数据结构 struct sockaddr_in 中， sin_addr 和 sin_port 需要转换为网络字节顺序，而sin_family 需不需要呢? 答案是： sin_addr 和 sin_port 分别封装在包的 IP 和 UDP 层。因此，它们必须要 是网络字节顺序。但是 sin_family 域只是被内核 (kernel) 使用来决定在数 据结构中包含什么类型的地址，所以它必须是本机字节顺序。同时， sin_family 没有发送到网络上，它们可以是本机字节顺序。 6）IP 地址和如何处理它们现在我们很幸运，因为我们有很多的函数来方便地操作 IP 地址。没有 必要用手工计算它们，也没有必要用”&lt;&lt;”操作来储存成长整字型。 首先，假设你已经有了一个sockaddr_in结构体ina，你有一个IP地 址”132.241.5.10”要储存在其中，你就要用到函数inet_addr(),将IP地址从 点数格式转换成无符号长整型。使用方法如下： 注意，inet_addr()返回的地址已经是网络字节格式，所以你无需再调用 函数htonl()。 我们现在发现上面的代码片断不是十分完整的，因为它没有错误检查。 显而易见，当inet_addr()发生错误时返回-1。记住这些二进制数字？(无符 号数)-1仅仅和IP地址255.255.255.255相符合！这可是广播地址！大错特 错！记住要先进行错误检查。 好了，现在你可以将IP地址转换成长整型了。有没有其相反的方法呢？ 它可以将一个in_addr结构体输出成点数格式？这样的话，你就要用到函数 inet_ntoa()(“ntoa”的含义是”network to ascii”)，就像这样： 它将输出IP地址。需要注意的是inet_ntoa()将结构体in-addr作为一个参数，不是长整形。同样需要注意的是它返回的是一个指向一个字符的 指针。它是一个由inet_ntoa()控制的静态的固定的指针，所以每次调用 inet_ntoa()，它就将覆盖上次调用时所得的IP地址。例如： 假如你需要保存这个IP地址，使用strcopy()函数来指向你自己的字符 指针。 上面就是关于这个主题的介绍。稍后，你将学习将一个类 似”“的字符串转换成它所对应的IP地址(查阅域名服务,稍 后)。 7）socket()函数我想我不能再不提这个了－下面我将讨论一下socket()系统调用。 下面是详细介绍： 但是它们的参数是什么? 首先，domain 应该设置成 “AF_INET”，就 象上面的数据结构struct sockaddr_in 中一样。然后，参数 type 告诉内核 是 SOCK_STREAM 类型还是 SOCK_DGRAM 类型。最后，把 protocol 设置为 “0”。(注意：有很多种 domain、type，我不可能一一列出了，请看 socket() 的 man帮助。当然，还有一个”更好”的方式去得到 protocol，同 时请查阅 getprotobyname() 的 man 帮助。) socket() 只是返回你以后在系统调用种可能用到的 socket 描述符，或 者在错误的时候返回-1。全局变量 errno 中将储存返回的错误值。(请参考 perror() 的 man 帮助。) 8）bind()函数 一旦你有一个套接字，你可能要将套接字和机器上的一定的端口关联 起来。(如果你想用listen()来侦听一定端口的数据，这是必要一步—MUD 告 诉你说用命令 “telnet x.y.z 6969”。)如果你只想用 connect()，那么这个步 骤没有必要。但是无论如何，请继续读下去。 这里是系统调用 bind() 的大概： sockfd 是调用 socket 返回的文件描述符。my_addr 是指向数据结构 struct sockaddr 的指针，它保存你的地址(即端口和 IP 地址) 信息。 addrlen 设置为 sizeof(struct sockaddr)。 简单得很不是吗? 再看看例子: 这里也有要注意的几件事情。my_addr.sin_port 是网络字节顺序， my_addr.sin_addr.s_addr 也是的。另外要注意到的事情是因系统的不同， 包含的头文件也不尽相同，请查阅本地的 man 帮助文件。 在 bind() 主题中最后要说的话是，在处理自己的 IP 地址和/或端口的 时候，有些工作是可以自动处理的。 my_addr.sin_port = 0; / 随机选择一个没有使用的端口 / my_addr.sin_addr.s_addr = INADDR_ANY; / 使用自己的IP地址 / 通过将0赋给 my_addr.sin_port，你告诉 bind() 自己选择合适的端 口。同样，将 my_addr.sin_addr.s_addr 设置为 INADDR_ANY，你告诉 它自动填上它所运行的机器的 IP 地址。 如果你一向小心谨慎，那么你可能注意到我没有将 INADDR_ANY 转 换为网络字节顺序！这是因为我知道内部的东西：INADDR_ANY 实际上就 是 0！即使你改变字节的顺序，0依然是0。但是完美主义者说应该处处一 致，INADDR_ANY或许是12呢？你的代码就不能工作了，那么就看下面 的代码： my_addr.sin_port = htons(0); / 随机选择一个没有使用的端口 / my_addr.sin_addr.s_addr = htonl(INADDR_ANY);/ 使用自己的IP地址 / 你或许不相信，上面的代码将可以随便移植。我只是想指出，既然你 所遇到的程序不会都运行使用htonl的INADDR_ANY。 bind() 在错误的时候依然是返回-1，并且设置全局错误变量errno。 在你调用 bind() 的时候，你要小心的另一件事情是：不要采用小于 1024的端口号。所有小于1024的端口号都被系统保留！你可以选择从1024 到65535的端口(如果它们没有被别的程序使用的话)。你要注意的另外一件小事是：有时候你根本不需要调用它。如果你使 用 connect() 来和远程机器进行通讯，你不需要关心你的本地端口号(就象 你在使用 telnet 的时候)，你只要简单的调用 connect() 就可以了，它会检 查套接字是否绑定端口，如果没有，它会自己绑定一个没有使用的本地端 口。 9）connect()程序现在我们假设你是个 telnet 程序。你的用户命令你得到套接字的文件 描述符。你听从命令调用了socket()。下一步，你的用户告诉你通过端口 23(标准 telnet 端口)连接到”132.241.5.10”。你该怎么做呢? 幸运的是，你正在阅读 connect()—如何连接到远程主机这一章。你可 不想让你的用户失望。 connect() 系统调用是这样的： sockfd 是系统调用 socket() 返回的套接字文件描述符。serv_addr 是 保存着目的地端口和 IP 地址的数据结构 struct sockaddr。addrlen 设置 为 sizeof(struct sockaddr)。 想知道得更多吗？让我们来看个例子： 再一次，你应该检查 connect() 的返回值—它在错误的时候返回-1，并 设置全局错误变量 errno。 同时，你可能看到，我没有调用 bind()。因为我不在乎本地的端口号。 我只关心我要去那。内核将为我选择一个合适的端口号，而我们所连接的 地方也自动地获得这些信息。一切都不用担心。 10）listen()函数是换换内容得时候了。假如你不希望与远程的一个地址相连，或者说， 仅仅是将它踢开，那你就需要等待接入请求并且用各种方法处理它们。处 理过程分两步：首先，你听—listen()，然后，你接受—accept() (请看下面的 内容)。 除了要一点解释外，系统调用 listen 也相当简单。 sockfd 是调用 socket() 返回的套接字文件描述符。backlog 是在进入 队列中允许的连接数目。什么意思呢? 进入的连接是在队列中一直等待直 到你接受 (accept() 请看下面的文章)连接。它们的数目限制于队列的允许。 大多数系统的允许数目是20，你也可以设置为5到10。 和别的函数一样，在发生错误的时候返回-1，并设置全局错误变量 errno。 你可能想象到了，在你调用 listen() 前你或者要调用 bind() 或者让内 核随便选择一个端口。如果你想侦听进入的连接，那么系统调用的顺序可 能是这样的： socket(); bind(); listen(); / accept() 应该在这 / 因为它相当的明了，我将在这里不给出例子了。(在 accept() 那一章的 代码将更加完全。)真正麻烦的部分在 accept()。 11）accept()函数准备好了，系统调用 accept() 会有点古怪的地方的！你可以想象发生 这样的事情：有人从很远的地方通过一个你在侦听 (listen()) 的端口连接 (connect()) 到你的机器。它的连接将加入到等待接受 (accept()) 的队列 中。你调用 accept() 告诉它你有空闲的连接。它将返回一个新的套接字文 件描述符！这样你就有两个套接字了，原来的一个还在侦听你的那个端口， 新的在准备发送 (send()) 和接收 ( recv()) 数据。这就是这个过程！ 函数是这样定义的： sockfd 相当简单，是和 listen() 中一样的套接字描述符。addr 是个指 向局部的数据结构 sockaddr_in 的指针。这是要求接入的信息所要去的地 方（你可以测定那个地址在那个端口呼叫你）。在它的地址传递给 accept 之 前，addrlen 是个局部的整形变量，设置为 sizeof(struct sockaddr_in)。 accept 将不会将多余的字节给 addr。如果你放入的少些，那么它会通过改 变 addrlen 的值反映出来。 同样，在错误时返回-1，并设置全局错误变量 errno。 现在是你应该熟悉的代码片段。 注意，在系统调用 send() 和 recv() 中你应该使用新的套接字描述符 new_fd。如果你只想让一个连接进来，那么你可以使用 close() 去关闭原 来的文件描述符 sockfd 来避免同一个端口更多的连接。 12）send() and recv()函数这两个函数用于流式套接字或者数据报套接字的通讯。如果你喜欢使 用无连接的数据报套接字，你应该看一看下面关于sendto() 和 recvfrom() 的章节。 send() 是这样的： sockfd 是你想发送数据的套接字描述符(或者是调用 socket() 或者是 accept() 返回的。)msg 是指向你想发送的数据的指针。len 是数据的长度。 把 flags 设置为 0 就可以了。(详细的资料请看 send() 的 man page)。 这里是一些可能的例子： send() 返回实际发送的数据的字节数—它可能小于你要求发送的数 目！ 注意，有时候你告诉它要发送一堆数据可是它不能处理成功。它只是 发送它可能发送的数据，然后希望你能够发送其它的数据。记住，如果 send() 返回的数据和 len 不匹配，你就应该发送其它的数据。但是这里也 有个好消息：如果你要发送的包很小(小于大约 1K)，它可能处理让数据一 次发送完。最后要说得就是，它在错误的时候返回-1，并设置 errno。 recv() 函数很相似： sockfd 是要读的套接字描述符。buf 是要读的信息的缓冲。len 是缓 冲的最大长度。flags 可以设置为0。(请参考recv() 的 man page。) recv() 返回实际读入缓冲的数据的字节数。或者在错误的时候返回-1， 同时设置 errno。 很简单，不是吗? 你现在可以在流式套接字上发送数据和接收数据了。 你现在是 Unix 网络程序员了！ 13）sendto() 和 recvfrom()函数“这很不错啊”，你说，“但是你还没有讲无连接数据报套接字呢？” 没问题，现在我们开始这个内容。 既然数据报套接字不是连接到远程主机的，那么在我们发送一个包之 前需要什么信息呢? 不错，是目标地址！看看下面的： 你已经看到了，除了另外的两个信息外，其余的和函数 send() 是一样 的。 to 是个指向数据结构 struct sockaddr 的指针，它包含了目的地的 IP 地址和端口信息。tolen 可以简单地设置为 sizeof(struct sockaddr)。 和函数 send() 类似，sendto() 返回实际发送的字节数(它也可能小于 你想要发送的字节数！)，或者在错误的时候返回 -1。 相似的还有函数 recv() 和 recvfrom()。recvfrom() 的定义是这样的： 又一次，除了两个增加的参数外，这个函数和 recv() 也是一样的。from 是一个指向局部数据结构 struct sockaddr 的指针，它的内容是源机器的 IP 地址和端口信息。fromlen 是个 int 型的局部指针，它的初始值为 sizeof(struct sockaddr)。函数调用返回后，fromlen 保存着实际储存在 from 中的地址的长度。 recvfrom() 返回收到的字节长度，或者在发生错误后返回 -1。 记住，如果你用 connect() 连接一个数据报套接字，你可以简单的调 用 send() 和 recv() 来满足你的要求。这个时候依然是数据报套接字，依 然使用 UDP，系统套接字接口会为你自动加上了目标和源的信息。 14）close()和shutdown()函数你已经整天都在发送 (send()) 和接收 (recv()) 数据了，现在你准备关 闭你的套接字描述符了。这很简单，你可以使用一般的 Unix 文件描述符 的 close() 函数： 它将防止套接字上更多的数据的读写。任何在另一端读写套接字的企 图都将返回错误信息。如果你想在如何关闭套接字上有多一点的控制，你可以使用函数 shutdown()。它允许你将一定方向上的通讯或者双向的通讯(就象close()一 样)关闭，你可以使用： sockfd 是你想要关闭的套接字文件描述复。how 的值是下面的其中之 一： 0 – 不允许接受 1 – 不允许发送 2 – 不允许发送和接受(和 close() 一样) shutdown() 成功时返回 0，失败时返回 -1(同时设置 errno。) 如果在无连接的数据报套接字中使用shutdown()，那么只不过是让 send() 和 recv() 不能使用(记住你在数据报套接字中使用了 connect 后 是可以使用它们的)。 15）getpeername()函数这个函数太简单了。 它太简单了，以至我都不想单列一章。但是我还是这样做了。 函数 getpeername() 告诉你在连接的流式套接字上谁在另外一边。函 数是这样的： sockfd 是连接的流式套接字的描述符。addr 是一个指向结构 struct sockaddr (或者是 struct sockaddr_in) 的指针，它保存着连接的另一边的 信息。addrlen 是一个 int 型的指针，它初始化为 sizeof(struct sockaddr)。 函数在错误的时候返回 -1，设置相应的 errno。 一旦你获得它们的地址，你可以使用 inet_ntoa() 或者 gethostbyaddr() 来打印或者获得更多的信息。但是你不能得到它的帐号。(如果它运行着愚 蠢的守护进程，这是可能的，但是它的讨论已经超出了本文的范围，请参 考 RFC-1413 以获得更多的信息。) 16）gethostname()函数甚至比 getpeername() 还简单的函数是 gethostname()。它返回你程 序所运行的机器的主机名字。然后你可以使用 gethostbyname() 以获得你 的机器的 IP 地址。 下面是定义： 参数很简单：hostname 是一个字符数组指针，它将在函数返回时保存 主机名。size是hostname 数组的字节长度。 函数调用成功时返回 0，失败时返回 -1，并设置 errno。 17）域名服务（DNS）如果你不知道 DNS 的意思，那么我告诉你，它代表域名服务(Domain Name Service)。它主要的功能是：你给它一个容易记忆的某站点的地址， 它给你 IP 地址(然后你就可以使用 bind(), connect(), sendto() 或者其它 函数) 。当一个人输入： $ telnet  telnet 能知道它将连接 (connect()) 到 “198.137.240.100”。 但是这是如何工作的呢? 你可以调用函数 gethostbyname()： 很明白的是，它返回一个指向 struct hostent 的指针。这个数据结构 是这样的： #define h_addr h_addr_list[0] 这里是这个数据结构的详细资料： h_name – 地址的正式名称。 h_aliases – 空字节-地址的预备名称的指针。 h_addrtype –地址类型; 通常是AF_INET。 h_length – 地址的比特长度。 h_addr_list – 零字节-主机网络地址指针。网络字节顺序。 h_addr - h_addr_list中的第一地址。 gethostbyname() 成功时返回一个指向结构体 hostent 的指针，或者 是个空 (NULL) 指针。(但是和以前不同，不设置errno，h_errno 设置错 误信息，请看下面的 herror()。) 但是如何使用呢? 有时候（我们可以从电脑手册中发现），向读者灌输 信息是不够的。这个函数可不象它看上去那么难用。 这里是个例子： 在使用 gethostbyname() 的时候，你不能用 perror() 打印错误信息 (因为 errno 没有使用)，你应该调用 herror()。 相当简单，你只是传递一个保存机器名的字符串(例如 ““) 给 gethostbyname()，然后从返回的数据结构 struct hostent 中获取信息。 唯一也许让人不解的是输出 IP 地址信息。h-&gt;h_addr 是一个 char ， 但是 inet_ntoa() 需要的是 struct in_addr。因此，我转换 h-&gt;h_addr 成 struct in_addr ，然后得到数据。 18）客户-服务器背景知识这里是个客户—服务器的世界。在网络上的所有东西都是在处理客户进 程和服务器进程的交谈。举个telnet 的例子。当你用 telnet (客户)通过23 号端口登陆到主机，主机上运行的一个程序(一般叫 telnetd，服务器)激活。 它处理这个连接，显示登陆界面，等等。 图 2 说明了客户和服务器之间的信息交换。 注意，客户—服务器之间可以使用SOCK_STREAM、SOCK_DGRAM 或者其它(只要它们采用相同的)。一些很好的客户—服务器的例子有 telnet/telnetd、 ftp/ftpd 和 bootp/bootpd。每次你使用 ftp 的时候，在远 端都有一个 ftpd 为你服务。 一般，在服务端只有一个服务器，它采用 fork() 来处理多个客户的连 接。基本的程序是：服务器等待一个连接，接受 (accept()) 连接，然后 fork() 一个子进程处理它。这是下一章我们的例子中会讲到的。 19）简单的服务器这个服务器所做的全部工作是在流式连接上发送字符串 “Hello, World!\\n”。你要测试这个程序的话，可以在一台机器上运行该程序，然后 在另外一机器上登陆： $ telnet remotehostname 3490 remotehostname 是该程序运行的机器的名字。 服务器代码： 如果你很挑剔的话，一定不满意我所有的代码都在一个很大的main() 函数中。如果你不喜欢，可以划分得更细点。 你也可以用我们下一章中的程序得到服务器端发送的字符串。 20）简单的客户程序 这个程序比服务器还简单。这个程序的所有工作是通过 3490 端口连接到命令行中指定的主机，然后得到服务器发送的字符串。 客户代码: 注意，如果你在运行服务器之前运行客户程序，connect() 将返回 “Connection refused” 信息，这非常有用。 21）数据包 Sockets我不想讲更多了，所以我给出代码 talker.c 和 listener.c。 listener 在机器上等待在端口 4590 来的数据包。talker 发送数据包到 一定的机器，它包含用户在命令行输入的内容。 这里就是 listener.c： 注意在我们的调用 socket()，我们最后使用了 SOCK_DGRAM。同时， 没有必要去使用 listen() 或者 accept()。我们在使用无连接的数据报套接 字！ 下面是 talker.c： 这就是所有的了。在一台机器上运行 listener，然后在另外一台机器上 运行 talker。观察它们的通讯！除了一些我在上面提到的数据套接字连接的小细节外，对于数据套接 字，我还得说一些，当一个讲话者呼叫connect()函数时并指定接受者的地 址时，从这点可以看出，讲话者只能向connect()函数指定的地址发送和接 受信息。因此，你不需要使用sendto()和recvfrom()，你完全可以用send() 和recv()代替。 22）阻塞阻塞，你也许早就听说了。”阻塞”是 “sleep” 的科技行话。你可能注意 到前面运行的 listener 程序，它在那里不停地运行，等待数据包的到来。 实际在运行的是它调用 recvfrom()，然后没有数据，因此 recvfrom() 说” 阻塞 (block)”，直到数据的到来。 很多函数都利用阻塞。accept() 阻塞，所有的 recv*() 函数阻塞。它 们之所以能这样做是因为它们被允许这样做。当你第一次调用 socket() 建 立套接字描述符的时候，内核就将它设置为阻塞。如果你不想套接字阻塞， 你就要调用函数 fcntl()： #include #include …… sockfd = socket(AF_INET, SOCK_STREAM, 0); fcntl(sockfd, F_SETFL, O_NONBLOCK); …… 过设置套接字为非阻塞，你能够有效地”询问”套接字以获得信息。如 果你尝试着从一个非阻塞的套接字读信息并且没有任何数据，它不允许阻 塞—它将返回 -1 并将 errno 设置为 EWOULDBLOCK。 但是一般说来，这种询问不是个好主意。如果你让你的程序在忙等状 态查询套接字的数据，你将浪费大量的 CPU 时间。更好的解决之道是用 下一章讲的 select() 去查询是否有数据要读进来。 23）select()—多路同步 I/O虽然这个函数有点奇怪，但是它很有用。假设这样的情况：你是个服 务器，你一边在不停地从连接上读数据，一边在侦听连接上的信息。 没问题，你可能会说，不就是一个 accept() 和两个 recv() 吗? 这么 容易吗，朋友? 如果你在调用 accept() 的时候阻塞呢? 你怎么能够同时接 受 recv() 数据? “用非阻塞的套接字啊！” 不行！你不想耗尽所有的 CPU 吧? 那么，该如何是好? select() 让你可以同时监视多个套接字。如果你想知道的话，那么它就 会告诉你哪个套接字准备读，哪个又准备写，哪个套接字又发生了例外 (exception)。 闲话少说，下面是 select()： #include #include #include int select(int numfds, fd_set readfds, fd_set writefds,fd_set exceptfds, struct timeval timeout); 这个函数监视一系列文件描述符，特别是 readfds、writefds 和 exceptfds。如果你想知道你是否能够从标准输入和套接字描述符 sockfd 读入数据，你只要将文件描述符 0 和 sockfd 加入到集合 readfds 中。参 数 numfds 应该等于最高的文件描述符的值加1。在这个例子中，你应该 设置该值为 sockfd+1。因为它一定大于标准输入的文件描述符 (0)。 当函数 select() 返回的时候，readfds 的值修改为反映你选择的哪个 文件描述符可以读。你可以用下面讲到的宏 FD_ISSET() 来测试。 在我们继续下去之前，让我来讲讲如何对这些集合进行操作。每个集 合类型都是 fd_set。下面有一些宏来对这个类型进行操作： FD_ZERO(fd_set *set) – 清除一个文件描述符集合 FD_SET(int fd, fd_set *set) - 添加fd到集合 FD_CLR(int fd, fd_set *set) – 从集合中移去fd FD_ISSET(int fd, fd_set *set) – 测试fd是否在集合中 最后，是有点古怪的数据结构 struct timeval。有时你可不想永远等待 别人发送数据过来。也许什么事情都没有发生的时候你也想每隔96秒在终 端上打印字符串 “Still Going…”。这个数据结构允许你设定一个时间，如果 时间到了，而 select() 还没有找到一个准备好的文件描述符，它将返回让 你继续处理。 数据结构 struct timeval 是这样的： 只要将 tv_sec 设置为你要等待的秒数，将 tv_usec 设置为你要等待 的微秒数就可以了。是的，是微秒而不是毫秒。1,000微秒等于1毫秒，1,000 毫秒等于1秒。也就是说，1秒等于1,000,000微秒。为什么用符号 “usec” 呢? 字母 “u” 很象希腊字母 Mu，而 Mu 表示 “微” 的意思。当然，函数 返回的时候 timeout 可能是剩余的时间，之所以是可能，是因为它依赖于 你的 Unix 操作系统。 哈！我们现在有一个微秒级的定时器！别计算了，标准的 Unix 系统 的时间片是100毫秒，所以无论你如何设置你的数据结构 struct timeval， 你都要等待那么长的时间。 还有一些有趣的事情：如果你设置数据结构 struct timeval 中的数据为 0，select() 将立即超时，这样就可以有效地轮询集合中的所有的文件描述 符。如果你将参数 timeout 赋值为 NULL，那么将永远不会发生超时，即 一直等到第一个文件描述符就绪。最后，如果你不是很关心等待多长时间， 那么就把它赋为 NULL 吧。 下面的代码演示了在标准输入上等待 2.5 秒： 如果你是在一个 line buffered 终端上，那么你敲的键应该是回车 (RETURN)，否则无论如何它都会超时。现在，你可能回认为这就是在数据报套接字上等待数据的方式—你是对 的：它可能是。有些 Unix 系统可以按这种方式，而另外一些则不能。你 在尝试以前可能要先看看本系统的 man page 了。 最后一件关于 select() 的事情：如果你有一个正在侦听 (listen()) 的套 接字，你可以通过将该套接字的文件描述符加入到 readfds 集合中来看是 否有新的连接。 gRPC编程python安装 安装必要的依赖项： 下载Python 3.7的源代码： 解压缩源代码文件： 进入解压缩后的目录： 配置编译选项： 编译并安装Python 3.7： 安装完成后，您可以使用以下命令验证Python 3.7的安装： 2.安装gRPC与gRPC工具 3. git克隆代码 4. 运行 Java RMI编程1. 安装JavaCentOS 7.7 64位 2. 定义接口 运行RMI注册表 编译文件 运行 问题：运行RMI注册表时需要单独占用一个终端，所以，运行成功RMI实验，至少需要同时打开三个终端 MoM编程 RocketMQ: "},{"title":"Software Engineering Review","date":"2023-06-14T16:00:00.000Z","url":"/2023/06/15/software-engineering/","tags":[["Undergraduate Courses","/tags/Undergraduate-Courses/"]],"categories":[["Undergraduate Courses","/categories/Undergraduate-Courses/"]],"content":"软件工程总复习2023春季学期 QingDao University 第一章软件的定义（P1）软件是能够完成预定功能和性能的可执行的计算机程序，包括使程序正常执行所需要的数据，以及有关描述程序操作和使用的文档。 工程学范畴技术包括？（P4） 软件工程的分代是哪三代？作比较（P9） 构件的定义（P8）构件(component,有些文献翻译为组件）可以理解为标准化（或者规格化）的对象类（参见2.3.3节）。它本质上是一种通用的、可支持不同应用程序的组件，正如硬件中 的标准件一样，插入不同的平台或环境后即可直接运行。 软件危机的定义、产生的主要原因（P2,3）定义：庞大的软件费用，加上软件质量的下降，对计算机应用的继续扩大构成了巨大的威胁。面对这种严峻的形势，软件界的有识之士发出了软件危机的警告。 原因： 软件维护费用急剧上升，直接威胁计算机应用的扩大 软件生产技术进步缓慢， 是加剧软件危机的重要原因 第二章软件生存周期的定义、分为哪三个周期、那些阶段（P19）一切工业产品都有自己的生存周期，软件（产品）也不例外。 一个软件从开始立项起，到废弃不用止，统称为软件的生存周期 (life cycle)。 软件生存周期被划为：计划、开发和运行3个周期。 需求分析 软件分析 软件设计 编码 软件测试 运行维护 增量模型的定义（P24）增量模型 (incremental model) 是瀑布模型的顺序特征与快速原型法的迭代特征相结合的 产物。这种模型把软件看作一系列相互联系的增量 (increments)，在开发过程的各次迭代中， 每次完成其中的一个增量。 各种演化模型特点、适合哪类软件开发(P24) 可行性研究的内容、步骤（P35） 研究的内容 经济可行性。 实现这个系统有没有经济效益？多长时间可以收回成本？ 技术可行性。 现有的技术能否实现这一新系统？有哪些技术难点？建议采用的技术先进程度怎样？ 运行可行性。 为新系统规定的运行方式是否可行？例如， 若新系统是建立在原来已担负其他任务的计算机系统上的，就不能要求它在实时在线的状态下运行， 以免与原有的任务相矛盾。 法律可行性。 新系统的开发会不会在社会上或政治上引起侵权、 破坏或其他责任问题？ 研究的步骤 对当前系统进行调查和研究。 导出新系统的解决方案。 提出推荐的方案。 编写可行性论证报告：系统概述、可行性分析、结论意见 风险分析包括哪三项活动（P36) 风险识别 风险预测 风险的驾驭与监控 第三章结构分析（SA）模型的组成（画图）*/(P45)第一步：通过对现实环境的调查研究，获取当前系统的具体模型 第二步： 分析需求， 建立系统分析模型， 包括当前系统模型和目标系统模型。 去掉上述模型中的非本质因素， 提炼出当前系统的逻辑模型。 分析当前系统与目标系统的差别， 建立目标系统的逻辑模型。 第三步： 整理综合需求， 编写系统需求规格说明书。 第四步： 验证需求， 完善和补充对目标系统的描述。 通过目标系统的人机界面， 和用户一起确认目标系统功能， 主要是区分哪些功能交给计算机去做， 哪些功能由人工完成。 复审需求规格说明书， 补充迄今尚未考虑过的细节， 例如确定系统的响应时间、 增加出错处理等。 各部分的作用/应用 DFD是什么？表达____的一种图形化技术(P46)数据流图，表达面向数据流的一种图形化技术 数据字典哪三类？(P47) 数据流 数据文件 数据项 画判定表、判定树*(P49) 数据流图哪两种类型(P57) 变换型结构 事务性结构 优化初始SC的指导原则(P62) 对模块划分的原则 高扇入／低扇出的原则 模块/详细设计的目的？(P65)详细设计的目的， 是为SC图中的每个模块确定采用的算法和块内数据结构， 用选定的表达工具给出清晰的描述。 表达工具可由开发单位或设计人员自由选择， 但它必须具有描述过程细节的能力， 而且能在编码阶段直接将它翻译为用程序设计语言书写的源程序。 模块化设计原则、方法——实现清晰意图、可靠(P66-68) 清晰第一的设计风格 “清晰第一，效率第二” 结构化的控制结构 为了方便使用或者提高程序效率， 大多数软件开发项目还允许在详细设计中补充使 用 DO-UNTIL 和 DO-CASE 两种控制结构。 在许多情况下， 当程序执行到满足某种条件时， 需要立即从循环中转移出来。 逐步细化的实现方法——错误较少，可靠性也比较高。 由粗到细地对程序进行逐步的细化。 在细化程序的过程时， 同时对数据的描述进行细化。 每步细化均使用相同的结构化语言， 最后一般直接用伪代码来描述， 以便编码时直接翻译为源程序。 第四章UML的两类图、五类视图、九种图是什么？(P82) 图 图是系统构架在某个侧面的表示，UML1.4提供了两大类图静态图和动态图，共计9种不同的图。 静态图 用例图、类图、对象图、构件图和部署图 动态图 状态图、时序图、协作图和活动图 视图 用例视图(usecase view)。用例视图表达从用户角度看到的系统应有的外部功能，有时也称用户模型视图(usermodel view)。 逻辑视图Clogical view)。逻辑视图主要用类图和对象图来描述系统的静态结构，它同时也描述对象间为实现给定功能发送消息时出现的动态协作关系，故称结构模型视图(structural model view)。 进程视图(processview)。进程视图用于展示系统的动态行为及其并发性，也称行为模型视图(behavioralmodel view)。 构件视图(componentview)。构件视图展示系统实现的结构和行为特征，包括实现模块和它们之间的依赖关系，也称实现模型视图(implementationmodel view)。 部署视图(deploymentview)。部署视图显示系统的实现环境和构件被部署到物理结 构中的映射，如计算机、设备以及它们相互间的连接，哪个程序或对象在哪台计算机上执行 等。 用例之间的关系（选择）(P85)用例之间的关系： 扩展关系 (extend) 包含关系 (include) UML建模——构建关系、类、对象P85—P91 第五章软件需求分析四个步骤(P108) 需求获取 需求建模 需求描述 需求验证 面向对象的需求模型包括：(P114)用例模型、补充规约和术语表 需求管理的特定实践包括：(P124) 获得对需求的理解。 获取需求承诺。 管理需求变更。 维护变更历史， 为调整与控制提供数据。 在需求变更后维护对需求的双向可追溯性。 标识项目工作（包括计划和产品）与需求的不一致性。 软件需求的定义、哪两方面理解(P105)软件需求主要指一个软件系统必须遵循的条件或具备的能力。 这里的条件或能力可以从两个方面来理解：一是用户解决问题或达到目标所需的条件或能力， 即系统的外部行为； 二是系统为了满足合同、 规范或其他规定文档所需具有的条件或能力， 即系统的内部特性。 需求描述SRS包括哪些（选择）(P123)在分析阶段要编写 SRS, 包括引言、 信息描述、 功能描述、 行为描述、 质量保证、 接口描述和其他描述等内容。 第六章OOA模型的组成（P139） 面向对象模型的组成——哪三种子模型（画图、描述） 哪三种分析类：边界类、实体类、控制类 方法 第七章区分软件分析和设计(P165)从表面上看， 设计模型同分析模型有许多相似之处， 但两者的目的有本质的区别。 分析模型强调的是软件 “应该做什么“，并不给出解决问题的方案， 也不涉及具体的技术和平台。例如，它不必关心在J2EE还是．NET平台上实现，是否应用EJB或一般的JavaBean, 系统是安装在WebSphere还是WebLogic环境下。 而设计模型要回答 “该怎么做＂ 的问题，而且要提供解决问题的全部方案， 包括软件如何实现、如何适应特定的实施环境等。当设计模型完成后，编程人员便可以进行编程了。 信息隐藏定义(P166)早在 1972 年， D. L. Parnas 就提出了把系统分解为模块时应遵守的指导思想， 称之信息隐藏 (information hiding)。他认为， 模块内部的数据与过程， 应该对不需要了解这些数据与 过程的模块隐藏起来。 只有为了完成软件的总体功能而必须在模块间交换的信息， 才允许在 模块间进行传递。 控制复杂性的基本策略、常用方法（填空）（P167）分解 (decomposition) 是处理复杂问题常用的方法。 模块化设计的定义(P167)模块化设计 (modular design) 由来已久， 目的是按照规定的原则把大型软件划分为一个个较小的、相对独立但相互关联的模块。 分解和模块独立性， 是实现模块设计的重要指导思想。 图7.6 耦合性、顺序、内聚 举例说明（P169-171）面向对象包括哪两任务、哪些具体任务(P174) 系统架构设计 系统高层结构设计。 确定设计元素。 确定任务管理策略。 实现分布式机制。 设计数据存储方案。 人机界面设计。 系统元素设计 类／对象设计。 子系统设计。 包设计。 设计包， 将逻辑上相关的设计元素组织在一起。 分层次方法(P176-177) 图7.34 泛化和聚类的关系 区分聚集和组合的关系(P204) 聚集关系 部分对象可以是任意整体对象的一部分。 组合关系 在组合关系中， 整体和部分有相同的生存周期， 若整体不在了， 部分也会随之消失。 第八章软件测试的目的：发现程序的错误(P233)软件测试的特性(P234) 挑剔性 复杂性 不彻底性 经济性 黑盒测试依据、又叫——、使用技术——(P234-238)黑盒测试就是根据被测试程序功能来进行测试， 所以也称为功能测试。 3种常用技术: 等价分类法 边界值分析法 错误猜测法 等价分类法的定义所谓等价分类， 就是把输入数据的可能值划分为若干等价类， 使每类中的任何一个测试用例， 都能代表同一等价类中的其他测试用例。 边界值分析定义实践表明， 程序员在处理边界情况时， 很容易因疏忽或考虑不周发生编码错误。 采用边界值分析法， 就是要这样来选择测试用例， 使得被测程序能在边界值及其附近运行， 从而更有效地暴露程序中隐藏的错误。 白盒测试依据、又叫——、使用技术——(P240)白盒测试以程序的结构为依据， 所以又称为结构测试。 为了区分这两种白盒测试技术，以下把前者称为逻辑覆盖测试(logic coverage testing), 后者称为路径测试(path testing)。 逻辑覆盖测试五种覆盖(P242) 流程图——&gt;程序图(P245)黑盒白盒的测试用例设计(P248)多模程序设计包括哪些层次、含义(P254) 用例图——&gt;描述含义、多少参与者、用例之间的关系"},{"title":"CET6","date":"2023-05-27T14:41:28.000Z","url":"/2023/05/27/cet6/","categories":[["undefined",""]],"content":"Writing开头表达： 引出问题或现象： In the era of [某事物/技术的普及], it has become imperative to examine the impact of [某现象/问题] on our lives. The issue of [某问题] has become a matter of great concern in today’s society. 提出对比或对立观点： The debate over whether [某事物/观点] is beneficial or detrimental has been ongoing for years. The topic of [某话题] has ignited intense discussions, as people hold contrasting views on this matter. 引用相关事件或趋势： With the rapid advancement of [某技术/领域], we are witnessing profound changes in [某方面]. The increasing prevalence of [某现象] has drawn widespread attention and triggered a heated debate. 引入个人经历或观察： As an avid observer of [某领域/现象], I have come to realize the significance of [某问题] in our daily lives. My personal encounters with [某事物/情况] have prompted me to ponder the implications it has for society. 结尾表达： 总结观点并呼吁行动： In light of these considerations, it is crucial for individuals and governments alike to take proactive measures to address this issue. Based on the aforementioned analysis, it is incumbent upon us to collaborate and strive for a more sustainable future. 引用名人名言或警句： As [名人] wisely stated, “__.” Let us embrace this wisdom and work towards a better tomorrow. The words of [名人] still resonate today: “__.” It is high time we internalize this message and take action accordingly. 提出展望或建议： Looking ahead, it is essential that we explore innovative approaches to tackle this challenge and shape a more promising future. Moving forward, fostering a greater awareness and understanding of [某问题] will be pivotal in fostering positive change. 呼应开头或强调重要性： The significance of addressing [某问题] cannot be overstated. Let us join hands to pave the way for a brighter and more harmonious society. The implications of [某现象] extend far beyond our immediate surroundings. It is our responsibility to confront this issue head-on and strive for lasting solutions. "},{"title":"Latex的细致学习","date":"2023-05-24T08:47:12.000Z","url":"/2023/05/24/latex-learn/","tags":[["Latex","/tags/Latex/"]],"categories":[["undefined",""]],"content":"IEEE会议latex模板 CCBR会议模板"},{"title":"Python如何打包为exe文件","date":"2023-04-05T04:32:32.000Z","url":"/2023/04/05/python-to-exe/","tags":[["Python","/tags/Python/"]],"categories":[["undefined",""]],"content":"为什么要打包？当你想把你做的python游戏或者是脚本等.py文件发给别人时，打包为.exe文件，即使对方没有安装python也能运行。 安装pyinstaller安装pyinstaller很简单，直接cmd使用pip命令即可 pyinstaller打包项目全过程项目包含多个 Python 文件、多个个资源文件夹，资源文件夹里面又包含了一些子文件夹和 图片。 第一步：启动虚拟环境，切换至目标文件夹路径位置虚拟环境创建方法见本文附录 如图已进入项目目录，虚拟环境已激活（py37） 第二步：虚拟环境安装pyinstaller与其他第三方包（使用虚拟环境是为了使得依赖包调用最少，尽量使exe文件为轻量级） 第三步：生成 spec 文件（Pyinstaller 已经安装） 第五步：编辑 spec 文件 如图所示，第一个方框内是写入所有打包的.py文件，datas中写入所有项目的资源文件夹，由于我将其他python文件编写为了python软件包并放在了虚拟环境目录下，所以只有一个Main.py需要被打包。 自己编写的包可以放在虚拟环境的”.\\venv\\Lib\\site-packages”路径下，关于如何将自己写的python文件编为python软件包：文件夹内含有“init.py”文件，则该文件夹默认为python软件包 第六步：打包项目（注意这里的对象是 spec 文件） 打包成功！ 注：我的项目文件中用到了虚拟环境中face_recognition_models文件夹下的shape_predictor_68_face_landmarks.dat，但是打包过程中无法将它跟随虚拟环境打包，于是我将它跟随资源文件打包，解决了这个问题。 最后在项目中的 dist 文件夹，打开后会有一个 exe 文件，该文件可在windows系统中独立执行，不再依赖自行创建的python环境。 参数选项 描述 -F, -onefile 只生成一个单个文件（只有一个 exe 文件） -D, -onedir 打包多个文件，在dist中生成很多依赖文件，适合以框架形式编写工具代码，这样代码易于维护 -K, –tk 在部署时包含 TCL/TK -a, -ascii 不包含编码在支持 Unicode 的 Python 版本上默认包含所有的编码 -d, -debug 产生 debug 版本的可执行文件 -w, -windowed, -noconsole 使用 Windows 子系统执行当程序启动的时候不会打开命令行(只对 Windows 有效) -c, -nowindowed, -console 使用控制台子系统执行(默认)(只对 Windows 有效)、pyinstaller -c xxxx.py、pyinstaller xxxx.py —console -s, -strip 可执行文件和共享库将 run through strip -X, -upx 如果有 UPX 安装(执行 Configure.py 时检测)，会压缩执行文件( Windows 系统中的 DLL 也会) -o DIR, -out=DIR 指定 spec 文件的生成目录，如果没有指定，而且当前目录是 PyInstaller 的根目录，会自动创建一个用于输出( spec 和生成的可执行文件)的目录、如果没有指定，而当前目录不是 Pyinstaller 的根目录，则会输出到当前的目录下 -p DIR, -path=DIR 设置导入路径(和使用 PYTHONPATH 效果相似)、可以用路径分割符( Windows 使用分号，Linux 使用冒号)分割，指定多个目录、也可以使用多个 -p 参数来设置多个导入路径，让 pyinstaller 自己去找程序需要的资源 -i -icon= 将 file.ico 添加为可执行文件的资源(只对 Windows 系统有效)，改变程序的图标 以上是pyinstaller常用命令，更多内容请见官网 附录 Python常用命令创建虚拟环境python创建虚拟环境 设置国内源 使用requirement.txt批量安装pip安装 requirments.txt 对项目生成requirement依赖，以便于其他人对项目的forkfreeze-适用于虚拟环境 检查cuda版本（cmd命令） 常用Python代码python消除警告 plt绘图中文乱码 使用pip安装notebook 更改juypter打开默认位置 "},{"title":"Dynamic Programming","date":"2023-03-30T03:28:45.000Z","url":"/2023/03/30/Dynamic-Programming/","tags":[["Algorithm","/tags/Algorithm/"],["蓝桥杯","/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/"]],"categories":[["蓝桥杯","/categories/%E8%93%9D%E6%A1%A5%E6%9D%AF/"]],"content":"动态规划动态规划是一种通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法，常用于有重叠子问题和最优子结构性质的问题。 动态规划的核心思想是拆分子问题，记住过往，减少重复计算。动态规划可以用递归或迭代的方式实现，一般需要定义一个状态转移方程来描述子问题之间的关系。"},{"title":"NewBing&ChatGPT","date":"2023-03-05T02:22:58.000Z","url":"/2023/03/05/Common-instruction-set/","tags":[["OpenAI","/tags/OpenAI/"]],"categories":[["undefined",""]],"content":"ChatGPTOpenAI官网 ChatGPT github 以下是OpenAI对训练ChatGPT所使用方法的简单描述： Methods We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format. To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers had with the chatbot. We randomly selected a model-written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can fine-tune the model using Proximal Policy Optimization. We performed several iterations of this process. 根据OpenAI的描述，ChatGPT采用的是Reinforcement Learning from Human Feedback (RLHF)方法对模型进行训练，下面是DataWhale开源社区关于强化学习的代码仓库，便于进行相关知识的学习：github 关于ChatGPTChatGPT是Transformer和GPT等相关技术发展的集大成者。总体来说，ChatGPT的性能卓越的主要原因可以概括为三点： 使用的机器学习模型表达能力强。 训练所使用的数据量巨大。 训练方法的先进性。 一 机器学习模型基于文法的模型这个阶段主要思路就是利用语言学家的智慧尝试总结出一套自然语言文法，并编写出基于规则的处理算法进行自然语言处理。我们熟悉的编译器也是通过这种方法将高级语言编译成机器语言的。可惜的是，自然语言是极其复杂的，基本上不太可能编写出一个完备的语法来处理所有的情况，所以这套方法一般只能处理自然语言一个子集，距离通用的自然语言处理还是差很远。 基于统计的模型在这个阶段，大家开始尝试通过对大量已存在的自然语言文本（我们称之为语料库）进行统计，来试图得到一个基于统计的语言模型。比如通过统计，肯定可以确定“吃”后面接“饭”的概率肯定高于接其他词如“牛”的概率，即P(饭|吃)&gt;P(牛|吃)。 虽然这个阶段有很多模型被使用，但是本质上，都是对语料库中的语料进行统计，并得出一个概率模型。一般来说，用途不同，概率模型也不一样。不过，为了行文方便，我们接下来统一以最常见的语言模型为例，即建模“一个上下文后面接某一个词的概率“。刚才说的一个词后面接另一个词的概率其实就是一元语言模型。 模型表达能力模型表达能力简单来说就是模型建模数据的能力，比如上文中的一元语言模型就无法建模“牛吃草”和“我吃饭”的区别，因为它建模的本质统计一个词后面跟另一个词的概率，在计算是选“草”还是选“饭”的时候，是根据“吃”这个词来的，而“牛”和“我”这个上下文对于一元语言模型已经丢失。 模型参数数量有人说，既然如此，为啥我们不基于更多的上下文来计算下一个词的概率，而仅仅基于前一个词呢？OK，这个其实就是所谓的n元语言模型。总体来说，n越大，模型参数越多，表达能力越强。当然训练模型所需要的数据量越大（显然嘛，因为需要统计的概率的数量变多了）。 模型结构然而，模型表达能力还有另一个制约因素，那就是模型本身的结构。对于基于统计的n元语言模型来说，它只是简单地统计一个词出现在一些词后面的概率，并不理解其中的各类文法、词法关系，那它还是无法建模一些复杂的语句。比如，“我白天一直在打游戏”和“我在天黑之前一直在玩游戏“两者语义很相似，但是基于统计的模型却无法理解两者的相似性。因此，就算你把海量的数据喂给基于统计的模型，它也不可能学到ChatGPT这种程度。 基于神经网络的模型RNN &amp; LSTM 下面我们要用魔法打败魔法： Attention Mechanisms Transformer ChatGPT所依赖GPT3.5语言模型的的底层正是Transformer。 二 训练数据训练数据和模型之间是紧密相关的。训练数据是用来训练机器学习模型的输入数据，而模型则是对这些数据进行处理和学习的算法或函数。 在机器学习中，训练数据通常包括输入数据和相应的输出或标签。输入数据可以是各种形式的数据，如图像、文本、声音等。输出或标签则是我们希望模型能够预测的目标值，例如图像分类任务中的物体类别、文本情感分类任务中的情感类别等。 模型是根据训练数据学习到的算法或函数，它能够将输入数据映射到输出或标签。模型的选择取决于任务类型和数据特征。例如，在图像分类任务中，卷积神经网络（CNN）是一种常用的模型，而在自然语言处理任务中，循环神经网络（RNN）和其变种，如长短时记忆网络（LSTM）和门控循环单元（GRU），则是常用的模型。 训练数据的质量和数量直接影响模型的性能。更多、更准确的数据有助于模型更好地学习输入数据和输出或标签之间的关系，提高模型的精度。因此，选择合适的训练数据和充足的数据量是训练模型的关键。同时，模型的参数和结构也需要进行适当的调整和优化，以便更好地适应数据和任务，并达到更好的性能。 三 训练方法监督学习 vs 无监督学习监督学习和无监督学习是机器学习中两种基本的学习方式，它们有以下主要区别： 监督学习需要标签数据，而无监督学习不需要。在监督学习中，训练数据包括输入数据和相应的标签或输出，而在无监督学习中，只有输入数据没有标签或输出。 监督学习的目标是学习一个输入到输出的映射，而无监督学习的目标通常是学习数据的结构和特征。在监督学习中，我们希望模型能够准确地预测输出，如分类、回归等任务。而在无监督学习中，我们希望模型能够自动地发现数据的结构和特征，如聚类、降维等任务。 监督学习通常需要更多的数据和计算资源，而无监督学习则相对较少。因为监督学习需要标签数据，而这些数据需要大量的人力和时间来收集和标注。而无监督学习通常只需要原始数据即可进行训练，因此数据的获取和标注成本更低。 监督学习和无监督学习的应用场景不同。监督学习适用于已知输入和输出之间的映射关系的情况，例如分类、回归等任务。而无监督学习适用于没有明确标签或输出的情况，例如数据探索、特征提取等任务。 总的来说，监督学习和无监督学习是机器学习中两种基本的学习方式，各有其优缺点和适用场景，需要根据任务和数据的特点来选择合适的学习方式。 迁移学习Generative Pre-Training (GPT) models are trained using unsupervised learning approaches, specifically language modeling. Here are the general steps for training a GPT model: Preparing the Training Data: The first step is to collect a large corpus of text data. This corpus can be any type of text, such as books, articles, web pages, or social media posts. The text data is then preprocessed and tokenized into sequences of words or subwords. Pre-Training the Model: The GPT model is then trained on the preprocessed text data using a language modeling objective. The goal of language modeling is to predict the next word in a sequence of words given the previous words. The model is trained to maximize the likelihood of predicting the correct next word. Fine-Tuning the Model: Once the GPT model is pre-trained, it can be fine-tuned on a specific downstream task. This involves training the model on a smaller, task-specific dataset using supervised learning. The pre-trained GPT model is used as a starting point, and the final layers of the model are replaced with task-specific layers. The fine-tuned model is then trained to minimize the loss on the task-specific dataset. During pre-training, the GPT model learns to generate high-quality text by modeling the underlying structure and patterns in the input text data. The model is able to capture complex relationships between words and phrases, and can generate coherent and grammatical text that is similar to the training data. This unsupervised pre-training approach allows the GPT model to learn from a large amount of data without requiring task-specific annotations or labels. This makes GPT models highly versatile and applicable to a wide range of natural language processing tasks. 上下文学习指令微调与强化学习下面是关于ChatGPT的两篇参考论文： Stiennon, Nisan, et al. “Learning to summarize with human feedback.” Advances in Neural Information Processing Systems 33 (2020): 3008-3021. pdf Gao, Leo, John Schulman, and Jacob Hilton. “Scaling Laws for Reward Model Overoptimization.” arXiv preprint arXiv:2210.10760 (2022). pdf 由于ChatGPT需要提示词进行引导，所以能够让其理解的清晰明了准确的提示词与规则是必要的，下面是github是一个关于prompt的代码仓库：github 当然，OpenAI也给了很多其他有趣的功能与体验，详见下图： 其中作为计算机科学技术系的学生，自然对Code Completion最感兴趣，下面是一个简单的测试： 经过测试它进行简单代码模块书写没有问题，但是在复杂一些的要求中会出现逻辑混乱的错误。 回到正题，ChatGPT与前代GPT-3的一个明显区别就是在训练策略上用上了强化学习，在ChatGPT里，具体就是让那n名外包人员不断地从模型的输出结果中筛选，判断哪些句子是好的，哪些是低质量的，这样就可以训练得到一个 reward 模型。通过 reward 模型来评价模型的输出结果好坏。 ChatGPT 的影响在于：只要模型足够大，数据足够丰富，reward 模型经过了更多的人迭代和优化，完全可以创造一个无限逼近真实世界的超级 OpenAI 大脑。 关于GPT一、简介根据发布时间和引用量可以看出，Transformer 无疑是 GPT 系列模型和 BERT 模型的技术基石。根据下面的讲解，我们将知道 GPT 仅使用 Transformer 的解码器，采用预训练与微调的方式进行训练；受此启发，BERT 的作者在一两个月的时间内就把 BERT 模型实验跑通了，并且发现效果比较理想，最后也就有了这篇经典的论文。其实，当我们了解了 GPT 和 BERT 之后，不难发现 BERT 作者大量借鉴了 GPT 模型的思想，比如 GPT 和 BERT 的主要模块都来自 Transformer，一个是解码器，一个是编码器；都采用预训练与微调的方法；都向输入中添加特殊符号等等。当然，这并不是说 BERT 没有创新性，甚至其影响力远远超过 GPT。 在详细介绍 GPT 系列模型之前，我们先简单对比一下这三个模型。三者的模型结构大同小异，都是仅对 Transformer 的解码器进行微小的调整。主要区别在于模型大小和训练方式的不同。 GPT：主要采用 BookCorpus 数据集来训练语言模型，另外可供选择的数据集是 Word Benchmark，总大小约 5GB。GPT 的模型参数总量与 BERT~base~ 近似，约为 1.1 亿。其训练思想与 BERT 一致，预训练加与特定下游任务相关的微调。只在少数下游任务上效果相对理想。 GPT-2：从网上收集了 40GB 的 WebText 数据集，该数据集包含 800 篇 Reddit 高赞文章，由于 Reddit 文章涉及各个领域，因此既保证了数据质量和数量，又保证了数据的多样性，训练出泛化能力更强的 GPT-2 模型。GPT-2 的模型参数总量为 15 亿，训练庞大的 GPT-2 模型耗费了 OpenAI 公司非常多资金，这也是 GPT-2 引用率不高的原因之一，但是，巨大的投入也带来比较理想的效果。GPT-2 的庞大性使得其无法在微调阶段不断更新模型参数，因为每次更新模型参数都将带来了巨大的成本消耗，因此放弃了微调方法，换成完全无监督的 Zero-shot 方法。 GPT-3：GPT-3 模型参数总量达到了惊人的 1750 亿，数据集更是高达 45TB，单单是一个 GPT-3 模型就可能需要容量为 700GB 的硬盘来存储。海量的数据来自五个不同的语料库：Common Crawl，WebText2，Books1，Books2 和 Wikipedia，每个语料库都有不同的权重，对高质量的数据集进行更频繁的采样。GPT-3 采用 Few-shot，而不是 Zero-shot，使得模型的效果进一步提升，而且即使模型参数达到 1750 亿，效果仍然呈上升趋势。另外，在许多非常困难的下游任务上，比如撰写人类难以分辨的文章，甚至是编程，都有着非常惊艳的表现。 二、GPT1.模型结构GPT 模型由与 Transformer 的编码器结构一致的部件堆叠而成，以解码器的机制运行。习惯上，我们会说：GPT 模型由多层 Transformer 解码器堆叠而成。模型结构如图所示。 与 BERT 相比，GPT 的输入仅由词元嵌入和位置嵌入相加得到；与 Transformer 相比，GPT 的位置嵌入不再由正余弦函数给出，而是通过训练学习。 GPT 训练分为两个阶段：第一个阶段是预训练阶段，主要利用大型语料库完成非监督学习；第二个阶段是微调，针对特定任务在相应的数据集中进行监督学习。 2.预训练GPT 的预训练过程本质上是自回归语言模型的训练过程，这与 Transformer 解码器的训练一致。假设某个语料的词元序列为{ , , } ，GPT 的预训练目标是最大化 (u). , , 其中，为上下文窗口大小，为由神经网络给出的条件概率，为神经网络的参数。在代码实现时，窗口大小由 Sequence Mask 控制。 条件概率的具体计算过程描述如下。假设预测出的第个位置的词元是，其之前个位置的词元为 , , ，简记为 。将词元嵌入和位置嵌入的加和输入到 GPT 模型（多层 Transformer 解码器）中，经过线性映射和 Softmax 得到输出： 其中，为解码器个数，为词元嵌入矩阵，为位置嵌入矩阵，为第层解码器在第个位置的输出向量。可见，训练模型本质上是在学习条件概率分布。 3.微调假设有标签的数据集中每个样本包含一个句子和对应的标签。微调阶段，将输入到完成预训练的模型中，获得最后一层解码器在最后一个位置的输出向量,将其输入到一个额外的线性层来预测标签: 我们认为最后一个位置的输出向量蕴含了整个句子的语义信息。GPT 微调目标是最大化 另外，论文作者发现将自回归语言模型作为微调的辅助目标有助于提高模型的泛化能力、加快模型收敛。因此，一般使用两个损失函数的加权作为微调的损失函数： 注意，GPT 在预训练阶段不会向数据中添加特殊符号，但是当涉及特定下游任务时，不同任务的输入格式有所不同（比如单语句分类和多语句分类），因此在微调时需要添加特殊符号 、 和 ，论文作者认为模型可以通过微调理解特殊符号的含义，所以不需要在预训练时引入特殊符号。三个符号分别插入到整个输入的起始位置、两段语句的中间以及整个输入的结尾位置。可见，这与 BERT 是不同的，BERT 在预训练阶段和微调阶段均向数据中添加了特殊符号。 4.下游任务论文作者为四个经典下游任务设计具体的处理方式，并在这些任务上对 GPT 进行评估。这四个经典的下游任务为，文本分类（Text Classification）、文本蕴涵（Textual entailment）、文本相似（Textual similarity）和问答与常识推理（Question Answering and Commonsense Reasoning）。与命名实体识别等任务不同，这些任务只要提取语句级语义向量即可，认为特殊符号 对应的输出向量包含了任务需要的语义信息。 文本分类：文本分类任务的微调过程如上所述，即标准微调过程。GPT 以添加特殊符号 和 的词元序列作为输入，将输出向量输入到线性映射层后 Softmax 分类。 文本蕴涵：文本蕴含的判断是非对称性句子关系任务，即 premise 只能作为第一段语句输入，hypothesis 只能作为第二段输入，不可交换顺序，两段语句由特殊符号 ，判断是否能从 premise 得到 hypothesis。处理方式与文本分类相同。 文本相似：文本相似性任务是对称性句子关系任务，即交换两段语句的输入顺序，不影响二者语义相似性的判断。为了反映对称性，论文作者分别向 GPT 模型中输入两种顺序的序列以生成两个输出向量，两个向量相加后输入到线性映射层用于分类。 问答与常识推理：将文章，问题和一组可能的答案 分别拼接，得到。GPT 模型独立处理每个序列，通过 Softmax 产生分布。 四个任务的处理如图所示。 5.ELMo、BERT 和 GPT 的比较ELMo、BERT 和 GPT 都解决了早期的 Word2Vec 等预训练模型无法处理一词多义的问题。 整体上来说，ELMo 采用双向 LSTM 提取特征，而 BERT 和 GPT 均采用 Transformer 模块来提取特征，其中 BERT 使用的是编码器，GPT 使用的是解码器。很多 NLP 任务都表明 Transformer 提取特征的能力强于 LSTM，LSTM 依然具有长期遗忘的特点。 三者中，只有 GPT 采用单向语言模型，ELMo 和 BERT 都采用双向语言模型。严谨来说，ELMo 实际上只是将不同方向语言模型提取的特征的拼接，这种拼接融合特征的方法显然没有 BERT 一体化特征融合的方法有效。 在训练方法上，三者都包括预训练过程。ELMO 采用并非真正意义上的微调，常将 ELMo 提取的特征作为下游任务的附加输入；而 BERT 和 GPT 主要采用微调的方法使模型适应下游任务。 相较于 BERT，作为语言模型的 GPT 能力明显更加强大。BERT 采用 Transformer 编码器使得其在语言理解的任务上具有更好的适用性，但是无法处理类似机器翻译和文章续写的生成式任务，在这类任务上 GPT 的解码器结构更有优势，带来了更多的可能。 REF[1] Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. 2018. [2] Radford A, Wu J, Child R, et al. Language models are unsupervised multitask learners[J]. OpenAI blog, 2019, 1(8): 9. [3] Brown T, Mann B, Ryder N, et al. Language models are few-shot learners[J]. Advances in neural information processing systems, 2020, 33: 1877-1901. [4] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30. [5] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018. [6] 《深入浅出 Embedding》吴茂贵等著 [7] GPT，GPT-2，GPT-3 论文精读【论文精读】- bilibili [8] 李宏毅-ELMO, BERT, GPT讲解 - bilibili [9] GPT综述-各模型之间的对比 - 知乎 [10] GPT系列模型详解 - CSDN [11] GPT2模型详解 - CSDN [12] The Illustrated GPT-2 (Visualizing Transformer Language Models) - jalammar.github.io [13] 零次学习（Zero-Shot Learning）入门 - 知乎 [14] tokenizers：BPE算法 - CSDN [15] 深入理解NLP Subword算法：BPE、WordPiece、ULM - 知乎 [16] BERT, GPT, ELMo模型对比 - CSDN NewBing&amp;ChatGPT的对比询问实时性问题：ChatGPT NewBing 询问知识性问题：ChatGPT NewBing "},{"title":"Big data analysis method","date":"2023-01-25T11:00:00.000Z","url":"/2023/01/25/Big-data-analysis-method/","tags":[["Undergraduate Courses","/tags/Undergraduate-Courses/"]],"categories":[["Undergraduate Courses","/categories/Undergraduate-Courses/"]],"content":"01 数据挖掘之频繁模式及关联规则挖掘Apriori算法频繁项集——&gt;关联规则 02 线性模型LDA 线性判别 03 决策树信息增益 * 基尼指数 预剪枝 后剪枝 ​ 04 机器学习链式法则求导 梯度下降 模型评估与选择 pr图 公式 roc曲线 K-折交叉验证法 BP算法推导05 贝叶斯分类器朴素贝叶斯分类 * 拉普拉斯修正 半朴素贝叶斯 06 集成学习并行法 训练法 07 聚类划分方法* 两个算法 K-means PAM 大数据 第一个大题 逻辑回归 梯度下降 LDA线性判别 k-折交叉验证法 朴素贝叶斯分类 KNN 第二个大题 神经网络和深度学习 BP算法 给你一个感知机结构 将参数标注好 使用链式法则展示参数更新过程 *卷积层的作用：减少参数数量，刻画训练集中某一类样本的共同模式 *池化层的作用：降低模型复杂度，避免过拟合 *relu激活函数 *卷积运算 *参数计算 第三个大题 聚类 基于划分的聚类方法：切比雪夫距离、绝对值距离 层次聚类 合并法 第四个大题 关联规则 第五个大题 给你一个场景 让你提供设计方案（开放题目） $在此鸣谢本文贡献者：Qing,Shunyao~Wu(advisor),Ying~Li(advisor)$"},{"title":"Artificial Intelligence","date":"2023-01-16T15:00:00.000Z","url":"/2023/01/16/artificial-intelligence/","tags":[["Undergraduate courses","/tags/Undergraduate-courses/"]],"categories":[["Undergraduate Courses","/categories/Undergraduate-Courses/"]],"content":"01 绪论02 自动推理1.概述确定性推理与不确定性推理 按推理时所用知识的确定性来划分，推理可分为确定性推理、不确定性推理 确定性推理：推理时所用的知识都是精确的，推出的结论也是确定的，其真值或者为真，或为假，没有第三种情况出现 不确定性推理：推理时所用的知识不都是精确的，推出的结论也不完全是肯定的，真值位于真与假之间，命题的外延模糊不清 启发式推理与非启发式推理 按推理过程中是否运用启发式知识分类： 启发式推理：在推理过程中运用了与问题有关启发式知识，即解决问题的策略、技巧和经验，加快了推理过程，提高了搜索效率。 非启发式推理：在推理过程中只按照一般的控制逻辑进行推理。缺乏对求解问题的针对性，推理效率较低，容易出现组合爆炸问题。 搜索策略 盲目搜索:也称为无信息搜索，即只按预定的控制策略进行搜索,在搜索过程中获得的中间信息不用来改进控制策略。 启发式搜索: 在搜索中加入了与问题有关的启发性信息,用于指导搜索朝着最有希望的方向进行,加速问题的求解过程并找到最优解。 2.启发式搜索前面讨论的方法都是盲目的搜索方法,即都没有利用问题本身的特性信息,在决定要被扩展的节点时,都没有考虑该节点在解的路径上的可能性有多大,它是否有利于问题求解以及求出的解是否为最优 启发信息的强度 强：降低搜索工作量，但可能导致找不到最优解 弱：一般导致工作量加大，极限情况下变为盲目搜索，但可能可以找到最优解 启发性信息和估价函数 用于指导搜索过程,且与具体问题有关的控制性信息称为为启发性信息 用于评价节点重要性的函数称为估价函数.记为 g(x)为从初始节点S~0~到节点x已经实际付出的代价 h(x)是从节点x到目标节点S~g~的最优路径的估计代价,体现了问题的启发性信息,称为启发函数 f(x)表示从初始节点经过节点x到达目标节点的最优路径的代价估价值,其作用是用来评估OPEN表中各节点的重要性,决定其次序。 爬山算法基本思想利用评估函数f(x)来估计目标状态和当前状态的“距离”。①当一个节点被扩展后，对子节点x进行评估得到f(x)，按f(x)的升序排列并把这些节点压人栈。因此，栈顶元素具有最小的f(x)值。②弹出栈顶元素并和目标状态比较。如果栈顶元素不是目标，则扩展栈顶元素，并计算其所有子状态的f值，并按升序把这些子状态压入栈中。③如果栈顶元素是目标，则算法退出。否则该过程循环下去，直至栈为空。 最好优先算法：从最有希望的节点开始，并生成其所有的子女节点。然后计算所有节点的性能，基于该性能选择最有希望的节点扩展，而不仅仅是从当前节点所生成的子女节点中选择。 A*算法启发式函数h(x)的选取、求解过程、画搜索图、open表与close表（详见 PPT） 启发性信息越强，搜索节点越少；启发性信息越弱，搜索节点越多； 3.与或图启发式搜索在问题求解过程中，将一个大的问题变换成若干个子问题，子问题又可以分解成更小的子问题，这样一直分解到可以直接求解为止，全部子问题的解就是原问题的解。 与或图（详见PPT） A*算法不能搜索与或图 4.博弈搜索$博弈树博弈树特点 博弈中两种最基本的搜索方法 MAXMIN基本思想 极大极小搜索过程 α-β搜索过程 α剪枝法 α-β剪枝的弱点 03 不确定性推理主观贝叶斯有原题 什么是不确定性推理? 是从不确定性的初始证据出发,通过运用不确定性的知识,最终推出具有一定程度的不确定性但却合理或者近乎合理的结论的思维过程。 确定性是智能问题的本质特征，可以说智能主要反映在求解不确定性问题的能力上。 可信度方法$ 主观贝叶斯$经典概率方法部分（见PPT） 05-1 机器学习基础机器学习方法分类最常见的分类方式： 监督（supervised）学习: 根据已知类别的训练样本（known sample） ，由机器从其中进行学习或者训练，从中勾画出各类事物在特征空间分布的规律性，进而对新样本进行判断; 无监督（unsupervised）学习或聚类（clustering）：由机器从未知类别的样本（unknown sample）中进行学习（自学习），从中发现有利于对象分类的规律； 半监督（semi‐supervised）学习：由机器利用部分已知类别的样本，从中恢复样本的相关附加信息，进而进行聚类分析。 机器学习中的几个关键数学问题机器学习不同类型的问题 聚类 2. 分类 3. 强化学习 4.回归 从数学的角度看分类问题已知： （1）函数的值域为有限个离散点（2）函数在某些点上的函数值。 回归问题与分类问题的关系分类： y = {1,2,3,...,m} 离散值因此回归可以看成是分类问题的推广，可以看成是类别数为不可数时的分类问题。但我们不能以此简单地认为回归问题比分类问题难，事实上由于回归问题的值域为整个实数域，常常更好处理。 如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法(min square)、梯度下降法、牛顿法。 最小二乘法的局限性 只有当矩阵A的每一列都是线性不相关的，矩阵A^T^ A才是可逆的。 A^T^ A不可逆，无法求解 当特征维数大时，A^T^ A求逆困难 梯度下降法(Gradient Descent) 梯度即是某一点最大的方向导数，沿梯度方向函数有最大的变化率（正向增加，逆向减少） 代价函数J沿梯度的负方向下降最快 梯度下降法的关键因素 初值选择 步长α的选择 梯度下降法小结 梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。 步长太大，会导致迭代过快，甚至有可能错过最优解 。步长太小，迭代速度太慢，很长时间算法都不能结束 。所以算法的步长需要多次运行后才能得到一个较为优的值。 逻辑回归（Logistics Regression）解决分类问题 线性回归+阈值 缺点：健壮性不够，对噪声敏感 如使用线性回归，则可得到图中粉色的直线 确定一个threshold:0.5进行预测：Misplaced & y = 1, if&amp;h(x) &gt;=0.5\\\\ y = 0, if&amp;h(x)&lt;0.5 能够很好地进行分类 因此，引入逻辑回归模型来解决分类问题 。 Sigmod函数性质逻辑回归（Logistics Regression）05-3 线性分类器05-4 非线性分类器支持向量机，一种线性和非线性数据有前途的新划分类方法。巧妙利用向量内积的回旋，通过引入非线性核函数将问题变为高维特征空间与低维输入空间的相互转换，解决了数据挖掘中的维数灾难。由于计算问题最终转化为凸二次规划问题，因此挖掘算法是无解或有全局最优解。 ※ 什么是支持向量（简单来说，就是支持或支撑平面上把两类类别划分开的向量点） ※ 这里的“机（machine，机器）”便是一个算法。在机器学习领域，常把一些算法看做是一个机器，如分类机（当然，也叫做分类器），而支持向量机本身便是一种监督式学习的方法，它广泛的应用于统计分类以及回归分析中。 目标：找到一个超平面，使得它能够尽可能多的将两类数据点正确的分开，同时使分开的两类数据点距离分类面最远。 最优分类超平面支持向量核方法make linear model work in nonlinear settings! 目前研究最多的核函数主要有三类： 多项式内核 高斯径向基函数内核RBF Sigmoid内核 SVM实现非线性分类的思想Boosting算法Adaboost分类器 弱分离器 强分类器 Adaboost 训练算法 级联（cascade）分类器 提升方法(boosting)分类器的实现—Adaboost算法05-5 决策树06-1 人工神经网络概述神经网络（neural networks, NN），也称作人工神经网络（ artificial neural networks， ANN），或神经计算(neural computing， NC)，是对人脑或生物神经网络的抽象和建模，具有从环境学习的能力，以类似生物的交互方式适应环境。 神经元是大脑的主要计算单元。 感知机感知器的学习就是修改权值和偏置的过程。 感知器学习规则感知器特别适合解决简单的模式分类问题。 单层感知器的局限性XOR问题的解决——多层感知器网络两层感知器结构多层感知器神经网络的分类能力多层感知器研究的瓶颈人工神经网络发展 前馈神经网络：BP网络、RBF网络 反馈神经网络： 以Hopfield模型为代表 自组织网络（竞争学习网络）： 以SOM模型为代表 深度神经网络：CNN、RNN 前馈神经网络MLP网络特性MLP网络特性：可以实现任意复杂的非线性映射关系用于分类： 两层网（一个隐层）可以实现空间内任意的凸区域的划分 三层网（两个隐层）可以实现任意形状（连续或不连续）区域的划分 BP算法 既然我们无法直接得到隐层的权值，能否先通过输出层得到输出结果和期望输出的误差来间接调整隐层的权值呢？ 提出由Sigmoid函数代替之前的阶跃函数作为激励函数来构造神经元。 Sigmoid函数是单调递增的非线性函数，函数本身及其导数都是连续的，无限次可微。 BP算法的基本思想BP网络的拓扑结构前馈神经网络的计算反向传播（back-propagation，BP）BP算法的改进BP网络的特点RBF神经网络基本结构 三层神经网络：输入层、输出层及一个隐层 隐层节点激活函数为径向基函数，输出节点激活函数为线性函数。 输入层与隐层无权连接，隐层与输出层是线性加权，即网络可调参数。 RBF与BP神经网络的区别全局逼近 vs 局部逼近BP网络对目标函数的逼近跟所有数据都相关 RBF网络对目标函数的逼近仅仅根据中心点附近的数据。 RBF神经网络与SVM的区别RBF网络的应用反馈神经网络：Hopfield网络Hopfield网络是神经网络发展历史上的一个重要的里程碑。由美国加州理工学院物理学家J.J.Hopfield教授于1982年提出，是一种单层反馈神经网络。 Hopfield网络分为离散型和连续型两种网络模型，分别记作DHNN (Discrete Hopfield Neural Network) 和CHNN(Continues Hopfield Neural Network) 。 06-2 BP算法推导07-1 深度学习CNN卷积神经网络的特点 可以当作是一个分类器，也可以看作是一种特征提取方式（把中间某一层的输出当作数据的一种特征表示）。 CNN能够得出原始图像的有效表征，这使得CNN能够直接从原始像素中，经过极少的预处理，识别视觉上面的规律。避免了传统识别算法中复杂的特征提取和数据重建过程。 基于大规模的数据。每个网络都有众多的参数，少量数据无法将参数训练充分。 卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。 卷积神经网络的核心思想 局部感知 权重共享 多卷积核 空间下采样 目标1：减少参数，降低复杂度 目标2：实现卷积特征自动提取 将局部感知、权值共享以及空间亚采样这三种结构思想结合起来获得了某种程度的位移、尺度、形变不变性。 由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数，降低了网络参数选择的复杂度。 卷积神经网络中的每一个特征提取层（C-层）都紧跟着一个用来求局部平均与二次提取的计算层（S-层），这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。 如何确定隐层神经元的个数？卷积神经网络核心思想四：空间下采样卷积神经网络的基本结构CNN网络 vs BP网络实例：卷积神经网络实现手写数字识别： LeNet-5 $上图模型的基本参数输入：224×224大小的图片，3通道 第一层卷积：5×5大小的卷积核96个，每个GPU上48个。步长为4，得到卷积图为55*55. 第一层max-pooling：2×2的 核。第二层卷积：3×3卷积核256个，每个GPU上128个。第二层max-pooling：2×2的核。第三层卷积：与上一层是全连接，3*3的卷积核384个。分到两个GPU上个192个。 模型参数第四层卷积：3×3的卷积核384个，两个GPU各192个。该层与上一层连接没有经过pooling层。第五层卷积：3×3的卷积核256个，两个GPU上各128个。第五层max-pooling：2×2的核。第一层全连接：4096维，将第五层max-pooling的输出连接成为一个一维向量，作为该层的输入。第二层全连接：4096维Softmax层：输出为1000，输出的每一维都是图片属于该类别的概率。 网络模型优化：梯度下降法 在处理复杂任务上，深度网络比浅层的网络具有更好的效果。 目前优化神经网络的方法都是基于误差反向传播的思想，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。 优化目标：损失函数Loss求最小值。 寻找最小值问题：梯度下降法。 梯度发散 vs 梯度爆炸梯度发散：sigmod函数梯度发散：tanh函数如何解决梯度消失及梯度爆炸？ReLU激活函数ReLU函数的主要贡献：ReLU缺点：梯度爆炸解决方案Batch normalization（批规范化）因此我们可以通过固定每一层网络输入值的分布来对减缓问题。 数据增强（data augmentation）L2 regularizationDropout Dropout基本思路： 在开始，随机删除掉隐藏层一部分的神经元，如图，虚线部分为开始时随机删除的神经元： 然后，在删除后的剩下的神经元上正向和反向更新权重和偏向； 再恢复之前删除的神经元，再重新随机删除一半的神经元，进行正向和反向更新w和b; 重复上述过程。 一般情况下，对于同一组训练数据，利用不同的神经网络训练之后，求其输出的平均值可以减少overfitting。Dropout就是利用这个原理，每次丢掉一半的一隐藏层神经元，相当于在不同的神经网络上进行训练，这样就减少了神经元之间的依赖性，即每个神经元不能依赖于某几个其他的神经元（指层与层之间相连接的神经元），使神经网络更加能学习到与其他神经元之间的更加健壮robust的特征。 07-3 深度学习RNNRNN是一类扩展的人工神经网络，它是为了对序列数据进行建模而产生的。 针对对象：序列数据。例如文本，是字母和词汇的序列；语音，是音节的序列；视频，是图像的序列；气象观测数据，股票交易数据等等，也都是序列数据。 核心思想：样本间存在顺序关系，每个样本和它之前的样本存在关联。通过神经网络在时序上的展开，我们能够找到样本之间的序列相关性 。 07-4 集成学习基本思想 在机器学习中，直接建立一个高性能的分类器是很困难的。 但是，如果能找到一系列性能较差的分类器（弱分类器），并把它们集成起来的话，也许就能得到更好的分类器。 集成学习：使用一系列学习器进行学习，并使用某种规则把各个学习结果进行整合从而获得比单个学习器更好的学习效果的一种机器学习方法。 如果把单个分类器比作一个决策者的话，集成学习的方法就相当于多个决策者共同进行一项决策。 学习方法Boosting：个体学习器间存在强依赖关系，必须串行生成的序列化方法；串行：下一个分类器只在前一个分类器预测不够准的实例上进行训练或检验。 Bagging：个体学习器间不存在强依赖关系，可同时生成的并行化方法。并行：所有的弱分类器都给出各自的预测结果，通过组合把这些预测结果转化为最终结果。 07-5 强化学习 智能体（Agent） 感知外界环境的状态（State）和奖励反馈（Reward），并进行学习和决策。智能体的决策功能是指根据外界环境的状态来做出不同的动作（Action），而学习功能是指根据外界环境的奖励来调整策略。 环境（Environment） 智能体外部的所有事物，并受智能体动作的影响而改变其状态，并反馈给智能体相应的奖励。 强化学习中的基本要素 环境的状态集合：S； 智能体的动作集合：A； 状态转移概率：p(s’|s,a)，即智能体根据当前状态s做出一个动作a之后，下一个时刻环境处于不同状态s’的概率； 即时奖励：R : S × A × S’ → R，即智能体根据当前状态做出一个动作之后，环境会反馈给智能体一个奖励，这个奖励和动作之后下一个时刻的状态有关。 在此鸣谢本文贡献者："},{"title":"(一)对Dlib库初识","date":"2022-12-13T12:30:00.000Z","url":"/2022/12/13/dlib-note/","tags":[["ML","/tags/ML/"]],"categories":[["undefined",""]],"content":"此处Dlib官方网站 一、安装Dlib库Dlib库简介 Dlib is a modern C++ toolkit containing machine learning algorithms and tools for creating complex software in C++ to solve real world problems. It is used in both industry and academia in a wide range of domains including robotics, embedded devices, mobile phones, and large high performance computing environments. Dlib’s open source licensing allows you to use it in any application, free of charge. To follow or participate in the development of dlib subscribe to dlib on github. Also be sure to read the how to contribute page if you intend to submit code to the project. To quickly get started using dlib, follow these instructions to build dlib. 安装cmake库 安装boost库 安装dlib库 Dlib库whl文件下载地址 Dlib相关文件 还可以添加face_recognition库等等。 二、 代码运行样例测试环境配置需求 代码样例 代码运行展示 三、 特征点标定使用openCV与dlib实现人脸68特征点的检测与手动修改 "},{"title":"Latex学习","date":"2022-09-03T15:00:00.000Z","url":"/2022/09/03/Latex-Use/","tags":[["Latex","/tags/Latex/"]],"categories":[["undefined",""]],"content":"一、Markdown与Latex的区别latex是纯学术风格,写paper写书用；markdown是程序员风格,写笔记贴代码片段用 简单说,latex适合长篇.精致,比如数学公式.图片位置调整.表格样式调整.而markdown就是粗线条,简易编辑。本篇文章就是以markdown语法进行编写。 markdown语法的官方教程 二、Latex的背景LaTeX（LATEX，音译“拉泰赫”）是一种基于ΤΕΧ的排版系统，由美国计算机学家莱斯利·兰伯特（Leslie Lamport）在20世纪80年代初期开发，利用这种格式，即使使用者没有排版和程序设计的知识也可以充分发挥由TeX所提供的强大功能，能在几天、甚至几小时内生成很多具有书籍质量的印刷品。对于生成复杂表格和数学公式，这一点表现得尤为突出。因此它非常适用于生成高印刷质量的科技和数学类文档。这个系统同样适用于生成从简单的信件到完整书籍的所有其他种类的文档。 Latex新手指南：The Short Introduction to LaTeX2e (Chinese Simplified) 三、Latex语法基础：命令与环境1.命令与环境命令什么是命令 不同于其他编程语言(C/C++, Python等)会使用关键词,函数和类来实现程序,Latex语法中大多以命令的形式存在而每一个命令都会有具体的功能,如标题制作,目录制作,或者是设置文档的编码格式等等 命令的格式Latex中的命令具体有以下三种形式: Latex语法举例 环境什么是环境我们编写的文档内容为了避免代码和我们书写的正文内容混杂,我们从夹杂代码和正文内容的纯文本的连续上下文中划分出部分空间,这部分空间将作为独立的整体,我们可以将正文内容写在这些空出来的部分中 对于这些空间,我们可以提前使用一些命令,设定好这些空间的环境,即设定好这些空间的环境,即编译好之后最终的展示格式,因此环境其实就是我们为特定部分的正文设定的格式,这个特定部分可以是后面的全文,也可以是正文的部分,例如一段话,一张图片等等 例如下面的例子(\\\\\\表示换行) 正如代码中提到的,我们的命令其实可以分为两种,第一种是产生效果的命令,例如制作标题的\\maketitle,另外一种就是对后文添加环境的命令 环境的作用和作用域就像诸多编程语言中变量具有作用域,我们的环境其实也具有作用域,具体来说就是环境可以影响的正文的范围 有的环境的作用域是全文,例如上面例子中一开始的\\documentclass[UTF8]{article}就规定了文章的体裁是论文,使用的编码方式是UTF-8,其作用域就是全文. 接下来的\\usepackage{ctex}指明我们将使用ctex这个包/宏包. 包/宏包类似于编程语言的库,库中有很多写好的函数给我们使用,宏包中也有很多能够产生各种效果的命令.这里使用的ctex宏包的一个功能就是提供中文的显示和排版.所以\\usepackage{ctex}这个命令的作用域也是全文. \\begin和\\end命令用于开始和结束特定的环境,我们这里使用的\\begin{document}和\\end{document}就是开始和结束名为document的环境,两个命令之间的内容都将受到环境document的作用. document的作用就是展示文档, 在document环境的作用域范围内的内容在编译后都将会展示出来.在document环境内的内容才会被展示, 而在document环境外的内容不会被展示 2.隔离上下文接下来我们继续向下看刚才的例子,我们用上面的例子引出 隔离上下文 我们编写了这样一句话&#123;\\textbf &#123;this&#125;&#125;,命令\\textbf的作用是加粗处于其作用域的文字. 我们首先看下\\textbf的作用域 我们能够发现第一句仅加粗了t,而第二句加粗了整句话,而从\\textbf这句话本身来看,其作用域就是其后紧接着的一个字符(注意由于其只能作用一个字符,因此如果我们加粗的第一个字符是中文的话就会报错,因此中文汉字作为第一个字符就会报错) 在正式编写的时候,我们通常将不同的段落放在不同的&#123; &#125;中来管理环境,例如(空行表示另起一段) 四、Latex语法基础：Latex文件格式简介Latex基本文件的基本结构 # $ % ^ &amp; _ { } \\ ——这些字符(reserved characters)在Latex中有特殊的意义，要想在生成的文档中显示这些字符，Latex文档中这些字符前加反斜杠”\\\\’’ $\\backslash$ 输出反斜杠(因为两个反斜杠”\\\\”在Latex中是换行命令) ~波浪线(tidle)在LaTeX中是插入空格命令，可用数学公式环境的$ \\sim $向TeX文档中插入波浪线. eg.如果在封面不标记页码，目录页使用小写罗马数字标记页码，正文部分使用阿拉伯数字标记页码 版面设置段落行间距 段落间空白 居中 左、右对齐 样例 verbatim 居中显示及其作用verbatim环境(抄录环境)使LaTex源文件的内容原样呈现于最终文档。这些内容不受center, flushleft, flushright等命令的影响。若想让verbatim环境中内容居中显示，需要使用 verbatimbox等扩展包. 线框使用\\fbox给文字加线框; 使用\\parbox给段落添加线框. 样例 图片插入及引用 表格 表格中行间距 使用\\noalign{\\smallskip} 也可以改变行间距. \\smallskip 命令等价于\\vspace{smallskipamount}。smallskipamount的大小依赖于文档类型. 与\\smallskip 命令类似的还有\\medskip,\\bigskip . 另外，也可以在每一行尾的 换行符 后设置行间距。 三线表样例Latex中默认的线条宽度是0.4pt, 如果想要使用粗一点的线条，可以使用 booktabs环境包. 这需要在Latex文档的导言部分添加命令: 示例 "},{"title":"山东大学第八届“可视计算”","date":"2022-07-11T12:50:00.000Z","url":"/2022/07/11/220710/","tags":[["CV","/tags/CV/"]],"categories":[["undefined",""]],"content":"基于二维图像集合的三维内容学习与生成童欣（xtong@microsoft.com) Deep learning has demonstrated its great advantages in many fields Unified network architecture for different representations and tasks Compact non-linear approximator for arbitrary functions Learning 3D representations from 2D image collections Exploiting huge amount of images/video datasets Learn high quality 3D representation from realistic images From image to 3D representation is a highly ill-posed problem No multiple views and no correspondence between images Two SolutionsLearning by synthesis Learning a network to infer underlying 3D representation from each 2D image The rendering of the derived 3D representation should match the input 2D image How to solve the ill-posed inverse rendering problem? Learning by discrimination Learning a generative model of the space of 3D representations from 2D images. The rendering of generated 3D representation from each view should be indistinguishable from 2D real. How to minimize the quality gap between synthesized images and real ones and preserve 3D consistency? 采用多幅图像对三维人脸进行生成 3D scene layout generation from RGB/RGBD image collections(Mingjia Yang, Yuxiao Guo, Bin Zhou, Xin Tong, Indoor Scene Generation from a Collection of Semantic-segmented Depthemages,ICCV 2021.) 3D object texture generation from real image collections(Rui Yu, Yue Dong, Pieter Peers, Xin Tong, Learning Texture Generators for 3D Shape Collections from internet Photo Sets, BMVC 2021.) 陈宝权教授的三位讲者报告： Simulation and Optimization of Magneto elastic Thin Shells JOINT NEURAL PHASE RETRIEVAL AND COMPRESSION FOR ENERGY-AND COMPUTATION-EFFICIENT HOLOGRAPHY ON THE EDGE 复杂三维形状的生成、分析与制造轻质与可控弹性耦合的微几何结构仿真设计与优化 基于深度学习的无标签高泛化的快速均质化方法 面向制造的高保真几何结构优化： 提出了制造约束与复杂几何结构的共分析方法，解决增材制造中因打印精度、局部重力、分块拼接等造成的几何形状走样，克服了大体积或高度复杂结构如植物、毛发等在制造中的失真问题。 三维物体跟踪与增强现实（钟凡）基本原理 PnP(Perspective-n-Points)问题 SIFT基于局部特征得到点对应 无纹理物体——利用轮廓形状信息（边缘信息与图像信息） 三维位姿优化 混合优化方法：只对面外旋转采用非局部搜索 可视化BoxAR tableau Canis - A Language for Data-Driven Chart Animations Deep Dive: Sampling for Progressive Visualization  Deformation of 3D Printed Soft Robots: Sensing, Simulation and Control(Charlie C.L.Wang)多模态信息感知的SLAM特征点优化的相机与激光外参数标定 两阶段视角不变匹配法的激光SLAM 基于凸优化与IMU-KLT线段跟踪的点线SLAM SLAM特征跟踪中存在问题:在弱纹理、重复纹理、运动模糊、光照变化下，鲁棒性和精度较低基于线的跟踪速度较慢 解决思路∶ 提出了点线特征混合策略:Point line with shorter tracks are categorized into “MSCKF”features and with longer tracks into ““SLAM”features.Especially，“SLAM” lines are added into the state vector toimprove accuracy of the proposed system. 提出了基于L1-范数图优化的线段跟踪 实验效果优于VINS-Mono，PL-VINS，OpenVINS 在运动模糊和弱纹理场景下，实验效果优于ORB-SLAM3. 基于深度哈希学习的SLAM闭环检测与位姿误差纠正基于随机森林学习与二值描述子学习相融合的并行搜索SLAM重定位软硬协同SLAMI展示和展望 严肃化和泛化的游戏引擎技术游戏化的xr工作——GritWorld 异构维度数据可视化应用的高效一体化构建方法（HUAWEI） 1.Aladdin定位:面向WEB应用的端到端智能无代码人机协同系统构建平台 生成既有静态页面、交互、业务逻辑与数据表达的全场景WEB应用 彻底打通从需求到设计到开发的全链路 2.Aladdin二维组件编辑方案:声明式语法、交互构建界面 3.Aladdin UVE Kernel三维编辑器 Taichi编程语言 作用：在图上优化代码 OpenGL中文文档"},{"title":"Operating System","date":"2022-06-20T15:31:28.000Z","url":"/2022/06/20/operating-system/","tags":[["Undergraduate Courses","/tags/Undergraduate-Courses/"]],"categories":[["Undergraduate Courses","/categories/Undergraduate-Courses/"]],"content":"第一章 操作系统引论OS的基本特征和主要功能OS的基本特征：并发性、异步性，虚拟性，共享性 OS的主要功能：处理机管理功能、设备管理功能、存储器管理功能、文件管理功能、操作系统与用户之间的接口 第二章 进程的描述和控制（CPU）用信号量机制实现进程并发中的同步与互斥理发师问题（PPT p20—p30） 线程是什么？ 线程是现代操作系统引入的一种执行实体 线程又称‘’轻型进程‘，是进程的组成部分 进程是资源占有单位，线程只是CPU调度单位 多线程结构进程的优点 快速线程切换 通信易于实现 减少管理开销 并发程度很高 进程与线程的区别： 进程：独立的实体单位 独立占有资源，进程拥有对资源的控制权和所有权 独立参与调度/执行，进程是一个可被操作系统调度和分派的单位 线程：调度运行的单位；不单独占有资源的单位，共享其所属进程的资源 第三章 处理机调度与死锁（CPU）银行家算法、死锁检测（资源有向图）死锁检测（资源有向图）PPT p37—p40 银行家算法 第四章 存储器管理（内存储器）带有快表机制的内存访问时间计算 第五章 虚拟存储器（内存储器）页面置换算法(LRU)第六章 输入/出系统（设备管理）中断与异常 简答 缓冲区技术磁盘调度算法FCFS,SSTF,SCAN,CSACNPPT p64—p73 第七章 文件管理（磁盘文件管理）文件的逻辑结构和物理结构 文件的逻辑结构：流式结构、记录式结构（用户使用角度）流式文件是指文件内的数据是一个完整的字符流，不可以进一步细分 记录式文件在逻辑上可看成是一组记录的集合。每个几里路又彼此相关的若干数据项组成。 文件的物理结构：顺序结构、链式结构、索引结构（操作系统存储角度）连续存储结构文件体在磁盘上占用连续的存储空间 非连续存储结构文件提在磁盘上占用不连续的存储空间（连接存储（隐式链接存储、显式连接存储）、索引存储） 采用多级索引的方式文件系统最大允许长度的计算PPT p165—p179 第八章 磁盘存储器的管理（磁盘文件管理）第九章 操作系统的接口操作系统接口：操作系统中有专门相应用户控制要求的接口，负责系统与用户之间的双向信息传送。 联机命令接口：1.命令行方式操作接口（键盘命令）2.图形化界面（鼠标点击） 程序级接口——系统调用 操作系统还提供一种适用于应用程序中的功能调用接口，叫做系统调用(system call)，允许用户在自己的应用程序中调用系统中提供的一些功能模块。 系统调用是应用程序获得操作系统服务的唯一途径。 CPU的两种工作状态：管态（执行系统程序的状态，允许执行所有指令）、目态（执行用户程序的状态，只允许执行非特权指令） 系统调用操作系统提供一种适用于应用程序中的功能调用接口，叫做系统调用，允许用户在自己的应用程序中调用系统中提供的一些功能模块。 系统调用分类： 进程和作业管理类 文件操作类 设备管理类 主存管理类 信息维护类 通信类 Urgent临界区——一段代码 ≠ 临界资源 临界区概念：每个进程中访问临界资源的那段程序称之为临界区。临界区不是内核对象，而是系统提供的一种数据结构，程序中可以声明一个该类型的变量，之后用它来实现对资源的互斥访问。当欲访问某一临界资源时，先将该临界区加锁（若临界区不空闲则等待），用完该资源后，将临界区释放。补充（待定）：分类：临界区也是代码的称呼，所以一个进程可能有多个临界区，分别用来访问不同的临界资源。内核程序临界资源：系统时钟普通临界资源：普通I/O设备，如打印机（进程访问这些资源的时候，很慢，会自动阻塞，等待资源使用完成） 内存分配算法——首次适应算法FF、循环首次适应NF、最佳适应BF、最坏适应WF算法 分段与分页区分 缓冲区的引入：速度不一致问题 无论是字符设备还是块设备，它们的运行速度都远低于CPU的速度。为了缓和CPU和I/O设备之间的矛盾、提高CPU的利用率，在现代OS中都无一例外地分别为字符设备和块设备配置了相应的缓冲区。缓冲区有着多种形式，如单缓冲区、双缓冲区、循环缓冲区、公用缓冲池等，以满足不同情况的需要。 为什么抽象为文件：可以用一套统一的API访问设备，排除了设备的特异性。 CPU的两种工作状态：管态（执行系统程序的状态，允许执行所有指令）、目态（执行用户程序的状态，只允许执行非特权指令） 操作系统基本特征 进程的状态的切换 fork() 中断——trap和interrupt的区别 可屏蔽中断：常由计算机的外设或一些接口功能产生，如时钟、键盘、打印机、串行口等，这种类型的中断可以在CPU要处理其它紧急操作时，被软件屏蔽或忽略. 非屏蔽中断：由意外事件导致，如电源断电、内存校验错误等.对于这种类型的中断事件，无法通过软件进行屏蔽，CPU必须无条件响应 外中断和内中断的区别 动态分区管理问题 计算访存次数（隐式链接） 关于死锁问题的资源分配图 文件的索引组织方式 启动磁盘的次数（第八章） 页面置换算法(LRU)与快表结合 不唯一的多缓冲区问题 理发师问题 内部碎片管理方法 缓冲池技术：缓冲池，说到底也是缓冲区，只不过这种缓冲区较其他缓冲区来说能更好的实现cpu并行和设备的利用率。 缓冲池里主要包括如下方面。用于对数据进行读写的公共区域： 1、 emq空缓冲队列：用于取出空缓冲区来进行读写等操作。 2、 inq输入队列：用来记录输入设备输入的数据，以便用户程序读取。 3、outq输出队列：用来记录用户程序输出到设备(显示屏或是文件等)的数据，以便用户程序读取。计算题： 低级调度 页面置换 文件存储空间分配 磁盘访问"},{"title":"Algorithm programming and analysis","date":"2022-06-19T01:59:13.000Z","url":"/2022/06/19/Algorithm-programming-and-analysis/","tags":[["Undergraduate Courses","/tags/Undergraduate-Courses/"]],"categories":[["Undergraduate Courses","/categories/Undergraduate-Courses/"]],"content":"第一章能判断它是一个P,NP问题（涉及复杂度）时间的上界和下界（最好、最坏情况）（通项）若含义：算法在任何实例情况下，其时间复杂性的阶不超过的阶 若含义：算法在任何实例情况下，其时间复杂性的阶不低于的阶 存在有和因此 一、递推方法求递归算法的时间复杂性 二、Master定理方法求递归算法时间复杂性 三、递归树求解递归方程 P、NP问题NP完全性理论 多项式与非多项式时间复杂性 易解的：可在多项式时间内求解 难解的：不可在多项式时间求解 P问题：确定的算法+多项式时间求解+可判定问题（多项式类型） NP问题：不确定的算法+多项式时间求解+可判定问题 NPC问题：NP问题+多项式归约为D 意义：有一个NPC问题找到多项式时间的解法，则全部解决 难问题：难 ：有 第二章 递归与分治循环赛日程表 棋盘覆盖问题 平衡(balancing)子问题的思想：将一个问题分成大致相等的k个子问题，算法最有效。 分治法所能解决的问题一股具有以下四个特征：●该问题的规模缩小到一定的程度就可以容易地解决。（能分）●该问题可以分解为若干个规模较小的相同问题。（规模小）●使用小规模的解可以合并成，该问题原规模的解。（能合）●该问题所分解出的各个子规模是相互独立的。（无重复子问题） 第三章 动态规划动态规划的基本要素： 最优子结构性质 重叠子问题性质 备忘录方法 分治与动态规划的联系和区别： 分治可使用递归和非递归方法，因为没有任何两个子问题是重复的， 所以没有多余的计算量。 动态规划采用自底向上的方法求解问题，每次求解的问题都是不同的 因为是自底向上的方式， 所以在求解一个大问题时，比他规模小的问题的答案已经存在了， 可以直接通过这些小问题的答案去合并为大问题答案即可 动态规划算法会产生至少一张表，就是用来记录答案的，还有一张表（辅助构造最优解的辅助表）不是必须的，有时候直接可通过第一张表就可构造出最优解。 动态规划最后求解的问题是规模最大的问题，也就是我们需要解决的问题 矩阵连乘问题（最大子段和问题） 凸多边形三角剖分 （）；（）（）（） 游艇问题 0-1背包问题（优先队列） 数字塔问题：计算最优解 第四章 贪心算法证明最优子结构和贪心选择性质 背包问题 哈夫曼树的贪心策略，构造编码树 贪心策略： 每次选择两个最低发生频率的结点构造一颗子树的根结点。并把产生的子树的根结点再插入到优先队列中 （按结点的频率值组织为最小优先队列） 贪心选择的基本要素： 贪心选择性质 贪心选择性质：是指所求问题的整体最优解，可以通过一系列局部最优的选择，即贪心选择来达到。 贪心选择性质证明方法：一步步的贪心选择（局部最优）最终导致问题的整体最优解。 最优子结构性质 当一个问题的最优解包含其子问题的最优解时，称此问题具有最优子结构性质。 因为最优解对应最优值，所以通常证明：问题的最优值包含其子问题的最优值。 贪心算法与动态规划算法的差异 性质相同点：贪心算法和动态规划算法都要求问题具有最优子结构性质 性质不同点：贪心算法要求问题具有贪心选择性质，动态规划算法则不要求。 计算方式的不同： 动态规划算法通常以自底向上的方式解各子问题 贪心算法以自顶往下的方式进行，每做一次贪心选择就将问题变为规模更小的子问题。 最优装载问题的证明： 贪心选择性质 设集装箱依其重量从小到大排序，（x1,x2,..xn)是其最优解，xi={0,1}，设xk是第一个等于1的。 （1）如k=1， 则满足贪心选择性质 （2）如k≠1，用x1替换xk，构造的新解同原解最优值相同，故也是最优解，满足贪心选择性质。 最优子结构性质 最优装载问题具有最优子结构性质. 设1至n个集装箱装上船的最大数量为T(1,n,w） 则 T(1,n,w)=1+T(2,n,w-w1) ； 因T(1,n,w)是最优值，则T(2,n,w-w1)一定是最优值。反证法证明之。 反证法 如果T(2,n,w-w1)不是该问题的最优值，则存在最优值 T ‘(2,n,w-w1)&gt; T(2,n,w-w1)，则 1+ T ‘(2,n,w-w1) = T ‘(1,n,w)&gt; T(1,n,w),这与大前提T(1,n,w)是最优值相矛盾， 故T(2,n,w-w1)一定是最优值 第五章 回溯法子集树、排列树 构造解空间 符号三角形问题 n皇后问题（排列树） 0-1背包问题 最大团问题 •解空间：子集树 •可行性约束函数：顶点i到已选入的顶点集中每一个顶点都有边相连。 •上界函数:有足够多的可选择顶点使得算法有可能在右子树中找到更大的团。 着色问题 •解向量：(x1, x2, … , xn)表示顶点i所着颜色xi •可行性约束函数：顶点i与已着色的相邻顶点颜色不重复。 圆排列问题 去镜像排列：1,2,3和3,2,1这种互为镜像的排列具有相同的圆排列长度，只计算一个就够了，可减少约一半的计算量。 方法：保证每个排列的首元素一定小于尾元素即可，排列的元素在本题中是圆的半径 剪枝、可行性约束、限界 用回溯法解装载问题的时间复杂性是O(2^n^)。 第六章 分支限界法回溯法与分支限界法的区别 （1）求解目标： 回溯法求解目标是找出解空间树中满足约束条件所有解 分支限界法的求解目标则是找出满足约束条件的一个解（或一个最优解） （2）搜索方式的不同： 回溯法以深度优先方式搜索解空间树 分支限界法则以广度优先或以最小耗费优先方式搜索解空间树 队列与优先队列的区别 队列式：活结点表是一个队列，新扩展出的满足条件的活结点追加在队尾。这里需要加入层次标志（如-1），或记录下层编号在活结点中。 优先队列式：活结点表是优先队列，一般使用堆（大顶堆或小顶堆）存储，优点是只需要O（logn）时间复杂性完成插入或者删除（取堆顶结点，即优先级最高的结点） 构造解方法， 活结点通过记录其父节点地址，及左孩子标志去，在找到最优值时，回溯方法找到最优解。也可在扩展出的结点中记录构造的解，如问题规模较大时，应考虑压缩存储。 分支限界法的剪枝方法： （1）对于子集树，左右分支剪枝策略不同， （2）对于排列树，n叉树， 剪枝策略是相同的。 算法的结束控制： （1）队列式分支限界， 活结点表为空 （2）优先队列式，叶子结点成为扩展结点(在确认后面的活结点不存在更好的解)或队列为空。通常叶子结点加入优先队列中。 装载问题 最优解构造 布线问题 0-1背包问题 第七章 概率算法数值随机化算法常用于数值问题的求解，得到的往往是近似解，且近似解的精度随计算时间的增加而不断提高。 蒙特卡罗算法(Monte Carlo)用于求问题的准确解。但这个解未必是正确的。其求得正确解的概率依赖于算法所用的时间，算法所用的时间越多，得到正确解的概率就越高。缺点：一般情况下，无法有效地判断得到的解是否肯定正确。（1主元素问题、素数问题、） 重复调用一个一致的、p正确的、偏y~0~蒙特卡罗算法k次，可得到一个1-(1-p)^k^正确的蒙特卡罗算法，且所得算法仍是一个一致的偏y~0~蒙特卡罗算法。 换句话说：如果蒙特卡洛对同一个实例，连续运行k次， 答案都是一样的， 则这个答案正确的可能性是 1-(1-p)^k^。 如k足够大，则该答案的正确性就足够高 设p是一个实数，且1/2&lt;p&lt;1。如果一个蒙特卡罗算法对于问题的任一实例得到正确解的概率不小于 p，则称该蒙特卡罗算法是p正确的，且称p-1/2是该算法的优势。 如果对于同一实例，蒙特卡罗算法不会给出2个不同的正确解答，则称该蒙特卡罗算法是一致的。对于一个解所给问题的蒙特卡罗算法MC(x)，如果存在问题实例的子集X使得：(1)当x不属于X时，MC(x)返回的解总是正确的；(2)当x∈X时，正确解是y0，但MC(x)返回解未必是y0。称MC(x)是偏y0的算法。 拉斯维加斯算法(Las Vegas)不会得到不正确的解。但有时找不到解。（n皇后） 舍伍德算法(Sherwood)总能求得问题的一个解，且求得的解总是正确的。精髓：不是避免算法的最坏情形行为，而是设法消除这种最坏情形与特定实例之间的关联性。（线性时间选择算法、快速排序算法、跳跃表）"},{"title":"Computer composition","date":"2022-06-18T15:30:30.000Z","url":"/2022/06/18/Computer-composition/","tags":[["Undergraduate Courses","/tags/Undergraduate-Courses/"]],"categories":[["Undergraduate Courses","/categories/Undergraduate-Courses/"]],"content":"基础概念计算机体系结构是指那些能够被程序员所见到的计算机系统的属性，即概念性的结构与功能特性。 计算机组成是指如何实现计算机体系结构所体现的属性，它包含了许多对于程序员来说是透明的硬件细节。 计算机硬件的主要技术指标：1.机器字长，2.存储容量，3.运算速度（MIPS百万条指令每秒或CPI执行一条指令所需的时钟周期） 计算机的发展：1.第一代电子管计算机 2.第二代电子管计算机 3.第三代集成电路计算机 按照在计算机系统中的作用不同：存储器可分为主存储器，辅助存储器，缓存存储器 存储器的层次结构：缓存-主存（解决CPU和主存速度不匹配的问题） 主存-辅存（解决存储系统容量的问题） 主存的技术指标：存储容量、存储速度、存储器带宽（会算带宽） 动态RAM的三种刷新方式及特点：1.集中刷新 2.分散刷新 3.异步刷新 提高访存速度的措施：1.单体多字系统 2.多体并行系统 p106计算 p117 例题8、9、10、11 对设备的编制方式： 简答1、总线仲裁有哪几种？各有什么特点？总线仲裁分为集中仲裁方式和分布式仲裁方式 集中仲裁方式：将控制逻辑集中在一处，分为链式查询，计数器定时查询和独立请求方式链式查询方式：总线上所有部件共用一根总线请求线，只需要很少几根线就能按一定的优先次序实现总线控制，易扩充，但对电路故障很敏感，优先级别低很难获得请求 计数器定时查询方式：采用一个计数器控制总线使用权，少了一根总线响应线，多了一组设备地址线，可以更改设备的优先次数，对电路故障不如链式查询敏感。但控制电路更加复杂 独立请求方式：每台设备均有一对总线请求线BR和总线同意线BG，响应速度快，优先次序控制灵活，控制线数量多。 分布式仲裁方式：不需要中央仲裁器，每个潜在的主模块有自己的仲裁号和仲裁器。2、DMA的请求步骤，工作过程三部分DMA的数据传送过程分为预处理、数据传送和后处理三个阶段。 预处理：CPU执行几条输入输出指令完成，完成程序的初始化。 数据传送：DMA以数据块为单位，以周期挪用的方式传送（以输入为例）设备准备好一个字，发出选通信号，读到DMA的数据缓冲寄存器BR设备向DMA接口发请求DMA接口向CPU申请总线控制权CPU发回HLDA信号，CPU将总线控制权交给DMA将主存地址送到地址总线将数据缓冲寄存器的内容送数据总线将信息送至地址总线指定的存储单元修改内容 3、RAM和ROM的特点 RAM的特点是可以随机读写，访问速度快，但断电后信息会丢失的特性 ROM的特点是只能读出而不能写入信息，ROM具有断电后信息不丢失的特性 4、Cache的作用1.避免CPU空等现象 2.解决CPU和主存之间速度的差异，从而解决速度与成本的矛盾 5、中断服务程序的流程1.保护现场，保存程序的断点，保存通用寄存器和状态寄存器的内容，存入寄存器和堆栈中保存 2.中断服务，对于不同的中断请求源,其中断服务操作内容是不同的, 3.恢复现场，在退出服务程序前,将原程序中断时的“现场”恢复至原来的寄存器中。可用取数指令或出栈指令(POP),将保存在存储器(或堆栈)中的信息送回到寄存器中。4.中断返回，程序返回到原程序的断点，继续执行原程序 6、影响流水线性能的因素结构相关：多条指令进入流水线后，硬件资源是否满足指令重叠执行的要求 数据相关：流水线中各条指令重叠操作，是否导致改变操作数的读写访问顺序 控制相关：转移指令破坏流水线的连续流动。 7、中断服务子程序的工作流程中断请求 中断屏蔽 中断响应 保护现场 恢复现场 8、双重中断的概念以及实现条件当CPU正在执行某个中断服务程序时，又来了一个新的中断请求，CPU响应了新的中断请求，暂停当前运行的程序，专区执行新的服务程序 成为双重中断。 条件：提前设置“开中断”指令。优先级别高的中断源有权中断优先级别低的中断源 概念cpu+存储器 = 主机 运算器+控制器 = CPU 第三章总线分类按数据传送方式： 并行传输总线和串行传输总线 按传输数据带宽：8，16，32，64位等传输总线 按连接部件： 片内总线（CPU 寄存器之间） 系统总线分为三类： 数据总线（双向，与机器字长、存储字长有关）， 地址总线：单项传输，位数与存储单元的个数有关， 控制总线：单根单向，多根双向； 通信总线：串行通信（远距离）和并行通信（近距离） 总线特性：机械 电器 功能 时间 （特性） 第四章存储器分类 按存储介质分为：半导体器件，磁性材料和光盘等 按存取方式分类：随机存储器、只读存储器、顺序存取存储器和直接存取存储器 按计算机作用：主存储器 辅助存储器和缓冲存储器 （速度 容量 位价） 层次结构：寄存器 缓存 主存 辅存 存储容量 = 存储单元个数 * 存储字长 静态RAM电源掉电后，原存信息丢失 故它属易失性半导体存储器 动态RAM刷新（刷新与行地址有关） 刷新方式： 集中刷新（有死区）和分散刷新（无死区）以及异步刷新（结合前两种）方式 讲解 断电后,RAM内保存的信息会丢失,ROM则可长期保存而不会丢失 双端口存储器双端口存储器是指同一个存储器具有两组相互独立的读写控制线路，是一种高速工作存储器；它提供了两个相互独立的端口，即左端口右端扣。他们分别具有各自的地址线、数据线和控制线，可以对存储器中任何位置上的数据进行独立的存取操作。可以实现一个存储器挂2个CPU。 当两个端口地址不相同时，在两个端口上进行读写操作，一定不会发生冲突。当任一端口被选中驱动时，就可以对整个存储器进行存取，每一个端口都有自己的片选控制和输出驱动控制 当两个端口同时存取存储器同一存储单元时，便发生冲突。为解决此问题，特设置了BUSY标志。 两个端口对同一主存操作有以下4种情况：1，两个端口不同时对同一地址单元存取数据 √2，两个端口同时对同一地址单元读出数据 √3，两个端口同时对同一地址单元写入数据 （×写入错误）4，两个端口同时对同一地址单元，一个写入数据，一个读出数据 （×读出错误） 解决办法：加一个控制信号，忙信号（如图所示）。 置“忙”信号为0，由判断逻辑决定暂时关闭一个端口（即被延时），未被关闭的端口正常访问，被关闭的端口延长一个很短的时间段后再访问。 提高访问速度的措施： 1.使用高速元件 2.采用层次结构 3.调整主存结构 单体多字 多体并行 第五章IO设备编址方式： 统一编址：将IO地址看做存储器地址的一部分，占用存储空间，减少了主存容量，无须专用IO指令 不统一编址：IO地址和存储器地址分开的，不占用主存空间，不影响主存容量，需设IO专用指令 设备寻址方式： 传送方式 联络方式 IO设备与主机的连接方式 中断向量 是中断服务程序的入口地址。 CPU响应中断的时间 一条指令结束从发出中断请求到进入中断处理所用的时间 第七章 指令的一般格式：操作码和地址码。操作码的位数决定了指令的种类数。 操作码的长度可以是固定的，也可以是变化的。 扩展操作码：较短的操作码不可以是较长的操作码的前缀，否则无法分清到底是几地址指令 指令字长取决于：操作码的长度、操作数的地址和操作数地址的长度。 寻址方式可以分为 数据寻址和指令寻址两种 指令寻址分为：顺序寻址和跳跃寻址 数据寻址分为： 不访存：立即寻址，隐含寻址，寄存器寻址，访存：直接寻址，间接寻址，寄存器间接寻址，基址寻址，变址寻址，相对寻址，堆栈寻址 各特点理解 RISC技术 RISC的主要特点 P330RISC与CISC的比较 P332 第八章为了方便起见，假设流水线由5段组成，他们分别是取指令（IF）、指令译码/读寄存器（ID）、执行/访存有效地址计算（EX）、存储器访问（MEM）、结果写回寄存器（WB） 分析计算题四个36分总线计算DMA接口计算浮点运算加减法cache映射 分析题24分寻址方式指令格式数据通路的题"},{"title":"吃瓜教程2","date":"2022-06-17T02:00:00.000Z","url":"/2022/06/17/pumpkin-book-2/","tags":[["ML","/tags/ML/"]],"categories":[["ML","/categories/ML/"]],"content":"第三章 线性模型3.1 基本形式 线性模型形式简单、易于建模,但却蕴涵着机器学习中一些重要的基本思想 .许多功能更为强大的非线性模 型(nonlinear model)叫可在线性模型的基础上通过引入层级结构或高维映射而得 .此 外，由于w直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释性（comprehensibility） 3.2 线性回归算法原理 线性回归 (linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记. 3.3 对数几率回归3.4 线性判别分析3.5 多分类学习3.6 类别不平衡问题3.7 阅读材料"},{"title":"吃瓜教程1","date":"2022-06-14T07:00:00.000Z","url":"/2022/06/14/pumpkin-book-1/","tags":[["ML","/tags/ML/"]],"categories":[["ML","/categories/ML/"]],"content":"第一章 绪论1.1 引言机器学习致力于研究如何通过计算的手段，利用经验来改善系统自身的性能.在计算机系统中，“经验”通常以 “数据”形式存在，因此，机器学习所研究的主要内容，是关于在计算机上从数据中产生“模型 “ （model）的算法，即 “学习算法”（learning algorithm）. 有了学习算法，我 们把经验数据提供给它，它就能基于这些数据产生模型；在面对新的情况时（例如看到一个没剖开的西瓜），模型会给我们提供相应的判断（例如好瓜）。 1.2 基本术语数据集(data set) 示例、样本(instance,sample) 属性、特征(attribute,feature) 属性值(attribute value) 样本空间(sample space) 特征向量(feature vector) 离散值——分类 (classification) 连续值——回归(regression) 两个类别——二分类 多个类别——多分类 1.3 假设空间归纳(induction)与演绎(deduction)是科学推理的两大基本手段.前者是从 特殊到一般的“泛化”(generalization)过程，即从具体的事实归结出一般性规 律；后者则是从一般到特殊的“特化”(specialization)过程，即从基础原理推演 出具体状况.例如，在数学公理系统中，基于一组公理和推理规则推导出与之 相洽的定理，这是演绎；而 “从样例中学习”显然是一个归纳的过程，因此亦称 “归纳学习 ”(inductive learning). 第二章 模型评估与选择2.1 经验误差与过拟合通常我们把分类错误的样本数占样本总数的比例称为“错误率”(error rate ),即如果在m个样本中有a 个样本分类错误，则错误率E = a/m ;相应的, 1-a /m 称为 “精度”(accuracy),即 “精度= 1-错误率”.更一般地,我们把学习器的实际预测输出与样本的真实输出之间的差异称为“误差”(error), 学习器在训练集上的误差称为 “训练误差 ”(training error)或 “经验误差”(empirical error),在新样本上的误差称为 “泛化误差”(generalization error). 2.2 评估方法通常，我们可通过实验测试来对学习器的泛化误差进行评估并进而做出选 择.为此 需使用一个“测试集”(testing set)来测试学习器对新样本的判别能力，然后以测试集上的“测试误差”(testing error)作为泛化误差的近似.通常，我们假设测试样本也是从样本真实分布中独立同分布采样而得.但需注意的是，测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过. 2.2.1 留出法 2.2.2 交叉验证法 2.2.3 自助法 2.2.4 调参与最终模型大多数学习算法都有些参数(parameter)需要设定，参数配置不同，学得模型的性能往往有显著差别.因此,在进行模型评估与选择时，除了要对适用学习算法进行选择，还需对算法参数进行设定，这就是通常所说的“参数调节”或简称 “调参 “ (parameter tuning). 2.3 性能度量对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需 要有衡量模型泛化能力的评价标准，这就是性能度量(performance measure).性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果；这意味着模型的“好坏”是相对的，什么样的模型是好的，不仅取决于算法和数据，还决定于任务需求. 2.3.1 错误率与精度 2.3.2 查准率、查全率与F1 "},{"title":"Craniofacial identity authentication(颅面身份认证)","date":"2022-05-10T14:42:32.000Z","url":"/2022/05/10/platform/","tags":[["Craniofacial Registration","/tags/Craniofacial-Registration/"]],"categories":[["Craniofacial Registration","/categories/Craniofacial-Registration/"]],"content":"背景计算机颅像重合与颅骨重建技术至今发展多年，研究内容涉及模式识别、解剖医学、计算机视觉和法医人类学等多个学科领域，毫无疑问已成为国内外研究的热点，近年来多位国内外学者致力于颅像重合与颅骨重建的研究。 颅面身份认证是一项针对颅骨重建与颅像重合的研究，在公安法医学，考古学，医学整形等诸多领域有普遍应用，传统手工颅骨重建与颅面复原耗费时间长且结果难以实现，近年来逐步兴起的计算机辅助颅骨重建与颅像重合的方法使得结果更加真切，高效。 基于深度学习预测特征点的三维颅面配准(TPS)[1]徐亚丽. 基于深度学习的颅面特征点标定与配准[D].青岛大学,2021.DOI:10.27262/d.cnki.gqdau.2021.002285.] 首先，使用深度学习网络来训练特征点检测模型，利用训练好的网络模型预测二维坐标，最后映射回三维数据；然后,将预测得到的三维特征点作为配准的控制点应用到 TPS 配准中。 基于深度学习预测特征点的 TPS 配准一共包括以下几个步骤：首先利用基于深度图的三维面貌特征点自动标定网络预测三维目标面貌数据的 7 个特征点；利用基于信息图的三维颅骨特征点自动标定网络预测三维目标颅骨数据的 8 个特征点。其次，确定参考面貌和颅骨数据上的特征点。第三，使用目标数据上预测的特征点以 及参考数据上标记好的特征点作为控制点来进行 TPS 变换，即将参考颅面向目标颅面变换。最后，在目标颅面上寻找每个参考颅面上点的最近点，得到最终的配准结果。 颅面复原方法综述[1]王琳,赵俊莉,段福庆,周明全.颅面复原方法综述[J].计算机工程,2019,45(12):8-18.DOI:10.19678/j.issn.1000-3428.0053279. 基于知识的颅面复原方法基于稀疏软组织厚度的颅面复原方法对 照 匹 配 法 ( 显式的稠密软组织复制方法)模 板 变 形 法 ( 隐式的稠密软组织复制方法)统计模型法基于最小二乘拟合的颅面统计复原方法基于后验概率最大的颅面统计复原基于层次化颅面统计模型的复原基于统计回归的颅面复原.OBJ文件读取(matlab) "},{"title":"Git的操作与使用","date":"2022-04-24T02:25:37.000Z","url":"/2022/04/24/git-use/","tags":[["Git","/tags/Git/"]],"categories":[["Git","/categories/Git/"]],"content":"GitGit是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理。也是Linus Torvalds为了帮助管理Linux内核开发而开发的一个开放源码的版本控制软件。 软件平台：Windows,Mac OS,Linux/Unix Git常用命令及方法大全 Git专用名词 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 本地分支关联远程 关于gitee（码云）的远程仓库链接（https） 首先输入命令，链接自己的远程https库 $ git remote add origin  使用拉取命令拉取库中数据 $ git pull --rebase origin master 拉取分支，提交代码等命令在下方有写 最后在完成后提交分支 一，新建代码库 # 在当前目录新建一个Git代码库 $ git init # 新建一个目录，将其初始化为Git代码库 $ git init [project-name] # 下载一个项目和它的整个代码历史 $ git clone [url] 二，配置Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 # 显示当前的Git配置 $ git config --list # 编辑Git配置文件 $ git config -e [--global] # 设置提交代码时的用户信息 $ git config [--global] user.name &quot;[name]&quot; $ git config [--global] user.email &quot;[email address]&quot; 三，增加/删除文件 # 添加指定文件到暂存区 $ git add [file1] [file2] ... # 添加指定目录到暂存区，包括子目录 $ git add [dir] # 添加当前目录的所有文件到暂存区 $ git add . # 添加每个变化前，都会要求确认 # 对于同一个文件的多处变化，可以实现分次提交 $ git add -p # 删除工作区文件，并且将这次删除放入暂存区 $ git rm [file1] [file2] ... # 停止追踪指定文件，但该文件会保留在工作区 $ git rm --cached [file] # 改名文件，并且将这个改名放入暂存区 $ git mv [file-original] [file-renamed] 四，代码提交 # 提交暂存区到仓库区 $ git commit -m [message] # 提交暂存区的指定文件到仓库区 $ git commit [file1] [file2] ... -m [message] # 提交工作区自上次commit之后的变化，直接到仓库区 $ git commit -a # 提交时显示所有diff信息 $ git commit -v # 使用一次新的commit，替代上一次提交 # 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend [file1] [file2] ... 五，分支 # 列出所有本地分支 $ git branch # 列出所有远程分支 $ git branch -r # 列出所有本地分支和远程分支 $ git branch -a # 新建一个分支，但依然停留在当前分支 $ git branch [branch-name] # 以远程分支为基础新建一个分支，并切换到该分支 $ git checkout -b [branch] origin/[remote-branch] # 新建一个分支，指向指定commit $ git branch [branch] [commit] # 新建一个分支，与指定的远程分支建立追踪关系 $ git branch --track [branch] [remote-branch] # 切换到指定分支，并更新工作区 $ git checkout [branch-name] # 切换到上一个分支 $ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间 $ git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支 $ git merge [branch] # 选择一个commit，合并进当前分支 $ git cherry-pick [commit] # 删除分支 $ git branch -d [branch-name] # 删除远程分支 $ git push origin --delete [branch-name] $ git branch -dr [remote/branch] 六，标签 # 列出所有tag $ git tag # 新建一个tag在当前commit $ git tag [tag] # 新建一个tag在指定commit $ git tag [tag] [commit] # 删除本地tag $ git tag -d [tag] # 删除远程tag $ git push origin :refs/tags/[tagName] # 查看tag信息 $ git show [tag] # 提交指定tag $ git push [remote] [tag] # 提交所有tag $ git push [remote] --tags # 新建一个分支，指向某个tag $ git checkout -b [branch] [tag] 七，查看信息 # 显示有变更的文件 $ git status # 显示当前分支的版本历史 $ git log # 显示commit历史，以及每次commit发生变更的文件 $ git log --stat # 搜索提交历史，根据关键词 $ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件 $ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名 $ git log --follow [file] $ git whatchanged [file] # 显示指定文件相关的每一次diff $ git log -p [file] # 显示过去5次提交 $ git log -5 --pretty --oneline # 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn # 显示指定文件是什么人在什么时间修改过 $ git blame [file] # 显示暂存区和工作区的差异 $ git diff # 显示暂存区和上一个commit的差异 $ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异 $ git diff HEAD # 显示两次提交之间的差异 $ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码 $ git diff --shortstat &quot;@&#123;0 day ago&#125;&quot; # 显示某次提交的元数据和内容变化 $ git show [commit] # 显示某次提交发生变化的文件 $ git show --name-only [commit] # 显示某次提交时，某个文件的内容 $ git show [commit]:[filename] # 显示当前分支的最近几次提交 $ git reflog 八，远程同步 # 下载远程仓库的所有变动 $ git fetch [remote] # 显示所有远程仓库 $ git remote -v # 显示某个远程仓库的信息 $ git remote show [remote] # 增加一个新的远程仓库，并命名 $ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并 $ git pull [remote] [branch] # 上传本地指定分支到远程仓库 $ git push [remote] [branch] e.g. $ git push origin master # 强行推送当前分支到远程仓库，即使有冲突 $ git push [remote] --force # 推送所有分支到远程仓库 $ git push [remote] --all $ git stash 备份当前的工作区的内容，从最近的一次提交中读取相关内容，让工作区保证和上次提交的内容一致。同时，将当前的工作区内容保存到暂存区中。 $ git pull 拉取服务器上的代码到本地。 $ git stash pop 从暂存区读取最近一次保存的内容，恢复工作区的相关内容。由于可能存在多个Stash的内容，所以用栈来管理，pop会从最近的一个stash中读取内容并恢复。 $ git stash list 显示暂存区中的所有备份，可以利用这个列表来决定从那个地方恢复。 $ git stash clear 清空暂存区。 九，撤销 # 恢复暂存区的指定文件到工作区 $ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit] [file] # 恢复暂存区的所有文件到工作区 $ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致 $ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset --keep [commit] # 新建一个commit，用来撤销指定commit # 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit] # 暂时将未提交的变化移除，稍后再移入 $ git stash $ git stash pop 十，其他 # 生成一个可供发布的压缩包 $ git archive Git分支管理策略一，主分支Master 首先，代码库应该有一个、且仅有一个主分支。所有提供给用户使用的正式版本，都在这个主分支上发布。 Git主分支的名字，默认叫做Master。它是自动建立的，版本库初始化以后，默认就是在主分支在进行开发。 二，开发分支Develop 主分支只用来分布重大版本，日常开发应该在另一条分支上完成。我们把开发用的分支，叫做Develop。 这个分支可以用来生成代码的最新隔夜版本（nightly）。如果想正式对外发布，就在Master分支上，对Develop分支进行”合并”（merge）。 Git创建Develop分支的命令： git checkout -b develop master 将Develop分支发布到Master分支的命令： 切换到Master分支 ​ git checkout master 对Develop分支进行合并 git merge —no-ff develop 这里稍微解释一下，上一条命令的—no-ff参数是什么意思。默认情况下，Git执行”快进式合并”（fast-farward merge），会直接将Master分支指向Develop分支。 使用—no-ff参数后，会执行正常合并，在Master分支上生成一个新节点。为了保证版本演进的清晰，我们希望采用这种做法。关于合并的更多解释，请参考Benjamin Sandofsky的《Understanding the Git Workflow》。 三，临时性分支 前面讲到版本库的两条主要分支：Master和Develop。前者用于正式发布，后者用于日常开发。其实，常设分支只需要这两条就够了，不需要其他了。 但是，除了常设分支以外，还有一些临时性分支，用于应对一些特定目的的版本开发。临时性分支主要有三种： 功能（feature）分支 预发布（release）分支 修补bug（fixbug）分支 这三种分支都属于临时性需要，使用完以后，应该删除，使得代码库的常设分支始终只有Master和Develop。 四、 功能分支 接下来，一个个来看这三种”临时性分支”。 第一种是功能分支，它是为了开发某种特定功能，从Develop分支上面分出来的。开发完成后，要再并入Develop。 功能分支的名字，可以采用feature-*的形式命名。 创建一个功能分支： git checkout -b feature-x develop 开发完成后，将功能分支合并到develop分支： git checkout develop git merge —no-ff feature-x 删除feature分支： git branch -d feature-x 五、预发布分支 第二种是预发布分支，它是指发布正式版本之前（即合并到Master分支之前），我们可能需要有一个预发布的版本进行测试。 预发布分支是从Develop分支上面分出来的，预发布结束以后，必须合并进Develop和Master分支。它的命名，可以采用release-*的形式。 创建一个预发布分支： git checkout -b release-1.2 develop 确认没有问题后，合并到master分支： git checkout master git merge —no-ff release-1.2 对合并生成的新节点，做一个标签 git tag -a 1.2 再合并到develop分支： git checkout develop git merge —no-ff release-1.2 最后，删除预发布分支： git branch -d release-1.2 六、修补bug分支 最后一种是修补bug分支。软件正式发布以后，难免会出现bug。这时就需要创建一个分支，进行bug修补。 修补bug分支是从Master分支上面分出来的。修补结束以后，再合并进Master和Develop分支。它的命名，可以采用fixbug-*的形式。 创建一个修补bug分支： git checkout -b fixbug-0.1 master 修补结束后，合并到master分支： git checkout master git merge —no-ff fixbug-0.1 git tag -a 0.1.1 再合并到develop分支： git checkout develop git merge —no-ff fixbug-0.1 最后，删除”修补bug分支”： git branch -d fixbug-0.1 版本回退-撤销文件修改 {针对文件修改恢复} 工作区修改一个文件后，又想回到修改前(git add前) 当然可以直接手动再在工作区中将文件修改回去 修改后，通过命令git status查看 $ git status # On branch master # Changes not staged for commit: # (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) # (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) # modified: readme.txt no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 这时Git会告诉你，git checkout — file可以丢弃工作区的修改： Note: \\1. git checkout — file命令中的—很重要，没有—，就变成了“切换到另一个分支”的命令，我们在后面的分支管理中会再次遇到git checkout命令。 \\2. 命令git checkout — readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况： 一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。总之，就是让这个文件回到最近一次git commit或git add时的状态。 \\3. 工作区、暂存区的概念不清楚的可见于Git版本控制教程 - Git本地仓库 如果在工作区中修改了文件还git add到暂存区（但是在commit之前） 用git status查看一下，修改只是添加到了暂存区，还没有提交： $ git status # On branch master # Changes to be committed: # (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) # modified: readme.txt Git同样告诉我们，用命令git reset HEAD file可以把暂存区的修改撤销掉（unstage），重新放回工作区： $ git reset HEAD readme.txt Unstaged changes after reset: M readme.txt git reset命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用HEAD时，表示最新的版本。 再用git status查看一下，现在暂存区是干净的，工作区有修改。 然后丢弃工作区的修改 $ git checkout -- readme.txt $ git status # On branch master nothing to commit (working directory clean) 不但修改了文件还从暂存区提交commit到了版本库 - 版本回退 版本回退可以回退到上一个版本。不过，这是有条件的，就是你还没有把自己的本地版本库推送到远程。Git是分布式版本控制系统。 在工作中对某个文件（如readme.txt）进行多次修改交commit。 可以通过版本控制系统命令告诉我们提交的历史记录，在Git中，我们用git log命令查看： $ git log commit 3628164fb26d48395383f8f31179f24e0882e1e0 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Tue Aug 20 15:11:49 2013 +0800 append GPL commit ea34578d5496d7dd233c827ed32a8cd576c5ee85 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Tue Aug 20 14:53:12 2013 +0800 add distributed commit cb926e7ea50ad11b8f9e909c05226233bf755030 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Mon Aug 19 17:51:55 2013 +0800 wrote a readme file Note: git log命令显示从最近到最远的提交日志，我们可以看到3次提交，最近的一次是append GPL，上一次是add distributed，最早的一次是wrote a readme file。 如果嫌输出信息太多，看得眼花缭乱的，可以试试加上—pretty=oneline参数： $ git log --pretty=oneline 3628164fb26d48395383f8f31179f24e0882e1e0 append GPL ea34578d5496d7dd233c827ed32a8cd576c5ee85 add distributed cb926e7ea50ad11b8f9e909c05226233bf755030 wrote a readme file 你看到的一大串类似3628164…882e1e0的是commit id（版本号），和SVN不一样，Git的commit id不是1，2，3……递增的数字，而是一个SHA1计算出来的一个非常大的数字，用十六进制表示，而且你看到的commit id和我的肯定不一样，以你自己的为准。为什么commit id需要用这么一大串数字表示呢？因为Git是分布式的版本控制系统，后面我们还要研究多人在同一个版本库里工作，如果大家都用1，2，3……作为版本号，那肯定就冲突了。 每提交一个新版本，实际上Git就会把它们自动串成一条时间线。如果使用可视化工具（如GitX、github的客户端、pycharm）查看Git历史，就可以更清楚地看到提交历史的时间线。 现在我们想要把readme.txt回退到上一个版本 如“add distributed”的那个版本，怎么做呢？首先，Git必须知道当前版本是哪个版本，在Git中，用HEAD表示当前版本，也就是最新的提交3628164…882e1e0（注意我的提交ID和你的肯定不一样），上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 现在，我们要把当前版本“append GPL”回退到上一个版本“add distributed”，就可以使用git reset命令： $ **git re****set --hard HEAD^** HEAD **is** now **at** ea34578 **add** distributed 这时readme.txt的内容就成了版本add distributed 我们用git log再看看现在版本库的状态： $ git log commit ea34578d5496d7dd233c827ed32a8cd576c5ee85 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Tue Aug 20 14:53:12 2013 +0800 add distributed commit cb926e7ea50ad11b8f9e909c05226233bf755030 Author: Michael Liao &lt;askxuefeng@gmail.com&gt; Date: Mon Aug 19 17:51:55 2013 +0800 wrote a readme file 最新的那个版本append GPL已经看不到了！ 恢复文件后，要是我们又想回到修改后的文件呢？（命令行窗口还没有被关掉） {这个是git reset —hard后，又反悔了，想回到修改后的状态} 只要上面的命令行窗口还没有被关掉，你就可以顺着往上找啊找啊，找到那个append GPL的commit id是3628164…，于是就可以指定回到未来的某个版本： $ git reset --hard 3628164 HEAD is now at 3628164 append GPL 版本号没必要写全，前几位就可以了，Git会自动去找。 Git的版本回退速度非常快，因为Git在内部有个指向当前版本的HEAD指针，当你回退版本的时候，Git仅仅是把HEAD从指向append GPL改为指向add distributed，然后顺便把工作区的文件更新了。所以你让HEAD指向哪个版本号，你就把当前版本定位在哪。 恢复文件后，要是我们又想回到修改后的文件呢？（命令行窗口早就关掉了） {这个是git reset —hard后，又反悔了，想回到修改后的状态} 想恢复到新版本怎么办？找不到新版本的commit id怎么办？当你用$ git reset —hard HEAD^回退到add distributed版本时，再想恢复到append GPL，就必须找到append GPL的commit id。 Git提供了一个命令git reflog用来记录你的每一次命令：[Git高级教程:git log与git reflog] $ git reflog ea34578 HEAD@&#123;0&#125;: reset: moving to HEAD^ 3628164 HEAD@&#123;1&#125;: commit: append GPL ea34578 HEAD@&#123;2&#125;: commit: add distributed cb926e7 HEAD@&#123;3&#125;: commit (initial): wrote a readme file 第二行显示append GPL的commit id是3628164。"},{"title":"My first blog","date":"2022-04-19T14:43:42.241Z","url":"/2022/04/19/hello-world/","categories":[["undefined",""]],"content":"Welcome to my personal blog! This is my very first post. 当你看到这段文字的时候，你就来到了我的第一个博客网页，也是我的第一篇文章。感谢你的来访。 致谢一程山水一年华，独愿此去经年；一生坦荡；一生纯善。祝愿看到这篇博客的每一个人：他日凌云，万事胜意。 More info: Writing"},{"title":"mathorcup四天比赛之旅","date":"2022-04-18T02:00:00.000Z","url":"/2022/04/18/mathorcup-1/","tags":[["Mathematical Modeling","/tags/Mathematical-Modeling/"]],"categories":[["Mathematical Modeling","/categories/Mathematical-Modeling/"]],"content":"必要的知识和技能模型和算法：统计回归，时间序列，多元统计，数学规划，微分方程，概率随机，网络优化。图形图像处理，智能算法，随机模拟。机器学习等； 软件工具： Matlab，SPSS，Lingo，Python，Eview 如何短时间提升？ 多看优秀论文 重视写作，摘要篇幅适中，独立成文，目录结构清晰 2022.4.14题目选择：D题 题目下载 3.覆盖问题 Covering Problem覆盖问题分为最大覆盖问题和集覆盖问题两类。 （1）集覆盖问题研究：在备选设施集合里，已知每个设施的服务范围，如何选择设施，使所有需求点得到服务，并且设施数p最小或成本最小。 （2）最大覆盖问题研究：在备选设施集合里，已知每个设施的服务范围，如何选择p个设施，使得服务的需求点数最多或需求量最大。应用场景：追求覆盖面的场景，比如移动基站的选址、物流中心的选址。 问题一 目标函数： 1.总任务量大 2.总成本小 决策变量： 1.是否建立基站 2.基站种类 约束条件： 1.基站点与弱覆盖点距离小于30或10 2.两个基站之间的距离大于等于10 3.达到90%弱覆盖点任务量 求解方法：LINGO，遗传算法，模拟退火算法，粒子群算法等。 问题二 同一，给出几种基本扇形方案 重新计算最佳方案的结果 问题三 区域聚类 DBSCAN算法（基于密度聚类） 其中k-means，层次聚类是时间复杂度较低、较为常用的方法 重点：时间复杂度 思路汇总 1.将所有业务量多的点划分入可选选址点（基站选择模型） 2.先聚类，先分开区域在进行1的操作 3.设计长方形矩阵进行遍历（基站选择模型） 4.附件一每个点建一个站址，再筛选 5.动态规划 6.被重复区域会带来成本增加 2022.04.15"},{"title":"天局","date":"2022-04-05T15:00:00.000Z","url":"/2022/04/05/shengtianbanzi/","tags":[["Literature","/tags/Literature/"]],"categories":[["Literature","/categories/Literature/"]],"content":"西庄有个棋痴，人都称他浑沌。他对万事模糊，惟独精通围棋。他走路跌跌斜斜，据说是踩着棋格走，步步都是绝招。棋自然是精了，却没老婆——正值四十壮年。但他真正的苦处在于找不到对手，心中常笼罩一层孤独。他只好跟自己下棋。 南三十里有个官屯小村，住着一位小学教师，是从北京迁返回乡的。传说他是围棋国手，段位极高，犯了什么错误，才窝在这山沟旮旯里。浑沌访到这位高手，常常步行三十里至官屯弈棋。 浑沌五大三粗，脸庞漆黑，棋风刚勇无比，善用一招“镇神头”，搏杀极凶狠。教师头回和他下棋，下到中盘，就吃惊地抬起头来：“你的杀力真是罕见!”浑沌谦虚地点点头。但教师收官功夫甚是出色，慢慢地将空拣回来。两人惺惺惜惺惺，英雄识英雄，成为至交。教师常把些棋界事情讲给他听。讲到近代日本围棋崛起，远胜中国，浑沌就露出鲁莽性了：“妈的，杀败日本!” 浑沌确是怪才。儿时，一位瘸子老塾师教会他围棋。三年自然灾害，先生饿死了。浑沌自生自长，跑野山，喝浑水，出息成一条铁汉。那棋，竟也浑然天成，生出一股巨大的蛮力，常在棋盘上搅起狂风骇浪，令对手咋舌。无论怎样坚实的堡垒，他强攻硬打，定将其摧毁。好像他伸出一双粗黑的大手，推着泰山在棋盘上行走。官屯教师常常感叹：“这股力量从何而来?国家队若是……”仿佛想起什么，下半句话打住。 腊月三十，浑沌弄到了一只猪头。他便绕着猪头转圈，嘴里嘀咕：“能过去年吗?能吃上猪头吗?落魄的人哪!”于是背起猪头，决意到官屯走一遭。 时值黄昏，漫天大雪。浑沌刚出门，一身黑棉衣裤就变了白。北风呼啸，仿佛有无数人劝阻他：“浑沌，别走!这大的雪——” “啊，不!” 千人万人拉不住他，他执拗而任性地投入原野。雪团团簇簇如浓烟翻滚。群山摇摇晃晃如醉汉不能守静。风雨夹裹逼得浑沌陀螺似的旋转，睁不开眼睛，满耳呼啸。天空中有隆隆声，神灵们驾车奔驰。冰河早被覆盖，隐入莽莽雪原不见踪迹。天地化作一片，无限广大，却又无限拥挤。到处潜伏着危险。 浑沌走入山岭，渐渐迷失了方向。天已断黑，他深一脚浅一脚，在雪地里跌跌撞撞。背上那猪头冻得铁硬，一下一下拱他脊背。他想：“要糟!”手脚一软，跌坐在雪窝里。 迷糊一阵，浑沌骤醒。风雪已停，天上悬挂一弯寒冰，照得世界冷寂。借月光，浑沌发现自己身处一山坳，平整四方，如棋盘。平地一侧是刀切般的悬崖，周围黑黝黝大山环绕。浑沌晓得这地方，村人称作迷魂谷。陷入此谷极难脱身，更何况这样一个雪夜!浑沌心中惊慌，拔脚就走。然而身如着魔，转来转去总回到那棋盘。 夜已深。雪住天更寒。浑沌要冻作冰块，心里却还清醒：“妈的，不能在这儿冻死!”四下巡视，发现山上皆黑石，块块巨大如牛。他索性不走，来回搬黑石取暖。本来天生蛮力，偌大的石块一叫劲，便擎至胸腹。他将黑石一块块置于平地。身子暖了，脑子却渐渐懵懂，入睡似的眼前模糊起来。 他似乎转过几个山角，隐约看见亮光。急赶几步，来到一座雅致的茅屋前。浑沌大喜：“今日得救了!”莽莽撞撞举拳擂门。屋里有人应道：“是你来了。请!” 浑沌进屋，但见迎面摆着一张大床，蚊帐遮掩，看不出床上躺着何人。浑沌稀奇：什么毛病?冬天怕蚊咬?蚊帐里传出病恹恹的声音：“你把桌子搬来，这就与你下棋。” 浑沌大喜：有了避风处，还捞着下棋，今晚好运气。又有几分疑惑：听口气那人认得我，却不知是谁。他把桌子般到床前，不由得探头朝蚊帐里张望。然而蚊帐似云似锦，叫他看不透。 “浑沌，你不必张望，下棋吧!” 浑沌觉得羞惭，抓起一把黑子，支吾道：“老师高手，饶我执黑先行。” 蚊帐中人并不谦让，默默等他行棋。浑沌思忖良久，在右下角置一黑子。蚊帐动动，伸出一只洁白的手臂。浑沌觉眼前一亮!那白臂如蛇游靠近棋盒，二指夹起一枚白子擎至空中，叭一声脆响，落子棋盘中央。浑沌大惊：这全不是常规下法!哪有第一着占天元位置的?他伸长脖颈，想看看蚊帐里究竟是什么人。 “你不必张望，你见不到我。” 声音绵绵软软如病中吟，比女子更细弱;但又带着仙气，仿佛从高远处传来，隐隐约约却字字清晰。这声音叫浑沌深感神秘，暗叹今夜有了奇遇。浑沌抖擞精神，准备一场好战! 棋行十六着，厮杀开始。白棋飞压黑右下角，浑沌毅然冲断。他自恃棋力雄健，有仗可打从不放手。白棋黑棋各成两截，四条龙盘卷翻腾沿边向左奔突。浑沌素以快棋著称，对方更是落子如飞。官庄教师常说浑沌棋粗，蚊帐中人却快而缜密。浑沌惊愕之心有增无减，更使足十二分蛮力。白棋巧妙地逼他做活，他却又把一条白龙截断。现在谁也没有退路了，不吃对方的大龙必死无疑。 围棋，只黑白二子，却最体现生存竞争的本质。它又不像象棋，无帅卒之分，仿佛代表天地阴阳，赤裸裸就是矛盾。一旦自己的生存受到威胁，谁不豁出老命奋起抗争呢?此刻，右下角燃起的战火越烧越旺，厮杀极惨烈。浑沌不顾一切地揪住一条白棋，又镇又压，穷追猛打。白棋却化作涓涓细流，悄悄地在黑缝中流淌，往黑棋的左上角渗透。假若不逮住这条白龙，黑棋将全军覆灭。浑沌额上沁出一层汗珠，心中狂呼：“来吧!拼吧!”义无反顾地奔向命运的决战场——左上角。 第九十八手，白棋下出妙手!蚊帐中人利用角部做了一个劫，即使浑沌劫胜了，也必须连走三手才能吃尽白棋。浑沌傻眼了。这岂止是妙手?简直是鬼手!但是，浑沌没有回旋余地，只得一手一手把白棋提尽。蚊帐中人则利用这劫，吃去黑右下角，又封住一条黑龙。 现在，轮到浑沌逃龙了。可是举目一望，周围白花花一片，犹如漫天大雪铺天盖地压来。浑沌手捏一枚黑子，泥塑般呆立。一子重千钧啊!他取胜一役，但又将败于此役。只有逃出这条龙，才能使白棋无法挽回刚才的损失。然而前途渺茫，出路何在? 正为难时，一阵阴风扑开门，瘸瘸拐拐进来个老先生。浑沌闻声回头，见是那死去多年的私塾先生。既已死，怎地又在这荒山僻野露脸?太蹊跷!紧急中浑沌顾不得许多，连呼：“老师，老师，帮我一把!” 私塾先生瘸至桌前，捻着山羊胡子俯身观棋。阴气沉重，压得灯火矮小如豆。那白臂翘起食指，对准罩子灯一点，火苗倏地跳起，大放光明。老先生一惊，身子翻仰，模样十分狼狈。 “哼哼。”帐内冷笑。 浑沌心中愤愤：这局棋，定要赢!一股热血冲向脑门，阳刚之气逼得黑发霍霍竖起。 瘸子先生似乎知道对手不是常人，一招手，门外进来他的同伴，先入二人羽扇纶巾，气宇轩昂，正是清代围棋集大成者：飘飘然大师范西屏，妙手盖天施襄夏。他们在当湖对弈十局，成为围棋经典;施襄夏因心力耗尽，终局时呕血而死。再进来一位，明代国手过百龄，他著的《官子谱》至今流传。宋代的围棋宗师刘仲甫扶着龙头拐的骊山老母蹒跚而入。一千年前他们在骊山脚下大战，只三十六着，胜负便知。直至春秋时代的弈秋进屋，围棋史上英豪们便来齐了。 浑沌端坐桌前。他再不猜测这些人如何来到人间，只把目光集中在那只手上。洁白如玉的手，如此超然，如此绝对，一圈神圣的光环围绕着它。它仿佛一直是人、鬼、神的主宰，一直是天地万物的主宰。它是不可抗拒的，不可超越的。浑沌明白，他是在与无法战胜的对手交战。他想赢，一定要赢! 大师们皆不言语，神情庄严肃穆。浑沌的穴位被一人一指按住，或风池或太阳，或大推或命门。霎时间灵气盈盈，人类智慧集于浑沌一身。他觉得脑子清明，心中生出许多棋路，更有一种力量十倍百倍地在体内澎湃。他拿起黑子，毅然投下，然后昂起头，目光灼灼，望着蚊帐里不可知的对手。 中原突围开始。浑沌在白棋大模样里辗转回旋，或刺或飞，或尖或跳，招数高妙决非昔日水平，连他自己也惊讶不已。然而蚊帐中人水涨船高，棋艺比刚才更胜几筹。那白棋好似行云流水，潇洒自如，步步精深，招招凶狠，逼得黑棋没有喘息的机会。黑棋仿佛困在笼中的猛兽，暴跳如雷，狂撕乱咬，却咬不开白棋密密匝匝的包围圈。浑沌双目瞪圆，急汗如豆。棋盘上黑棋败色渐浓。 忽然，浑沌脑中火花一闪，施出一着千古奇绝的手筋。白棋招架之际露出一道缝隙，黑棋敏捷地逮住时机，硬挤出白色的包围圈。现在，右边广阔的处女地向他招手。只要安全到达右边，黑色的大龙就能成活。但是，白棋岂肯放松?旁敲侧击，步步紧逼，设下重重障碍。黑棋艰难地向右边爬行。追击中，白棋截杀黑龙一条尾巴。这一损失教浑沌心头剧痛，好像被人截去一只左脚。他咬着牙，继续向处女地进军。白棋跳跶闪烁，好似舞蹈着的精灵，任意欺凌负伤的黑龙。黑龙流着血，默默地呻吟着，以惊人的意志爬向目的地。只要有一线生存的希望，无论忍受多少牺牲，浑沌都顽强地抓牢不放!棋盘上弥漫着沉闷的气氛。人生的不幸，似乎凝聚在这条龙身上。命运常常这样冷酷地考验人的负荷能力。 终于，浑沌到达了彼岸。他马上返过身，冲击白棋的薄弱处。蚊帐中人翘起食指，指尖闪耀五彩光辉。这是一种神秘的警告。浑沌定定地望着那手指，朦胧地感到许多自己从不知晓的东西。白子叭地落在下边，威胁着刚刚逃脱厄运的黑龙。他必须止步。他必须放弃进攻，就地做活。但是，这样活多么难受啊!那是令人窒息的压迫，你要活，就必须像狗一样。浑沌抬起头，那食指依然直竖，依然闪耀着五彩光辉。浑沌把头昂得高高，夹起一枚黑子，狠狠地打入白阵! 这是钢铁楔子，刚刚追击黑龙的白棋，被钉在将遭歼灭的耻辱柱上。下边的白棋又跳一手，夺去黑龙的眼位，使它失去最后的生存希望。于是，好像两位立在悬崖边上的武士，各自抽出寒光闪闪的宝剑，开始一场你死我活的决斗。 这是多么壮烈的决斗啊!围棋在此显示出慷慨悲歌的阳刚之美：它不是温文尔雅的游戏，它是一场血肉横飞的大搏杀!看，浑沌使出天生蛮力，杀得白棋惨不忍睹;蚊帐中人猛攻黑龙，一口接一口地紧气，雪白的手臂竟如此阴冷，刽子手一样扼住对手的喉咙。浑沌走每一步棋，都仿佛在叫喊：“我受够了!我今天才像一条汉子!”白棋却简短而森人地回答：“你必死!”黑棋的攻势排山倒海，招招带着冲天的怒气。一个复仇的英雄才会具备那样的力量，这力量如此灼热，犹如刚刚喷出火山口的岩浆，浩浩荡荡，毁灭万物。白棋置自己的阵地不顾，专心致志地扼杀黑龙。两位武士都不防卫，听任对方猛砍自己的躯体，同时更加凶恶地刺向对方的要害。 屋外响起一声琵琶，清亮悠扬。琵琶先缓后急，奏的是千古名曲《十面埋伏》。又有无数琵琶应和，嘈嘈切切，声环茅屋。小小棋盘升起一股血气，先在屋内盘桓，积蓄势大，冲破茅屋，红殷殷直冲霄汉。天空忽然炸响焦雷，继而群雷滚滚而下。琵琶声脆音亮，激越如潮，仿佛尖利的锥子，刺透闷雷，挺头而出。两者互压互盖，反复交错，伴那一柱血光，渲染得天地轰轰烈烈。 蚊帐中人吃了浑沌的黑龙，浑沌霸占了先前白阵。沧海桑田，一场大转换。棋细势均，胜负全在官子上。浑沌回头看看，列位先师耗尽真力，已是疲惫不堪。浑沌方知这场大战非自己一人所为。人、鬼、神结为一阵，齐斗那高深莫测一只手。 官子争夺亦是紧张。俗语道：“官子见棋力”。那星星点点的小地方，都是寸土必争;精细微妙，全在其中。《官子谱》、《玄玄棋经》连珠妙着尽数用上，妙中见巧，巧中见奇。小小棋盘，竟是大千世界。 棋圣们一面绞尽脑汁，一面审度形势。范西屏丢了羽扇，先失飘然神韵;刘仲甫扯去纶巾，不见大家风采。瘸子先生挨不到桌边，急得鼠窜，却被诸多大腿一绊一跌，显出饿死鬼的猴急。骊山老母最擅计算，已知结局，扁着没牙嘴巴喃喃道：“胜负半子，全在右下角那一劫上……”心里急，手上一运仙力，竟把龙头拐杖折断。 果然，官子收尽，开始了右下角的劫争。围棋创造者立下打劫规则，真正奇特之极：出现双方互相提子的局面，被提一方必须先在别处走一手棋，逼对方应了，方可提还一子。如此循环，就叫打劫。打劫胜负，全在双方掌握的劫材上。浑沌的大龙死而不僵，此时成了好劫材，逼得蚊帐中人一手接一手应，直到提尽为止。黑阵内的白棋残子也大肆骚乱，扰得浑沌终不得粘劫。两个人你提过去，我提回来，为此一直争得头破血流。 鸡将啼，天空东方一颗大星雪亮。浑沌劫材已尽，蚊帐中人恰恰多他一个。大师们一起伸长脖颈，恨不得变作棋子跳入棋盘。然而望眼欲穿，终于不能替浑沌找出一个劫材。一局好棋，眼看输在这个劫上。满桌长吁短叹，皆为半子之负嗟惜。浑沌呆若木鸡，一掬热泪滚滚而下。 列位棋祖转向浑沌，目光沉沉。浑沌黑袄黑裤，宛如一颗黑棋子。祖师们伸手指定浑沌，神情庄严地道：“你去!你做劫材!” 浑沌巍巍站起。霎时屋内外寂静，空气凝结。浑沌一腔慷慨，壮气浩然。推金山，倒玉柱，浑沌长跪于地。 “罢，浑沌舍啦!” 蚊帐中人幽幽叹息：“唉……”一只白臂徐徐缩回，再不复出。 浑沌背猪头出西庄，几日不回。西庄人记得除夕雪大，不禁惴惴。知底细者都道浑沌去了官屯，便打发些腿快青年去寻。官屯小学教师见西庄来人，诧异道：“我没有见到浑沌，他哪来我这里?” 众人大惊，漫山遍野搜寻浑沌。教师失棋友心焦急，不顾肺病，严寒里东奔西颠。半日不见浑沌踪迹，便有民兵报告公安局。 有一老者指点道：“何不去迷魂谷找找?那地方多事。”于是西庄、官屯两村民众，蜂拥至迷魂谷。 迷魂谷白雾漫漫。人到雾收，恰似神人卷起纱幔。众人举目一望，大惊大悲。只见谷中棋盘平地，密匝匝布满黑石。浑沌跪在右下角，人早冻僵;昂首向天，不失倔犟傲气。一只猪头搁在树下，面貌凄然。 浑沌死了。有西庄人将猪头捧来，告诉教师：只因浑沌送猪头给他过年，才冻僵于此。教师紧抱猪头，被棋友情义感至肺腑，放声嚎啕，悲怆欲绝。 有人诧异：浑沌背后是百丈深谷，地势极险，他却为何跪死此地?众人作出种种推测，议论纷纷。教师亦觉惶惑，止住泣涕，四处蹒跚寻思。 他在黑石间转绕几圈，又爬到高处，俯瞰谷地。看着看着，不觉失声惊叫：“咦——” 谷地平整四方如棋盘，黑石白雪间隔如棋子，恰成一局围棋。教师思忖许久，方猜出浑沌冻死前搬石取暖，无意中摆出这局棋。真是棋痴!再细观此局，但见构思奇特，着数精妙，出磅礴大气，显宇宙恢宏，实在是他生平未见的伟大作品。群山巍峨，环棋盘而立;长天苍苍，垂浓云而下;又有雄鹰盘旋山涧，长啸凄厉…… 官屯教师身心震动，肃穆久立。 众人登山围拢教师，见他异样神情皆不解。纷纷问道：“你看什么?浑沌干啥?”教师答：“下棋。”“深山旷野，与谁下棋?”教师沉默不语。良久，沉甸甸道出一字：“天!” 俗人浅见，喳喳追问：“赢了还是输了?” 教师细细数目。数至右下角，见到那个决定胜负的劫。浑沌长跪于地，充当一枚黑子，恰恰劫胜!教师崇敬浑沌精神，激情澎湃。他双手握拳冲天高举，喊得山野震荡，林木悚然—— “胜天半子!”"},{"title":"FaceGen的使用（二）与3DFFA_V2代码的实现","date":"2022-03-31T10:00:00.000Z","url":"/2022/03/31/paperreading-4/","tags":[["Feature Point Punctuation Algorithm","/tags/Feature-Point-Punctuation-Algorithm/"]],"categories":[["Feature Point Punctuation Algorithm","/categories/Feature-Point-Punctuation-Algorithm/"]],"content":"FaceGen的使用 （接上篇） FaceGen Modeller与FaceGen 3D Print可以使用其他很多格式进行导出与查看 以下为三款程序之间的对比： FaceGen Artist支持直接导入图片自动生成人脸模型，支持多张人脸图片的组合，允许调整人脸的各种细节（包括年龄、性别、种族、肤色、皱纹、痣，头发和许多其他参数），直到达到您想要的效果，依此类推。 非常易于使用。您只需导入您要调整的正确脸部照片，然后软件将分析图像并创建新的脸部网格以及相应的整体皮肤纹理。创建好的人脸模型可以直接导入DAZ Studio使用。此外，它还支持连接3D打印机程序进行打印操作。 FaceGen Artist细节处理非常细致，五官的处理非常精准。它可以帮助用户轻松调整整张脸的各种细节，从而完美完成整张脸模型的设计和调整，让人物形象更加生动。 FaceGen Modeller 作为一款小巧易用、功能强大、高效的 3D 人脸建模工具，可以随机或从照片中创建 3D、逼真的人脸，支持多种面部细节的调整，包括种族、性别、年龄、眼睛、眉毛、鼻孔等150多个五官。此外，它采用带有表情和色调的动画形式实时显示结果，可以存储为BMP、JPG、TGA、TIF等格式的图像，也可以导出为FBX、OBJ、3DS、WRL等3D 文件格式。 FaceGen 3D Print Demo塑造一个有底座的头部人像，导出格式支持PNG,JPEG,TARGA,BMP文件格式。 Towards Fast, Accurate and Stable 3D Dense Face Alignment关于快速，准确和稳定的三维密集人脸对准 1.采用轻量级的网络模型回归出3DMM的参数（论文2.1节），然后为该网络设置了meta-joint optimization优化策略(论文2.2节)，动态的组合wpdc(Weighted Parameter Distance）和vdc(Vertex Distance Cost）损失函数，从而加速了拟合的速度，也使得拟合的效果更加精确 2.提出landmark-regression regularization（特征点回归正则化）来加速拟合的速度和精确度 3.为了解决在video上的三维人脸对齐任务（相邻帧之间的三维重建更加稳定，快速，连续性），在基于video数据上训练的模型，但video视频数据库缺乏时，提出了3D aided short-video-synthesis（三维辅助短视频合成技术），将一个静止的图片在平面内还有平面外旋转变成一个短视频 3DMM1.回归参数截取 2.参数的归一化处理 3.3D人脸与2D人脸之间的公式只差一个正投影P参数 损失函数设计设计fWPDC 在使用fwpdc损失函数训练好模型后，然后保存此次模型的训练结果；之后开始进行下次训练，在训练前，将模型的参数初始化为上次保存的模型，然后在使用vdc损失函数继续对该模型进行训练！基于这种发现，提出了meta-joint optimization ，即动态的组合fwpdc和vdc损失函数进行训练。 3D Aided Short-video-synthesis(三维辅助短视频合成技术)经测试发现，该技术共有两个作用； 一是对模型的训练有促进作用（用NME来衡量模型是否优越，可用21，68，NK个三维特征点之间的距离损失来计算dist）；二是促进了在视频上进行三维人脸重建时候的稳定性（用相邻帧之间的偏移差异来衡量视频中三维人脸重建的稳定性） 3DFFA_V2代码的实现首先，附属代码未提供测试与训练的代码，只提供了训练好的模型并将其储存 关于定义204[3, 64]个点，为什么要定义u_base , w_sha_base , w_exp_base的说明 (此解释 by CSDN 博主 LX-CV提供) obj文件： 将人脸保存为obj文件 即保存了这个特征点的形状属性，又保存了这个点的纹理属性（r1, g1, b1） ；ply文件：将人脸保存为ply文件， 只保存了这个特征点的形状属性(x1, y1, z1)，不保存纹理属性； Faceboxs边界框检测器： (本代码 by CSDN 博主 LX-CV提供) facebox检测器传入人脸照片，返回的是个列表，用来存储边界框bbox(是个列表，共有5个数据)，有几个人脸就有几个边界框 ； 接下来可以利用bbox边界框获得roi_box参数，然后便可以利用roi_box参数画出边框或者精确的剪切出人脸照片 挂vpn会出现无法pip的情况 Windows上运行需修改部分代码与函数，包括但不仅限于： 运行效果如下： .bbplayer{width: 100%; max-width: 850px; margin: auto} document.getElementById(\"mmedia-JLsWpFVpYAOLAKnw\").style.height=document.getElementById(\"mmedia-JLsWpFVpYAOLAKnw\").scrollWidth*0.76+\"px\"; window.onresize = function(){ document.getElementById(\"mmedia-JLsWpFVpYAOLAKnw\").style.height=document.getElementById(\"mmedia-JLsWpFVpYAOLAKnw\").scrollWidth*0.76+\"px\"; }; .bbplayer{width: 100%; max-width: 850px; margin: auto} document.getElementById(\"mmedia-JclfcXMRoPQctemf\").style.height=document.getElementById(\"mmedia-JclfcXMRoPQctemf\").scrollWidth*0.76+\"px\"; window.onresize = function(){ document.getElementById(\"mmedia-JclfcXMRoPQctemf\").style.height=document.getElementById(\"mmedia-JclfcXMRoPQctemf\").scrollWidth*0.76+\"px\"; }; .bbplayer{width: 100%; max-width: 850px; margin: auto} document.getElementById(\"mmedia-zwzgAXPemLwbgzBJ\").style.height=document.getElementById(\"mmedia-zwzgAXPemLwbgzBJ\").scrollWidth*0.76+\"px\"; window.onresize = function(){ document.getElementById(\"mmedia-zwzgAXPemLwbgzBJ\").style.height=document.getElementById(\"mmedia-zwzgAXPemLwbgzBJ\").scrollWidth*0.76+\"px\"; }; .bbplayer{width: 100%; max-width: 850px; margin: auto} document.getElementById(\"mmedia-eicvzOOFQAwObUuF\").style.height=document.getElementById(\"mmedia-eicvzOOFQAwObUuF\").scrollWidth*0.76+\"px\"; window.onresize = function(){ document.getElementById(\"mmedia-eicvzOOFQAwObUuF\").style.height=document.getElementById(\"mmedia-eicvzOOFQAwObUuF\").scrollWidth*0.76+\"px\"; }; 仍建议使用Linux或MacOS运行"},{"title":"FaceGen使用与三维人脸对准代码","date":"2022-03-26T14:00:00.000Z","url":"/2022/03/26/paperreading-3/","tags":[["Feature Point Punctuation Algorithm","/tags/Feature-Point-Punctuation-Algorithm/"]],"categories":[["Feature Point Punctuation Algorithm","/categories/Feature-Point-Punctuation-Algorithm/"]],"content":"FaceGen的使用 FaceGen是由 Singular Inversions 生产 的 3D 人脸生成3D 建模 中介软件。 FaceGen 还可以从面部的正面和侧面图像生成 3D 模型，或者通过分析单张照片。 免费版本：FaceGen Artist、FaceGen 3D Print 和 FaceGen Modeller 的免费演示版可从公司网站下载。这些允许用户以程序专有的“.fg”格式创建、编辑、加载和保存文件。免费版具有与付费版相同的功能，只是在生成的模型的额头上放置了一个徽标，并且只提供了一些额外的功能，如发型和胡须。 ——Create——Photo：使用闪光灯在室内拍摄护照风格的照片(利用面部的正面和侧面图像生成 3D 模型) ——New：可修改种族，性别以及进行随机设定生成模型 ——Scan：通过3D扫描（或其他网格）获取面部形状 ——Modify（修饰）——Demographics：对年龄，性别，人种，肤色，等进行综合调整 ——Interactive：手动调整脸部模型 ——Undo（撤销与重做）——View（微调）——File——Face：加载或保存FaceGen面文件（.FG）与其他FaceGen产品兼容 拖放：将FG文件放置到视口中（左侧） ——Export： 参数组 您的形状变形将放置在参数选项卡的头部区域内，并将默认为显示的组路径。您可以将其更改为您喜欢的任何内容，只要您记住正斜杠“/”创建一个子组并且您的组路径不应以斜杠结尾。 参数名称 不要在文件名中使用任何不允许的字符。 Daz Studio 内容库 Daz Studio 可以从您的任何内容目录中读取您的新变形。 如果由于某种原因您想使用不同的目录，首先将其添加为 Daz Studio 中的内容目录（编辑→首选项→内容→内容目录管理器→ Daz Studio 格式→添加），然后重新启动 FaceGen Artist，它会出现在名单上。 关于R3DM,DLIN,FCNR3DM与DLIN经检索查询： Syed Zulqarnain Gilani（论文作者/通信作者）主页：代码（X） paperwithcode.com网站：代码（X） codeocean.com网站：代码（X） google 关键词检索：代码（X） FCN经检索查询： 论文中引用路径：J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for semantic segmentation, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3431–3440. 此论文本身没有给出代码，但有其他类似代码给出： 本周进展“Look Ma, no landmarks!” – Unsupervised, Model-based Dense Face Alignment ECCV 2020 · Tatsuro Koizumi, William A. P. Smith 代码： Weakly-Supervised Multi-Face 3D Reconstruction 6 Jan 2021 · Jialiang Zhang, Lixiang Lin, Jianke Zhu, Steven C. H. Hoi 代码： Joint 3D Face Reconstruction and Dense Face Alignment from A Single Image with 2D-Assisted Self-Supervised Learning Jiashi Feng, Zheng Ma, Linxiao He, Yang Zhao, Mei Xie, Yao Luo, Zihang Jiang, Jian Zhao, Xiaoguang Tu - 2019 代码： Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry 19 Oct 2021 · Cho-Ying Wu, Qiangeng Xu, Ulrich Neumann 代码： img2pose: Face Alignment and Detection via 6DoF, Face Pose Estimation CVPR 2021 · Vítor Albiero, Xingyu Chen, Xi Yin, Guan Pang, Tal Hassner 代码： Towards Fast, Accurate and Stable 3D Dense Face Alignment ECCV 2020 · Jianzhu Guo, Xiangyu Zhu, Yang Yang, Fan Yang, Zhen Lei, Stan Z. Li 代码： LUVLi Face Alignment: Estimating Landmarks’ Location, Uncertainty, and Visibility Likelihood CVPR 2020 · Abhinav Kumar, Tim K. Marks, Wenxuan Mou, Ye Wang, Michael Jones, Anoop Cherian, Toshiaki Koike-Akino, Xiaoming Liu, Chen Feng 代码： 附录论文代码查询方式： 1.作者网站 2.Google广搜 3. 4. 5. 6.插件Find Code for Research Papers（可应用于Chrome和Firefox） 7."},{"title":"数模竞赛基础","date":"2022-03-22T11:00:00.000Z","url":"/2022/03/22/matlab-learning/","tags":[["Mathematical Modeling","/tags/Mathematical-Modeling/"]],"categories":[["Mathematical Modeling","/categories/Mathematical-Modeling/"]],"content":"（未完待续）对于今年的数学建模比赛的编程工作的准备与提高，按照本学期数模课程的进度更新，仅作为复习使用，持续更新 第一章 数组运算与数组化编程第一节 数组的创建matlab中的运算和操作主要是以数组（Array）为对象的，数组又包括：数值数组、字符数组、元胞数组等。 一、数值数组的建立： 1. 直接输入法 2.通过数组编辑器生成矩阵 3. 数据导入法 4.用函数创建数组 二、字符数组、元胞数组的建立 第二节 数组的操作1. 数组元素与子数组的提取 2.改变数组中的元素值与数组的拼接 ——改变数组形状的命令 ——数组的复制 3.常用的数组操作命令 ——确定数组大小的命令 ——排序命令 ——稀疏矩阵与满矩阵的转化 ——diag命令 ——find命令 ——排列组合 4.数组的运算 例题： 第二章 MATLAB 图形功能第一节 曲线作图一，曲线作图与图形控制 1.plot函数 2.图形窗口的控制 二，子图与多重线 1.子图 2.多重线 三，数字函数的简易作图 四，极坐标作图 第二节 空间曲线和曲面作图一，空间曲线作图 二，曲面作图 三，等高线图 例题： 第三章 Matlab程序设计第一节 自定义函数一，符号函数 二，句柄函数 三，m-文件函数（子程序） 第二节 流程控制语句一，分支结构 ——if语句 ——switch语句 二，循环结构 ——for循环语句 ——while-end循环语句 三，程序的流程控制命令 第四章 Matlab数值运算第三节 数据插值一，一维插值 ——拉格朗日插值法 ——分段插值 ——三次样条插值 （1）yq=interp1(x,y,xq,’spline’) （2）使用spline函数： ​ 用法1：yq=spline(x, y, xq) ​ 用法2：pp=spline(x, y) ​ 获得三次样条插值的分段多项式pp， ​ 可使用ppval计算插值。 （3）使用csape函数： ​ 用法： pp=csape(x, y) ​ 可以添加参数选择边界条件。 二，二维插值 1.网格节点数据插值函数：interp2 2.散点数据插值函数：griddata 第四节 数值积分一，数值积分的常用方法 （1）用梯形法计算积分 （2）一维数值积分 二，重积分的数值计算 （1）数值二重积分 （2）数值三重积分 例题 "},{"title":"基于深度学习的3维人脸特征点自动检测","date":"2022-03-13T14:00:00.000Z","url":"/2022/03/13/paperreading-2/","tags":[["Feature Point Punctuation Algorithm","/tags/Feature-Point-Punctuation-Algorithm/"]],"categories":[["Feature Point Punctuation Algorithm","/categories/Feature-Point-Punctuation-Algorithm/"]],"content":"（基于深度学习的3维人脸特征点自动检测） 将3维模型转化为2维图像的方法Deep landmark identification network, DLIN结论： ·提出一个多线性算法自动建立密集的点对点对应的任意数量的3D人脸，提出深度特征点识别网络对合成图像进行训练。 ·使用DLIN检测显著特征点，使用它们通过水平集曲线生产测地线距离图将3D人脸分割为Voronoi区域；利用这些区域中的·关键点以非刚性的方式对人脸中的相似区域进行对齐，通过神经网络搜索建立密集对应关系。 ·提出一种基于区域的三维可变型模型（R3DM），以有效地将密集的通信传播到大型数据集。 ·不依赖于现有的3D人脸模型。 ·论文原意将其应用于对诸如睡眠呼吸暂停症和自闭症谱系障碍等疾病进行表型分析。 1.生成合成训练数据 为了训练深度地标识别网络(DLIN)，作者使用商业软件(FaceGenTM)，通过改变面部形状和面部表情，综合生成真实的3D人脸。因为人脸是由模型生成的，所以特征点的真实位置就已经知道了。训练数据覆盖了由于年龄、种族和表情而导致的面部形状变化的巨大空间。还在训练数据中诱导姿态变化，以适应测试图像中的姿态变化。 2.训练深度特征点识别网络 卷积层数为5层，上采样层改为4x层 实验表明，减少8个卷积层可以使学习时间减少约3倍，特征点检测时间减少4倍。 3.使用DLIN进行特征点检测 基于区域的密集对应 原因：医学应用中，特征点检测和对应的准确性至关重要，密集的对应必须非常精确. 输入：一组用Fj表示的N个3D人脸及其对应的基准特征点Lj 通过计算最短曲面距离（又称为geodesic distance）利用Fsat Marching算法求解的LHS，找到了geodesic distance距离映射，然后将3D人脸划分为五个特征点相关的区域 ——关于可形变模型拟合 R3DM：通过可变形模型拟合将密集对应传播到大量的查询面，比建立基于区域的对应更快 作为验证密集通信算法质量的一种方法。 US为主成分(PCs),V列为对应的荷载 人脸通过DLIN特征点识别，分为五个面部区域，R3DM和查询人脸的鼻尖剧中，实现初始人脸对齐。通过上式给出的各区域统计模型进行变形，合成查询人脸区域内三位点随即子集，进行两步优化（矢量化查询和刚性变换）。最后将所有区域参数串联起来，作为表征人脸的特征。 具体实验 合成数据集训练DLIN，实验在真实的3D人脸数据集进行。 使用数据集：基准的FRGCv2和Bosphorus数据集 DLIN与FCN的比较：为了评估DLIN在FCN上的改进，作者遵循相同训练协议。更具体地说，DLIN是从零开始学习的，而FCN是在相同的训练数据上进行微调的。在4,007张FRGCv2扫描图上测试了这两种网络，并比较了地标定位误差和每幅图像的检测时间。从结果可以看出，DLIN的地标检测速度比FCN快4倍，而对所有地标的精度都等于或略优于FCN。 作者在不同数据集上进行测试并给出比较结果。例如，在博斯普鲁斯数据集上使用类似的策略，对未见过的面孔进行无偏地标检测。DLIN探测到的11个地标的精度在2.95mm和1.83mm之间，从而将精度提高了30%。结果表明，该方法具有较高的精度和对大姿态和表情变化的鲁棒性。 构建了两个模型R3DM1和R3DM2。 Conclusions We presented an algorithm for dense correspondence over a large number of 3D faces across varying facial expressions and identities. We trained a Deep Landmark Identification Network (DLIN) using synthetic images to detect salient landmarks and used them to segment the 3D face into Voronoi regions by generating a geodesic distance map through level set curves. Keypoints in these regions were used to align similar regions across faces in a non-rigid manner and dense correspondence was established through NN search. We also proposed a Region based 3D Deformable Model (R3DM) to propagate the dense correspondences to large datasets efficiently. Experiments on benchmark datasets with challenging protocols show that our algorithm is faster and more accurate than existing state-of-the-art. Our algorithm is able to generate population specific accurate deformable 3D face models from scratch without relying on existing linear 3D face models such as the BFM. In the future, we intend to use our algorithm for phenotyping medical conditions such as Sleep Apnoea and Autistic Spectrum Disorder. 我们提出了一种算法，用于在不同面部表情和身份的大量3D人脸上进行密集通信。我们使用合成图像训练深度地标识别网络(DLIN)来检测显著地标，并使用它们通过水平集曲线生成测地线距离图将3D人脸分割为Voronoi区域。利用这些区域中的关键点以非刚性的方式对人脸中的相似区域进行对齐，通过神经网络搜索建立密集对应关系。我们还提出了一种基于区域的三维可变形模型(R3DM)，以有效地将密集的通信传播到大型数据集。在具有挑战性协议的基准数据集上的实验表明，我们的算法比现有的最先进的算法更快、更准确。我们的算法能够从无开始生成特定人群的精确可变形3D人脸模型，而不依赖于现有的线性3D人脸模型(如BFM)。在未来，我们打算用我们的算法来对诸如睡眠呼吸暂停症和自闭症谱系障碍等疾病进行表型分析。 思路：首先对三维人脸建立稠密准确的密集对应联系，据此组合不同身份和不同表情的人脸进行数据集的身份和表情扩增；也采用了HPR算法对旋转人脸的自遮挡进行了模拟，据此进行了数据集的姿态扩增。通过以上方法构建了一个百万级的三维人脸数据集，然后利用GridFit算法插值得到深度，俯角和仰角的三通道图像，从头训练了一个神经网络，在目前所有的公开三维人脸数据集中获得了非常好的成绩。 基于3维形变模型的方法3D morphable models,3DMM算法主要流程从上面的叙述中，我们可以直观的想象，主要有两个步骤，第一个是从人脸数据库中所有脸构建出一个平均的脸部模型，第二个完成形变模型与照片的匹配。这两个步骤中，都暗含了人脸与人脸之间的每一个点都拥有对应关系，且必须找到这种对应关系，完成点与点之间的配准，是最主要的难题。 因此在进行建模的过程中，需要完成以下两个关键的问题： 模型与照片的配准如何避免生成怪异不可能的模型文章主要组织为以下几部分对思路进行介绍： 三维形变的脸部模型这里作者将人脸分为了两种向量：一种是形状向量（shape-vector），包含了X , Y , Z 坐标信息；另一种是纹理向量（texture-vector）， 包含了R , G , B颜色值信息在有了以上的表示方法后，我们使用的建立三维形变的脸部模型由m mm个脸部模型组成，其中每一个都包含相应的$S_{i}, T_{i} $两种向量。 将形变模型与照片对应在有了形变模型之后，对于一张给定的人脸照片，我们需要将模型与人脸照片进行配准，然后对模型的参数进行调整，使其与照片中的人脸差异值达到最小。简单而言，不断依据模型与输入的人脸照片进行比对，不断进行迭代，使两者之间的比对误差达到最小，这个时候，我们可以近似认为该模型即为对应输入的人脸照片的三维模型。 3D dense face alignment,3DDFAFace AlignmentFace Alignment 人脸对齐任务是基于一定量的训练集，得到一个模型，使得该模型对输入的一张任意姿态下的人脸图像能够进行特征点(landmark)标记。Face Alignment 任务一般的呈现方式是人脸特征点的检测与标记。一般的二维人脸图像 Face Alignment 得到的结果是特征点的二维位置坐标信息。Full pose range文中将人脸转动角度小于45度的姿势称为 small-medium pose，对于转动角度大于45度，小于等于90度的姿势称为 large pose。即，此篇论文解决的问题是大姿势下的人脸对齐任务。最极端情况则是整张人脸图像上只有侧脸信息。 通用的一些FA模型在大姿势的情况下会面临一些挑战： 这些模型假定所有的特征点可见；随着姿势的变化，人脸呈现的角度也会发生变化，在大姿势的情况下无法保证所有特征点可见；不同的姿势带来的特征点标记困难程度也会上升，而对于一些不可见的点，现有的办法大部分是靠猜。 3DDFA(3D Dense Face Alignment)问题的转换将常见的2D人脸特征点标记问题转换为3D拟合的任务。具体表现在需要求解的参数上。 2D人脸特征点标记得到的结果：一系列2D特征点位置坐标3D拟合需要求解的参数pose 姿势参数：scale 缩放因子, rotation 旋转矩阵, translation 平移向量morphing 变形参数: shape 形状系数, expression 表情系数人脸形状的表示——3DMM选择3DMM对人脸形状进行表示。在传统3DMM的基础上，加入3DMM拓展模型的 identity 和 expression 参数。那么，人脸形状 S的表示方式如下： 其中，S 表示平均人脸形状，A 表示主成分向量，α 表示相应的系数。 参数估计方法——级联回归+CNN级联回归：通过一系列回归器将一个指定的初始预测值逐步细化，每一个回归器都依靠前一个回归器的输出来执行下一步操作。联系到FA任务中，则是基于初始人脸形状，一般选择的是平均人脸形状，通过多次回归把参数回归到ground-truth的地方。 卷积神经网络CNN：将人脸特征点标记看作从图像像素到特征点位置的回归操作。 3DDFA方法则是将级联回归中的回归器设置为CNN。 根据这个式子，需要了解的主要是四个部分： ·回归目标，即参数p·卷积神经网络的输入，即图像特征Fea的设计·卷积神经网络的设计，即Net·代价函数的设计 这篇论文主要是将二维的人脸特征标记问题转为了二维图像与三维模型的拟合问题。主要是通过级联的卷积神经网络，将二维人脸图像与三维变形模型进行拟合；使用3D变形模型构建得到的三维人脸并映射到二维人脸图像上的结果。 重点还是在于大姿势人脸的一个三维重建，主要学习的也是三维映射所需要的各种参数。从而使得正面的三维人脸在映射过程中随着各个参数的变化而转变人脸呈现的角度。 Deep face feature,DFF本文基于深度学习的图像特征提取方法 提出了一个由三个卷积网络组成的从粗到细的学习框架，经过训练可将 深度人脸特征(DFF)是使用不同视图渲染的人脸图像之间的对应关系进行训练的。 使用经过训练的DFF模型，可以为人脸图像的每个像素提取特征向量，从而区分不同的人脸区域，并被证明比通用的特征描述符更有效的人脸相关任务，如匹配和对齐。 使用CNN提高回归精度，对于人脸图像的每个像素，使用该模型提取一个高维特征，该特征可以在无约束条件下准确地表示不同人脸图像上的相同解剖面部点 ·使用一个构造良好的训练数据集和一个新的损失函数 ·利用新设计的人脸特征提取器 本文介绍了传统的经典人脸对齐方法，提出它们在不同视角下的局限性。 DFF是一种基于深度学习的人脸特征提取方法，利用不同姿势和表情下的对应关系，得到不同人脸图像中相同语义像素的深度人脸特征具有相似的值。因此，DFF能够捕获人脸图像的全局结构信息，在匹配、对齐等人脸相关任务上比SIFT等常用特征描述符更有效。作者提出了一种新的基于DFF的人脸对齐方法，实现了最先进的大姿态人脸对齐结果。"},{"title":"基于深度学习的2维人脸特征点自动检测","date":"2022-03-07T14:00:00.000Z","url":"/2022/03/07/paperreading-1/","tags":[["Feature Point Punctuation Algorithm","/tags/Feature-Point-Punctuation-Algorithm/"]],"categories":[["Feature Point Punctuation Algorithm","/categories/Feature-Point-Punctuation-Algorithm/"]],"content":"基于深度学习的2维人脸特征点自动检测 基于级联卷积神经网络的方法DCNN(deep convolu- tional neural network)论文出处：Sun Y, Wang X G and Tang X O. 2013. Deep convolutional network cascade for facial point detection / / Proceedings of 2013 IEEE Con- ference on Computer Vision and Pattern Recognition. Portland, USA: IEEE: 3476-3483 [DOI: 10. 1109 / CVPR. 2013. 446]进展：1.将CNN应用到人脸特征点识别2.提出了级联卷积神经网络3.对于关键点初始化进行了创新，避免陷入局部最优的漩涡4.借鉴了局部共享权值的概念，人的五官为定点位置出现，故不需采用全局权值共享研究方法:带有三级的级联卷积神经网络提取全局特征级联卷积神经网络由三个level构成，level-1由三个CNN构成；level-2与level-3分别由十个CNN识别五个特征点但输入不同Level-1：三个CNN：F1（face 1）、EN1（eye，nose）、NM1（nose，mouth）；F1的input3939，输出5个点；EN1input3931，输出是3个点；NM1的input3931，输出3个点。Level-1的输出是由三个CNN输出取平均得到局部权值共享来源文献2012-CVPR Learning hierarchical representations for face verification with convolutional deep belief networks*研究结果：解决了容易陷入全局最优的问题，使用卷积神经网络的提取能力，由粗到细的级联回归，提高了检测精度 LSTM(long short-term memory)优势：以模型尺寸的名义上的增加显着增强了全卷积网络的性能，并且需要对数据集进行最少的预处理;通过注意力长期短期记忆完全卷积网络（ALSTM-FCN）改善时间序列分类;模型输入的通用性，因此它在多种序列建模任务（例如文本分析，音乐识别和语音检测）上具有广泛的适用性;由于其体积小，效率高，因此可以轻松地部署到实时系统或嵌入式系统中 基于深度端到端回归的方法DeCaFA(deep convolutional cascade for face alignment)进展：为每一定位任务生成特征点注意图；加权中间监督以及各阶段之间的有效特征融合；允许学习以端到端的方式逐步完善注意图；训练数据集少且精度合理研究方法：采用端到端的深度全卷积级联结构保持全空间分辨率，使用多个具有空间softmax的链式转移层来为每个地标对齐任务生成地标式注意图。加权中间监督，以及各阶段之间的有效特征融合，允许学习以端到端方式逐步细化注意图。 基于自动编码器网络的方法自动编码器(auto-encoder)论文出处：Zhang C Q, Liu Y Q and Fu H Z. 2020. AE2-nets: autoencoder in autoencoder networks/ / Proceedings of 2019 IEEE / CVF Conference on Computer Vision and Pattern Recognition ( CVPR ). Long Beach, USA: IEEE: 2572-2580 [ DOI: 10. 1109 / CVPR. 2019. 00268]进展：把来自异构视图的内在信息编码为一个全面的表示，并且自动的平衡不同视图的互补性和一致性由内部AE网络编码重建原始视图的输入，然后由外部AE网络对内部AE网络的输出进行重构编码表示。每个单一视图。本文的主要贡献概括如下：•我们提出了一种新颖的无监督多视图表示学习框架 - 自动编码器网络中的自动编码器（AE2-Nets），用于异构数据，可以灵活地将多个异构视图集成到一个完整的表示中。•新型嵌套自动编码器网络可以联合形成视图特定表示学习和多视图表示学习 - 内部自动编码器网络有效地从每个单个视图中提取信息，而外部自动编码器网络模拟降级过程进行编码从每个单一视图的内在信息到共同的完整表示。上图为算法核心表示，利用自动编码器思想(1)式为autoencoder公式，一共M+1层，中间有M层的连接，前M/2层是encoder过程，即把图片降维为一个低维的表示，后面的M/2层就为decoder过程，即把已经降维的表示再升维为原来的维度，图(c)的降维就是获取了autoencoder的第M/2层，因为这一层就是中间的维度较低的那一层，用这一层表示肯定更简单一点，因为维度较低，然后用一个隐藏层H来表示这个M/2层，H作为隐藏层，先给H层赋任意值，通过loss function优化H，最终找到适合网络代码具体执行过程：1.先作autoencoder，做autoencoder时候优化的是autoencoder网络但是同时要计算encoder产生的中间层和gi(L,v)的loss，把autoencoder的loss和中间层和gi(L,v)产生的loss加起来优化autoencoder网络。2.根据训练好的autoencoder网络，然后再优化H到中间层的网络3.现在已知H到中间层的网络，反过来找一个最佳输入4.循环执行前者 基于小滤波器的深度卷积神经网络(deep con- volution neural network with small filter, DCNNSF)发展：解决在复杂环境下，如姿势不同、光照条件以及遮挡等因素导致传统人脸特征点检测算法的精度大幅度下降的问题有效减少参数的数量，避免了过拟合的问题，使模型达到更高精度和鲁棒性。研究方法：采用基于小滤波器的神卷积神经网络具体架构解决人脸5特征点预测问题网络的 I（Data）层为网络输入层，该层为整个网络输入一张 128×128的三通道彩色图片。网络的 Conv1_1层为第 一个卷积层（ConvolutionLayer），该层由 32个特征图谱（Feature Map）构成。特征图中的每个单元与输入层的一个 3×3的相邻区域相连，卷积的输入区域大小是 3×3，因此特征图是由 32个大小为3×3的卷积核从输入图片中卷积得到，每个特征图谱内参数共享。其输出 32个大小为 128×128的特征图。网络的 Conv1_2层为第二个相连 3×3卷积层，该层输入信息为 Conv1_1层中输出的 32个大小为 128×128特征图信息，其输出 64个大小为 128×128的特征图。网络的 Conv1_3层为第三个相连 3×3卷积层，该层输入信息为 Conv1_2层中输出的 64个大小为 128×128特征图信息。它的作用是与 Conv1_1层、Conv1_2层叠加，从而替代大型 7×7卷积层，减少参数，并达到相同的卷积效果。该层最终将 96个大小为 128×128的特征图输出到下一层中。网络的 Pool1层为第一个池化层（Pooling Layer），该层对输入的特征图进行压缩，首先可以使特征图变小，有效简化网络计算复杂度；其次对特征进行压缩，以便于提取主要特征。Pool1接收输入为 Conv1_3层 64个大小为 128×128的特征图信息 ，使用最大池化（Max Pooling），池化领域 2×2最大领域，由于步长为 2，所输出的特征图为 128/2 = 64个，数据规模为原来的 1/4，最终输出 96个大小为 128×128的特征图。其他卷积层与池化层的具体操作同 Conv1_1、Conv1_2、Conv1_3、Pool1层类似，每个大型的卷积层之间初始参数以 32、64、96、128、160的方式递增，除了第一个大型的卷积层，其他大型卷积层中的 3×3小卷积层之间的参数都以倍增的形式增加。因此 Pool5层最终输出 320个 4×4的特征图。Conv6_1、Conv6_2与 Conv7层都为全连接层 。Conv6_1层接收来自 Pool5输出的特征图信息，将其转化为一维向量，用来表示整个面部点，并将结果输出到Conv6_2层中，使模型表达能力和非线性表达能力得到增强，最终通过 Conv7层输出一维向量（如式（1）所示），其中 x、y代表 10个人脸坐标信息。2维人脸特征点自动标定存在问题及发展趋势：摘自：Xu Y L, Zhao J L, Lyu Z H, Zhang Z M, Li J H and Pan Z K. 2021. Automatic facial feature points location based on deep learning: a review. Journal of Image and Graphics,26(11):2630-2644(徐亚丽,赵俊莉,吕智涵,张志梅,李劲华,潘振宽. 2021. 深度学习人脸特征点自动定位综述. 中国图象图形学报,26(11):2630-2644)[DOI:10. 11834 / jig. 200278]在 2 维图像数据中,具有真实特征点标签的训练数据非常丰富。针对 2 维数据的特征点自动定位方法的研究已比较深入,包括级联卷积神经网络、深度端到端回归网络、自动编码器网络及其他改进CNN的网络,已取得较为理想的效果,并在人脸识别和人脸编辑等实际任务中广泛应用。 存在的主要问题和未来发展趋势包括以下方面:1)从特征提取角度入手,使用自动学习更符合人脸图像的特征来优化特征点定位性能。2)端到端的学习不需要手动标注但需要依靠大量数据,适用于大规模的数据集。所以端到端的方法可以从数据增强、数据生成以及如何构建小样 本深度学习网络结构方面继续研究。3)自动编码器依据特定的样本进行训练,因此 其适用性很大程度上局限于与训练样本相似的数据且很容易过拟合,如何降低时间复杂度、避免陷入局 部最小值是亟待解决的问题。4)对于姿态变化较大的图像,如何更好地处理姿态、初始化以及减少计算成本仍然值得探讨。综上所述,针对 2 维数据的人脸特征点自动标定研究,需要从网络结构、数据增强、特征选取和 初始化等方面创新。 同时,减少数据集的局限性、避免过拟合和减少时间复杂度也是需要考虑的问题。"},{"title":"关于深度学习人脸特征点自动定位的学习","date":"2022-02-25T14:00:00.000Z","url":"/2022/02/25/Automatic-facial-feature-points-location/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"关于深度学习人脸特征点自动定位的学习 生成式对抗网络原理深度学习模型分为：一，判别式模型(GoogLeNet,ResNet,Faster RCNN,YOLO等等)；二，生成式模型研究意义：➢ 是对我们处理高维数据和复杂概率分布的能力很好的检测➢当面临缺乏数据或缺失数据时，我们可以通过生成模型来补足。比如，用在半监督学习中。➢可以输出多模态（multimodal）结果➢最大似然估计法以真实样本进行最大似然估计，参数更新直接来自于样本数据，导致学习到的生成模型受到限制。➢近似法由于目标函数难解一般只能在学习过程中逼近目标函数的下界，并不是对目标函数的逼近。➢马尔科夫链方法计算复杂度高生成式对抗网络Generative Adversarial Networks (GANs)生成网络G的输入是一个来自常见概率分布的随机噪声矢量，输出是计算机生成的图片；判别网络D的输入是图片x，x可能是真实图片也可能是生成图片，判别网络D的输出是一个标量，用来代表x是真实图片的概率。GAN的核心思想来源于博弈论的纳什均衡，它设定参与游戏双方分别为一个生成器和一个判别器，生成器的目的是尽量去学习真实的数据分布，而判别器的目的是尽量正确判别输入数据是来自真实数据还是来自生成器；为了取得游戏胜利，两个参与者需要不断优化，各自提高自己的生成能力和判别能力，这个学习的优化过程就是寻找二者之间的一个纳什均衡。 关于深度特征点识别网络(deep land- mark identification network,DLIN) 选摘：在此背景下，我们提出了一种全自动多线性算法，可以在大量不同身份和表情的3D人脸上建立密集对应(≥13,500点)。我们首先训练一个深度卷积神经网络(CNN)来识别地标。cnn已被广泛应用于二维纹理图像，其中地面真实标签的训练数据是丰富可用的。然而，对于3D人脸，却缺乏包含面部形状、种族和表情显著变化的训练数据。我们的深度地标识别网络(DLIN)使用商业软件(FaceGenTM)生成的合成3D图像进行训练，能够以高准确度和高效率检测11个生物意义重大的[28]面部地标。接下来，我们使用测地进化的关卡集曲线[29]将3D人脸划分为五个Voronoi区域。在每个区域内检测出一组稀疏的识别关键点，并用于弹性对齐两个给定人脸的相应区域形状。然后通过所有训练人脸的区域顶点之间的最近邻匹配实现密集对应。最后，由密集对应的人脸构造三维可变形模型，通过迭代优化拟合可变形模型，将对应信息传递到不可见的三维人脸。 原文： In this context, we propose a fully automatic multilinear algorithm that can estab-lish dense correspondence (≥ 13, 500 points) over a large number of 3D human faceswith varying identities and expressions. We first train a deep Convolutional NeuralNetwork (CNN) for landmark identification. CNNs have been extensively used for 2Dtexture images where the training data with ground truth labels is abundantly available.However, in case of 3D faces there is a dearth of training data that contains significantvariation in facial shape, ethnicity and expressions. Our Deep Landmark IdentificationNetwork (DLIN) is trained on synthetic 3D images generated from a commercial soft-ware (FaceGenTM) and is able to detect 11 biologically significant [28] facial landmarkswith high accuracy and efficiency. Next, we divide the 3D face into five Voronoi regionsaround a subset of these landmarks using geodesically evolved level set curves [29]. Asparse set of discriminative keypoints are detected within each region and used to elas-tically align the corresponding region shapes of two given faces. Dense correspondenceis then achieved through nearest neighbour matches between the region vertices of alltraining faces. Finally, a 3D deformable model is constructed from the densely corre-sponding faces and the correspondence information is transferred to unseen 3D facesby fitting the deformable model in an iterative optimization. 关于面向多姿态的人脸对齐的 3D 解决方案(3D dense face alignment,3DDFA) 摘要：人脸对齐技术是将人脸模型与图像进行拟合，提取人脸像素的语义含义，是CV社区研究的重要课题。然而，大多数算法设计的脸在小到中等的姿态(低于45◦)，缺乏的能力对齐脸在大的姿态高达90◦。面临的挑战有三个方面:首先，常用的基于地标的人脸模型假设所有地标都是可见的，因此不适合用于剖面视图。其次，从正面到侧面，脸部外观在大的姿势中变化更明显。第三，在大的姿势中标记地标是非常具有挑战性的，因为隐形地标需要猜测。在本文中，我们提出了一种新的对齐框架，即3D密集人脸对齐(3D Dense Face alignment, 3DDF a)，该框架通过卷积神经网络(convolutional neutral network, CNN)将密集的3D人脸模型拟合到图像上。我们还提出了在剖面视图中合成大规模训练样本的方法来解决第三个数据标注问题。在具有挑战性的AFLW数据库上的实验表明，我们的方法比最先进的方法取得了显著的改进。 原文： Face alignment, which fits a face model to an imageand extracts the semantic meanings of facial pixels, hasbeen an important topic in CV community. However , mostalgorithms are designed for faces in small to medium pos-es (below 45◦), lacking the ability to align faces in largeposes up to 90◦. The challenges are three-fold: Firstly, thecommonly used landmark-based face model assumes thatall the landmarks are visible and is therefore not suitablefor profile views. Secondly, the face appearance variesmore dramatically across large poses, ranging from frontalview to profile view. Thirdly, labelling landmarks in largeposes is extremely challenging since the invisible landmarkshave to be guessed. In this paper , we propose a solution tothe three problems in an new alignment framework, called3D Dense Face Alignment (3DDF A), in which a dense 3Dface model is fitted to the image via convolutional neutralnetwork (CNN). We also propose a method to synthesizelarge-scale training samples in profile views to solve thethird problem of data labelling. Experiments on the chal-lenging AFLW database show that our approach achievessignificant improvements over state-of-the-art methods. "},{"title":"蓝桥杯","date":"2022-02-05T12:00:00.000Z","url":"/2022/02/05/lanqiaobei-1/","tags":[["蓝桥杯","/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/"]],"categories":[["蓝桥杯","/categories/%E8%93%9D%E6%A1%A5%E6%9D%AF/"]],"content":"（持续更新） 递归与递推概念递归：从已知问题的结果出发，用迭代表达式逐步推算出问题的开始的条件，即顺推法的逆过程，称为递归。递推：递推算法是一种用若干步可重复运算来描述复杂问题的方法。递推是序列计算中的一种常用算法。通常是通过计算机前面的一些项来得出序列中的指定象的值。递归与递推区别：相对于递归算法,递推算法免除了数据进出栈的过程，也就是说,不需要函数不断的向边界值靠拢,而直接从边界出发,直到求出函数值。 例题题目描述100 可以表示为带分数的形式：100 = 3 + 69258 / 714。还可以表示为：100 = 82 + 3546 / 197。注意特征：带分数中，数字1 ~ 9分别出现且只出现一次（不包含0）。类似这样的带分数，100 有 11 种表示法。输入从标准输入读入一个正整数N (N&lt; 10001000)输出程序输出该数字用数码1 ~ 9不重复不遗漏地组成带分数表示的全部种数。注意：不要求输出每个表示，只统计有多少表示法！样例输入100样例输出11题目描述小明正在玩一个“翻硬币”的游戏。桌上放着排成一排的若干硬币。我们用 表示正面，用 o 表示反面（是小写字母，不是零）。比如，可能情形是： oo oooo如果同时翻转左边的两个硬币，则变为：oooo oooo现在小明的问题是：如果已知了初始状态和要达到的目标状态，每次只能同时翻转相邻的两个硬币,那么对特定的局面，最少要翻动多少次呢？我们约定：把翻动相邻的两个硬币叫做一步操作。 输入两行等长的字符串，分别表示初始状态和要达到的目标状态。每行的长度&lt; 1000 输出一个整数，表示最小操作步数。 样例输入*o o o**o** o o* 样例输出1 题目描述**Time Limit: 1000 msMemory Limit: 256 mb 在一个旧式的火车站旁边有一座桥，其桥面可以绕河中心的桥墩水平旋转。一个车站的职工发现桥的长度最多能容纳两节车厢，如果将桥旋转180度，则可以把相邻两节车厢的位置交换，用这种方法可以重新排列车厢的顺序。于是他就负责用这座桥将进站的车厢按车厢号从小到大排列。他退休后，火车站决定将这一工作自动化，其中一项重要的工作是编一个程序，输入初始的车厢顺序，计算最少用多少步就能将车厢排序。 输入输出格式输入描述:输入文件有两行数据，第一行是车厢总数N（不大于10000），第二行是N个不同的数表示初始的车厢顺序。输出描述:一个数据，是最少的旋转次数。输入输出样例输入样例:44 3 2 1输出样例:6思路：实则为交换排序问题，采用冒泡排序即可 二分和前缀和概念二分就是断确定边界的过程，二分一定有解所以当二分到无法再分时的那个元素就是解，即l=r指向的值。 二分不一定要有单调性，二分的本质是寻找某种性质的分界点。只要可以找到某种性质，使得区间的前半部分满足，后半部分不满足，那么就可以用二分把这个分界点找到。 前缀和预处理求出s[1~n]，然后就能快速求出任意数组任意区间里一段数的和，如求l ~ r区间的和，就是s[r]-s[l-1]，数组要从1开始从，s[0]=0，这样比如算s[1]-s[9]，就可以用s[9]-s[0],而s[0]其实就是0，没有元素，避免越界 S[i] = a[1] + a[2] + … a[i]a[l] + … + a[r] = S[r] - S[l - 1] 例题题目描述儿童节那天有K位小朋友到小明家做客。小明拿出了珍藏的巧克力招待小朋友们。小明一共有N块巧克力，其中第i块是Hi x Wi的方格组成的长方形。为了公平起见，小明需要从这 N 块巧克力中切出K块巧克力分给小朋友们。切出的巧克力需要满足： 形状是正方形，边长是整数 大小相同例如一块6x5的巧克力可以切出6块2x2的巧克力或者2块3x3的巧克力。当然小朋友们都希望得到的巧克力尽可能大，你能帮小Hi计算出最大的边长是多少么？输入第一行包含两个整数N和K。(1 &lt;= N, K &lt;= 100000)以下N行每行包含两个整数Hi和Wi。(1 &lt;= Hi, Wi &lt;= 100000)输入保证每位小朋友至少能获得一块1x1的巧克力。输出输出切出的正方形巧克力最大可能的边长。样例输入2 106 55 6样例输出2 问题描述给定一个长度为N的数列，A1, A2, … AN，如果其中一段连续的子序列Ai, Ai+1, … Aj(i &lt;= j)之和是K的倍数，我们就称这个区间[i, j]是K倍区间。你能求出数列中总共有多少个K倍区间吗？输入格式第一行包含两个整数N和K。(1 &lt;= N, K &lt;= 100000)以下N行每行包含一个整数Ai。(1 &lt;= Ai &lt;= 100000)输出格式输出一个整数，代表K倍区间的数目。样例输入5 212345样例输出6 栈和队列概念由于大二上学期数据结构已学习，故略去，资料可参考书籍与ppt 例题题目描述给定序列 (a1, a2, · · · , an) = (1, 2, · · · , n)，即 ai = i。小蓝将对这个序列进行 m 次操作，每次可能是将 a1, a2, · · · , aqi 降序排列，或者将 aqi , aqi+1, · · · , an 升序排列。请求出操作完成后的序列。输入输入的第一行包含两个整数 n, m，分别表示序列的长度和操作次数。接下来 m 行描述对序列的操作，其中第 i 行包含两个整数 pi, qi 表示操作类型和参数。当 pi = 0 时，表示将 a1, a2, · · · , aqi 降序排列；当 pi = 1 时，表示将 aqi , aqi+1, · · · , an 升序排列。输出输出一行，包含 n 个整数，相邻的整数之间使用一个空格分隔，表示操作完成后的序列。样例输入3 30 31 20 2样例输出3 1 2题目描述小的时候，你玩过纸牌游戏吗？有一种叫做“拉马车”的游戏，规则很简单，却很吸引小朋友。其规则简述如下：假设参加游戏的小朋友是A和B，游戏开始的时候，他们得到的随机的纸牌序列如下：A方：[K, 8, X, K, A, 2, A, 9, 5, A]B方：[2, 7, K, 5, J, 5, Q, 6, K, 4]其中的X表示“10”，我们忽略了纸牌的花色。从A方开始，A、B双方轮流出牌。当到某一方出牌时，他从自己的纸牌队列的头部拿走一张，放到桌上，并且压在最上面一张纸牌上（如果有的话）。此例中，游戏过程：A出K，B出2，A出8，B出7，A出X，此时桌上的序列为：K,2,8,7,X当轮到B出牌时，他的牌K与桌上的纸牌序列中的K相同，则把包括K在内的以及两个K之间的纸牌都赢回来，放入自己牌的队尾。注意：为了操作方便，放入牌的顺序是与桌上的顺序相反的。此时，A、B双方的手里牌为：A方：[K, A, 2, A, 9, 5, A]B方：[5, J, 5, Q, 6, K, 4, K, X, 7, 8, 2, K]赢牌的一方继续出牌。也就是B接着出5，A出K，B出J，A出A，B出5，又赢牌了。5,K,J,A,5此时双方手里牌：A方：[2, A, 9, 5, A]B方：[Q, 6, K, 4, K, X, 7, 8, 2, K, 5, A, J, K, 5]注意：更多的时候赢牌的一方并不能把桌上的牌都赢走，而是拿走相同牌点及其中间的部分。但无论如何，都是赢牌的一方继续出牌，有的时候刚一出牌又赢了，也是允许的。当某一方出掉手里最后一张牌，但无法从桌面上赢取牌时，游戏立即结束。对于本例的初始手牌情况下，最后A会输掉，而B最后的手里牌为：9K2A62KAX58K57KJ5本题的任务就是已知双方初始牌序，计算游戏结束时，赢的一方手里的牌序。当游戏无法结束时，输出-1。输入输入为2行，2个串，分别表示A、B双方初始手里的牌序列。我们约定，输入的串的长度不超过30输出输出为1行，1个串，表示A先出牌，最后赢的一方手里的牌序。样例输入96J5A898QA6278A7Q973样例输出2J9A7QA6Q6889977 并查集和kmp概念并查集初始化将每个节点包装成一个个集合，每个集合中存放这个元素并使其父节点指向自己。建立三个数据结构。1.v → Element 表示集合的节点类，将原始输入元素包装成集合的类节点。2.Element → Element 一个将每个节点指向父节点的映射3.Element → size 这个Element指一个集合中，获得所有节点的祖先节点，他的父节点就是自己，作为一个集合的代表节点，并将其映射到集合的规模。并查集的相关操作查找代表节点findheap：对于一个集合中每一个节点，可以通过数据结构2寻找父节点，并不断往上遍历寻找。直到找到唯一的最顶层的祖先节点，此节点的父节点指向自己，就是一个集合的代表节点。集合合并union：对于两个集合，获得他们的代表节点。在数据结构3，获得各自的size，将size进行比较。对规模较小的集合，把其代表节点的父节点由指向自己变为指向规模较大的代表节点，这样就形成了一个新的集合，代表节点为原规模较大的集合的代表节点。查询find，判断两个节点是否属于一个集合：参考findheap过程，只需对两个节点findheap，得到各自代表节点，代表节点若是同一个，则为同一个节点。kmp算法从一个字符串中寻找连续子串的问题，令str为主串，k为寻找的部分算法很经典，大二上学期数据结构已学习，故不再赘述 例题题目描述给定一个长度为 N 的数组 A = [A1, A2, · · · AN ]，数组中有可能有重复出现 的整数。现在小明要按以下方法将其修改为没有重复整数的数组。小明会依次修改 A2,A3,··· ,AN。当修改 Ai 时，小明会检查 Ai 是否在 A1 ∼ Ai−1 中出现过。如果出现过，则 小明会给 Ai 加上 1 ;如果新的 Ai 仍在之前出现过，小明会持续给 Ai 加 1 ，直 到 Ai 没有在 A1 ∼ Ai−1 中出现过。当 AN 也经过上述修改之后，显然 A 数组中就没有重复的整数了。 现在给定初始的 A 数组，请你计算出最终的 A 数组输入第一行包含一个整数 N。 第二行包含N个整数A1,A2,··· ,AN对于 80% 的评测用例，1 ≤ N ≤ 10000。对于所有评测用例，1 ≤ N ≤ 100000，1 ≤ Ai ≤ 1000000。输出输出N个整数，依次是最终的A1,A2,··· ,AN。样例输入52 1 1 3 4样例输出2 1 3 4 5题目描述w星球的一个种植园，被分成 m n 个小格子（东西方向m行，南北方向n列）。每个格子里种了一株合根植物。这种植物有个特点，它的根可能会沿着南北或东西方向伸展，从而与另一个格子的植物合成为一体。如果我们告诉你哪些小格子间出现了连根现象，你能说出这个园中一共有多少株合根植物吗？输入第一行，两个整数m，n，用空格分开，表示格子的行数、列数（ 1&lt; m, n &lt; 1000）。接下来一行，一个整数k，表示下面还有k行数据(0 &lt; k &lt; 100000)接下来k行，第行两个整数a，b，表示编号为a的小格子和编号为b的小格子合根了。格子的编号一行一行，从上到下，从左到右编号。比如：5 4 的小格子，编号：1 2 3 45 6 7 89 10 11 1213 14 15 1617 18 19 20输出多少株样例输入5 4162 31 55 94 87 89 1010 1111 1210 1412 1614 1817 1815 1919 209 1313 17样例输出5 最小生成树与最短路径概念最小生成树一个连通图的生成树是一个极小的连通子图，它含有图中全部顶点，但只有足以构成一棵树的n-1条边。那么我们把构造连通网的最小代价生成树称为最小生成树。找连通网的最小生成树，经典的有两种算法，普里姆算法和克鲁斯卡尔算法。最短路径对于网图而言，最短路径就是两顶点之间经过的边上权值之和最少的路径，并且我们称路径上的第一个顶点是源点，第二个顶点是终点。而非网图可以理解为所有边的权值都为1的网。 例题题目描述有n块芯片，有好有坏，已知好芯片比坏芯片多。每个芯片都能用来测试其他芯片。用好芯片测试其他芯片时，能正确给出被测试芯片是好还是坏。而用坏芯片测试其他芯片时，会随机给出好或是坏的测试结果（即此结果与被测试芯片实际的好坏无关）。给出所有芯片的测试结果，问哪些芯片是好芯片。输入输入数据第一行为一个整数n，表示芯片个数。第二行到第n+1行为nn的一张表，每行n个数据。表中的每个数据为0或1，在这n行中的第i行第j列（1≤i, j≤n）的数据表示用第i块芯片测试第j块芯片时得到的测试结果，1表示好，0表示坏，i=j时一律为1（并不表示该芯片对本身的测试结果。芯片不能对本身进行测试）。（2≤n≤20）输出按从小到大的顺序输出所有好芯片的编号样例输入31 0 10 1 01 0 1*样例输出1 3 质数与快速幂概念思想：每一步都把指数分成两半，而相应的底数做平方运算。不仅能把非常大的指数给不断变小，所需要执行的循环次数也变小，而最后表示的结果却一直不会变。原理：(a b) % m = ((a % m) (b % m)) % m 扩展欧几里得与高斯消元概念同余两个整数a和b及模m，如果a%m=b%m，称a和b对m同余。同余也可以理解为a−b是m的倍数:m∣( a−b )，例如6∣(23 −11)，23和11对模6同余。同余符号计为a≡b(mod m)。逆元给出a和m，求解ax≡1(mod m)，即ax除m的余数是1。那么该方程的一个解x称为a模m的逆元，注意这样的x有很多，都称为逆。逆元与除法取模逆元一重要作用就是求除法的模，首先我们看一下模运算：加：(a+b)%m=(a%m+b%m)%m减：(a−b)%m=(a%m−b%m)%m乘：(a∗b)%m=(a%m∗b%m)%m除：(a/b)%m≠(a%m/b%m)%m对于除法可以用逆元求解设b的逆元是k，则：（a/b)%m=(a/b%m)(bk%m)=(a/b)bk%m=ak%m即(a/b)%m=(ak)%m费马小定理假如p是质数，且gcd(a,p)=1，那么 a(p-1)≡1（mod p）。即：假如a是整数，p是质数，且a,p互质(即两者只有一个公约数1)，那么a的(p-1)次方除以p的余数恒等于1。素数筛枚举所有小于数，看是否它能整除其他自然数，但实际上只需要枚举根号次。欧拉函数就是对于一个正整数n，小于n且和n互质的正整数（包括1）的个数，记作φ(n) 。欧拉函数的通式：φ(n)=n(1-1/p1)(1-1/p2)(1-1/p3)(1-1/p4)……(1-1/pn)其中p1, p2……pn为n的所有质因数，n是不为0的整数。φ(1)=1（唯一和1互质的数就是1本身）。欧拉函数的一些性质：① 当m,n互质时，有phi（mn）= phi（m）phi（n）；② 若i%p==0，有phi（ip） = p phi(i)；③ 对于互质x与p，有x^phi§≡1（mod p),因此x的逆元为x^(phi§-1)，即欧拉定理。（特别地，当p为质数时，phi（p）=p-1,此时逆元为x^(p-2)，即费马小定理）④ 当n为奇数时，phi(2n)=phi(n)⑤ 若x与p互质，则p-x也与p互质，因此小于p且与p互质的数之和为phi(x)x/2;⑥N&gt;1，不大于N且和N互素的所有正整数的和是 1/2 N eular(N)。⑦若(N%a == 0 &amp;&amp; (N/a)%a==0) 则有:E(N)=E(N/a)a;⑧若(N%a==0 &amp;&amp; (N/a)%a!=0) 则有:E(N)=E(N/a)*(a-1); 欧拉定理若gcd(a,p)=1,则a^φ(p) ≡ 1 (mod p) 这个定理可以求乘法逆元（乘法逆元讲解）。而且包含了费马小定理（a ^ (p-1) ≡ 1 (mod p)要求p为质数），因为当p为质数时φ(p) = p-1。 背包问题 常用代码模板——基础算法 "},{"title":"猫狗大战：迁移学习及应用","date":"2022-01-26T14:00:00.000Z","url":"/2022/01/26/machine-learning-11/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"猫狗大战案例介绍 1.迁移学习2.Dataset数据集和TFRecord数据集的制作与应用 Cats vs. Dogs（猫狗大战）是Kaggle上的一个竞赛，利用给定的数据集，用算法实现猫和狗的识别 tf.data.Dataset数据集制作Dataset数据集面对一堆格式不一的原始数据文件，数据预处理的过程往往十分繁琐，甚至比模型的设计还要耗费精力。• 处理各种赃数据• 设计 Batch 的生成方式TensorFlow 提供了tf.data这一模块，包括了一套灵活的数据集构建API，能够帮助快速、高效地构建数据输入的流水线，尤其适用于大数据量场景。tf.data.Dataset类是tf.data的核心，提供了对数据集的高层封装tf.data.Dataset由一系列的可迭代访问的元素（element）组成，每个元素包含一个或多个张量用 tf.data.Dataset.from_tensor_slices()是建立 tf.data.Dataset 的基本方法，适用于数据量较小（能够整个装进内存）的情况Dataset数据集对象的预处理tf.data.Dataset 类提供了多种数据集预处理方法，常用的包括：• Dataset.map(f) 对数据集中的每个元素应用函数f• Dataset.shuffle(buffer_size) 将数据集打散，实现乱序• Dataset.batch(batch_size) 将数据集分成批次• Dataset.prefetch() 预取出数据集中的size个元素• Dataset.take () 截取数据集中的前size个元素猫狗大战：Dataset数据集构建确定样本图像的标签值：·文件名包含分类标签信息·不同类别的文件放不同的目录数据读取·定义数据读取函数·定义解码图片和调整图片大小的函数——读取图片文件并解码，调整图片的大小并标准化建立猫狗大战Dataset数据集·读取图像数据并处理·乱序——Dataset.shuffle(buffer_size) ：将数据集打乱（设定一个固定大小的缓冲区（Buffer），取出前 buffer_size 个元素放入，并从缓冲区中随机采样，采样后的数据用后续数据替换）·分批——Dataset.batch(batch_size)：将数据集分成批次，即对每 batch_size 个元素，在第 0 维合并成为一个元素定义准备Dataset数据集的函数读取训练数据并处理Dataset数据集的使用把Dataset数据集转换为迭代器显示一批样本的图像和标签 基于VGG16的迁移学习模型构建与应用迁移学习利用已经训练好的模型作为新模型训练的初始化的学习方式。• 所需样本数量更少；• 模型达到收敛所需耗时更短• 比如：网络在Cifar-10数据集上迭代训练5000次收敛，将一个在Cifar-100上训练好的模型迁移至Cifar-10上，只需1000次就能收敛。• 当新数据集比较小且和原数据集相似时• 当算力有限时微调（finetuining）(1) trainable参数变动在进行Finetuning对模型重新训练时，对于部分不需要训练的层可以通过设置trainable=False来确保其在训练过程中不会被修改权值；(2) 加上特定的全连接层预训练的VGG是在ImageNet数据集上进行训练的，对1000个类别进行判定，若希望利用已训练模型用于其他分类任务，需要修改最后的全连接层。编译模型读取训练数据模型训练·定义超参数·进行训练模型存储·需要安排pyyaml包，pip install pyyaml·将模型结构和模型权重参数分开存储 应用模型模型装载·恢复模型的结构·导入模型的权重参数定义读取测试集图片的函数定义预测函数执行预测 TFRecord文件与应用TFRecordTFRecord 是 TensorFlow 中的数据集中存储格式。将数据集整理成 TFRecord 格式后，TensorFlow 就可以高效地读取和处理这些数据集，从而更高效地进行大规模的模型训练。TFRecord 可以理解为一系列序列化的 tf.train.Example 元素所组成的列表文件，而每一个 tf.train.Example 又由若干个 tf.train.Feature 的字典组成。TFRecord格式数据文件处理过程将形式各样的数据集整理为 TFRecord 格式，可以对数据集中的每个元素进行以下步骤：1）读取该数据元素到内存；2）将该元素转换为 tf.train.Example 对象（每一个 tf.train.Example 由若干个 tf.train.Feature 的字典组成，因此需要先建立 Feature 的字典）；3）将该 tf.train.Example 对象序列化为字符串，并通过一个预先定义的 tf.io.TFRecordWriter 写入 TFRecord 文件。读取 TFRecord 数据可按照以下步骤：1）通过 tf.data.TFRecordDataset 读入原始的 TFRecord 文件（此时文件中的 tf.train.Example 对象尚未被反序列化），获得一个 tf.data.Dataset 数据集对象；2）通过 Dataset.map 方法，对该数据集对象中的每一个序列化的 tf.train.Example 字符串执行 tf.io.parse_single_example 函数，从而实现反序列化。读取数据集的图片文件名列表及标签定义生成TFRecord格式数据文件函数TFRecord格式tf.train.Feature 支持三种数据格式：· tf.train.BytesList ：字符串或原始 Byte 文件（如图片），通过 bytes_list 参数传入一个由字符串数组初始化的 tf.train.BytesList 对象；· tf.train.FloatList ：浮点数，通过 float_list 参数传入一个由浮点数数组初始化的 tf.train.FloatList 对象；· tf.train.Int64List ：整数，通过 int64_list 参数传入一个由整数数组初始化的 tf.train.Int64List 对象。如果只希望保存一个元素而非数组，传入一个只有一个元素的数组即可。生成TFRecord格式数据文件定义TFRecord数据文件解码函数定义读取TFRecord文件，解码并生成Dataset数据集的函数读取TFRecord文件，解码并生成Dataset数据集"},{"title":"电影评论情感分析：RNN循环网络原理及自然语言处理NLP应用","date":"2022-01-20T12:00:00.000Z","url":"/2022/01/20/machine-learning-10/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"电影评论情感分析：RNN循环网络原理及自然语言处理NLP应用 电影评论情感分析案例描述情感分析（sentiment analysis），又称意见挖掘、倾向性分析，是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程。本例使用IMDb网络电影数据集，对电影评论的情感倾向进行分析。电影评论情感分析IMDb数据集IMDb全称Internet Movie Database(互联网电影资料库),是一个关于电影演员、电影、电视节目、电视明星、电子游戏和电影制作的在线数据库。利用模型进行预测对影评文字进行数据预处理得到特征，通过特征建造深度学习模型，通过模型预测分出正面评价与负面评价 自然语言处理基础知识分词自然语言处理常见特征：• 单独词• 词的n元组• 单独词或n元组出现的频率• 词的词性• 词的位置……结巴分词• 三种分词模式：① 精确模式：试图将句子最精确地切开，适合文本分析② 全模式：把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义③ 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词• 支持繁体分词• 支持自定义词典• jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型• jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词• jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的generator，可以使用 for 循环来获得分词后得到的每一个词语，或者用jieba.lcut 以及 jieba.lcut_for_search 直接返回 list• 开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词 • 用法： jieba.load_userdict(file_name)• file_name 为文件类对象或自定义词典的路径• 词典格式：一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。 • file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码词的数字化表示方法与词嵌入词的表示方法NLP的需要寻求恰当的文本表示方法分词之后，词还需要表示成机器容易理解的形式，常见表示方法有：独热编码（One-Hot）词嵌入• 将词汇嵌入到低维的连续向量空间中，即词被表示为实数域上的向量• 能捕捉到词汇间的联系，比如，通过计算两个词嵌入的余弦值得到两个词汇的相关程度• 运算速度快• 能提高情感分析、机器翻译等众多自然语言处理问题的效果词嵌入（word embedding）一个词嵌入是一个稠密浮点数向量（向量长度可以设置）它们是可以训练的参数一般词嵌入是8维（对于小型数据集）到1024维（大型数据集）更高的维度嵌入可以捕获词之间更细的关系，但是需要更多数据去学习 常见的词嵌入模型有word2vec、GloVe等 word2vec通过对具有数十亿词的新闻文章进行训练，Google提供了一组词向量的结果，可以从获取GloVe（Wikipedia 2014+ Gigaword 5）GloVe词嵌入可以从下面地址获得：如果想要训练中文文本，这里有一些预训练的中文词嵌入可用： IMDB数据集获取与处理（非TF集成模式）非预设数据处理如果是自有数据，或者网络其他数据，需要写一套处理数据的程序基本思路：1 、获取数据，确定数据格式 规范；2 、文字分词。英文分词可以按照空格分词，中文分词可以参考 jieba ；3 、建立词索引表，给每个词一个数字索引 编号；4 、段落文字转为词索引 向量；5 、段度文字转为词嵌入 下载数据集及解压 数据读取 数据处理 texts_to_sequences(texts)texts：待转为序列的文本列表返回值：序列的列表，列表中每个序列对应于一段输入文本填充序列pad_sequenceskeras.preprocessing.sequence.pad_sequences(sequences, maxlen=None,dtype=’int32’, padding=’pre’, truncating=’pre’, value=0.)参数sequences：浮点数或整数构成的两层嵌套列表maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将填0.dtype：返回的numpy array的数据类型padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断value：浮点数，此值将在填充时代替默认的填充值0 构建模型 如：tf.keras.layers.Embedding(input_dim=3000,output_dim=50,input_length=200输入的整型矩阵size： (batch, 200)词典的词汇数：3000输出shape： (batch, 200, 50) 循环神经网络RNN与LSTM介绍数据的时序与含义很多问题具有时序性，自然语言处理、视频图像处理、股票交易信息等等多层全连接的神经网络或者卷积神经网络都只能根据当前的状态进行处理，不能很好地处理时序问题RNN的模型结构普通RNN的不足：梯度消失与梯度爆炸长距离依赖问题 LSTM网络结构（改进）传统RNN每一步的隐藏单元只是执行一个简单的tanh或ReLU操作。LSTM基本结构和RNN相似，主要不同是LSTM对隐含层进行了改进在LSMT中，每个神经元相当于一个记忆细胞（Cell）基本状态LSTM的关键是细胞的状态，细胞的状态类似于传送带，直接在整个链路上运行，只有一些少量的线性交互门 (Gate)LSTM有通过精心设计的称作“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。包含一个Sigmoid神经网络层和一个点乘操作 。Sigmoid层输出0到1之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”1 代表“允许任何量通过”LSTM 拥有三个门，来保护和控制细胞状态。遗忘门：决定丢弃上一步的哪些信息。该门会读取 ht−1和 xt，输出一个在 0 到 1 之间的数值给每个在细胞状态 Ct−1中的数字。1 表示“完全保留”，0 表示“完全舍弃”输入门：决定加入哪些新的信息。tanh层创建一个新的候选值向量 Ct~，加到状态中。这里sigmoid层称“输入门层”，起到一个缩放的作用。这两个信息产生对状态的更新。形成新的细胞状态 旧的细胞状态 Ct−1与 ft 相乘来丢弃一部分信息 再加上 it∗Ct~（i是input输入门），生成新的细胞状态Ct输出门：决定输出哪些信息。把Ct输给 tanh 函数，得到一个候选的输出值运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去LSTM相比于RNN的优点： 梯度下降法中，RNN的梯度求解是累乘的形式，由于LSTM的隐含层更复杂，LSTM在梯度求解时，式子是累加的形式，所以LSTM的梯度往往不会是一个接近于0的小值，缓解了梯度消失的问题。 使用了门结构，解决了长距离依赖的问题。 基于LSTM结构的模型构建"},{"title":"2021 ICM Problem D-The Influence of Music","date":"2022-01-18T12:00:00.000Z","url":"/2022/01/18/ICM-2021-D/","tags":[["ICM","/tags/ICM/"]],"categories":[["Mathematical Modeling","/categories/Mathematical-Modeling/"]],"content":"2021 ICMProblem D: The Influence of Music 题目原文 题目原文(点击此栏展开) Music has been part of human societies since the beginning of time as an essential component of cultural heritage. As part of an effort to understand the role music has played in the collective human experience, we have been asked to develop a method to quantify musical evolution. There are many factors that can influence artists when they create a new piece of music, including their innate ingenuity, current social or political events, access to new instruments or tools, or other personal experiences. Our goal is to understand and measure the influence of previously produced music on new music and musical artists. Some artists can list a dozen or more other artists who they say influenced their own musical work. It has also been suggested that influence can be measured by the degree of similarity between song characteristics, such as structure, rhythm, or lyrics. There are sometimes revolutionary shifts in music, offering new sounds or tempos, such as when a new genre emerges, or there is a reinvention of an existing genre (e.g. classical, pop/rock, jazz, etc.). This can be due to a sequence of small changes, a cooperative effort of artists, a series of influential artists, or a shift within society. Many songs have similar sounds, and many artists have contributed to major shifts in a musical genre. Sometimes these shifts are due to one artist influencing another. Sometimes it is a change that emerges in response to external events (such as major world events or technological advances). By considering networks of songs and their musical characteristics, we can begin to capture the influence that musical artists have on each other. And, perhaps, we can also gain a better understanding of how music evolves through societies over time. Your team has been identified by the Integrative Collective Music (ICM) Society to develop a model that measures musical influence. This problem asks you to examine evolutionary and revolutionary trends of artists and genres. To do this, your team has been given several data sets by the ICM: 1) “influence_data” 1 represents musical influencers and followers, as reported by the artists themselves, as well as the opinions of industry experts. These data contains influencers and followers for 5,854 artists in the last 90 years. 2)“full_music_data”2 provides 16 variable entries, including musical features such as danceability, tempo, loudness, and key, along with artist_name and artist_id for each of 98,340 songs. These data are used to create two summary data sets, including: a.mean values by artist “data_by_artist”, b.means across years “data_by_year”. 1 These data were scraped from AllMusic.com2 These data were obtained from Spotify’s API Note: DATA provided in these files are a subset of larger data sets. These files CONTAIN THE ONLY DATA YOU SHOULD USE FOR THIS PROBLEM. To carry out this challenging project, the ICM Society asks your teams to explore the evolution of music through the influence across musical artists over time, by doing the following: ·Use the influence_data data set or portions of it to create a (multiple) directed network(s) of musical influence, where influencers are connected to followers. Develop parameters that capture ‘music influence’ in this network. Explore a subset of musical influence by creating a subnetwork of your directed influencer network. Describe this subnetwork. What do your ‘music influence’ measures reveal in this subnetwork? ·Use full_music_data and/or the two summary data sets (with artists and years) of music characteristics, to develop measures of music similarity. Using your measure, are artists within genre more similar than artists between genres? ·Compare similarities and influences between and within genres. What distinguishes a genre and how do genres change over time? Are some genres related to others? ·Indicate whether the similarity data, as reported in the data_influence data set, suggest that the identified influencers in fact influence the respective artists. Do the ‘influencers’ actually affect the music created by the followers? Are some music characteristics more ‘contagious’ than others, or do they all have similar roles in influencing a particular artist’s music? ·Identify if there are characteristics that might signify revolutions (major leaps) in musical evolution from these data? What artists represent revolutionaries (influencers of major change) in your network? ·Analyze the influence processes of musical evolution that occurred over time in one genre. Can your team identify indicators that reveal the dynamic influencers, and explain how the genre(s) or artist(s) changed over time? ·How does your work express information about cultural influence of music in time or circumstances? Alternatively, how can the effects of social, political or technological changes (such as the internet) be identified within the network? Write a one-page document to the ICM Society about the value of using your approach to understanding the influence of music through networks. Considering the two problem data sets were limited to only some genres, and subsequently to those artists common to both data sets, how would your work or solutions change with more or richer data? Recommend further study of music and its effect on culture. The ICM Society, an interdisciplinary and diverse group from the fields of music, history, social science, technology, and mathematics, looks forward to your final report. Your PDF solution of no more than 25 total pages should include:·One-page Summary Sheet.·Table of Contents.·Your complete solution.·One-page document to ICM society.·References list. Note: New for 2021! The ICM Contest now has a 25-page limit. All aspects of your submission count toward the 25-page limit: Summary Sheet, Table of Contents, Main Body of Solution, Images and Tables, One-page Document, Reference List, and any Appendices.AttachmentsWe provide the following four data files for this problem. THE DATA FILES PROVIDED CONTAIN THE ONLY DATA YOU SHOULD USE FOR THIS PROBLEM.1.influence_data.csv2.full_music_data.csv3.data_by_artist.csv4.data_by_year.csv Data Descriptions1.influence_data.csv(Data is encoded in utf-8 to allow for handling of special characters): -influencer_id: A unique identification number given to the person listed as influencer. (string of digits)-influencer_name: The name of the influencing artist as given by the follower or industry experts. (string)-influencer_main_genre: The genre that best describes the bulk of the music produced by the influencing artist. (if available) (string)-influencer_active_start: The decade that the influencing artist began their music career. (integer)-follower_id: A unique identification number given to the artist listed as follower. (string of digits)-follower_name: The name of the artist following an influencing artist. (string)-follower_main_genre: The genre that best describes the bulk of the music produced by the following artist. (if available) (string)-follower_active_start: The decade that the following artist began their music career. (integer) 2.full_music_data.csv 3. data_by_artist.csv 4. data_by_year.csv Spotify audio features from the “full_music_data”, “data_by_artist”, “data_by_year”:-artist_name: The artist who performed the track. (array)-artist_id: The same unique identification number given in the influence_data.csv file. (string of digits) Characteristics of the music:-danceability: A measure of how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. (float)-energy: A measure representing a perception of intensity and activity. A value of 0.0 is least intense/energetic and 1.0 is most intense/energetic. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. (float)-valence: A measure describing the musical positiveness conveyed by a track. A value of 0.0 is most negative and 1.0 is most positive. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). (float)-tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. (float)-loudness: The overall loudness of a track in decibels (dB). Values typical range between -60 and 0 db. Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). (float)-mode: An indication of modality (major or minor), the type of scale from which its melodic content is derived, of a track. Major is represented by 1 and minor is 0.-key: The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value for key is -1. (integer) Type of vocals:-acousticness: A confidence measure of whether the track is acoustic (without technology enhancements or electrical amplification). A value of 1.0 represents high confidence the track is acoustic. (float)-instrumentalness: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. (float)-liveness: Detects the presence of an audience in a track. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. (float) -speechiness: Detects the presence of spoken words in a track. The more exclusively speech- like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. (float)-explicit: Detects explicit lyrics in a track (true (1) = yes it does; false (0) = no it does not OR unknown). (Boolean) Description:-duration_ms: The duration of the track in milliseconds. (integer)-popularity: The popularity of the track. The value will be between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played more frequently now will have a higher popularity than songs that were played more frequently in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity are derived mathematically from track popularity. (integer)-year: The year of release of a track. (integer from 1921 to 2020)-release_date: The calendar date of release of a track mostly in yyyy-mm-dd format, however precision of date may vary and some just given as yyyy.-song_title (censored): The name of the track. (string) Software was run to remove any potential explicit words in the song title.-count: The number of songs a particular artist is represented in the full_music_data.csv file. (integer) 题目译文2021年ICM问题D：音乐的影响 题目译文(点击此栏展开) 音乐是人类社会的一部分，是文化遗产的重要组成部分。 作为理解音乐在人类集体经验中所扮演角色的努力的一部分，我们被要求开发一种方法来量化音乐进化。 当艺术家创作一首新音乐时，有许多因素可以影响他们，包括他们与生俱来的创造力、当前的社会或政治事件、获得新的乐器或工具的机会或其他个人经历。 我们的目标是了解和衡量以前制作的音乐对新音乐和音乐艺术家的影响。 一些艺术家可以列出十几个或更多的其他艺术家，他们说他们影响了他们自己的音乐作品。 还有人建议，影响可以用歌曲特征之间的相似程度来衡量，如结构、节奏或歌词。 音乐有时会发生革命性的变化，提供新的声音或节奏，例如当一个新的体裁出现时，或者有一个现有的体裁的重新发明(例如。 古典、流行/摇滚、爵士乐等。)。 这可能是由于一系列微小的变化，艺术家的合作努力，一系列有影响力的艺术家，或社会内部的转变。 许多歌曲有着相似的声音，许多艺术家为音乐流派的重大转变做出了贡献。 有时这些变化是由于一个艺术家影响另一个艺术家。 有时，它是针对外部事件（如重大世界事件或技术进步)而出现的变化）。 通过考虑歌曲网络及其音乐特征，我们可以开始捕捉音乐艺术家对彼此的影响。 也许，我们还可以更好地了解音乐是如何随着时间的推移在社会中演变的。 您的团队已经被整合集体音乐(ICM)协会确定，以开发一个衡量音乐影响的模型。 这个问题要求你研究艺术家和流派的进化和革命趋势。 为了做到这一点，ICM给了您的团队几个数据集： 1) “influence_data”1 代表音乐影响者和追随者，如艺术家自己报告的，以及行业 专家的意见。 这些数据包含了过去90年来5,854名艺术家的影响者和追随者。 2)“full_music_data”2 提供16个可变的条目，包括音乐特征，如舞蹈性，节奏，响度和键，以及98，340首歌曲的artist_name和artist_id。 这些数据用于创建两个汇总数据集，包括： a.艺术家“data_by_artist”的平均价值”，b.意味着多年的“data_by_year”。 1 这些数据来自All Music.com2 这些数据是从Spotify的API中获得的 注意：这些文件中提供的数据是较大数据集的子集。 这些文件包含了你要为这个问题使用的唯一数据。 为了实施这个具有挑战性的项目，ICM协会要求您的团队通过音乐艺术家随时间的影响来探索音乐的演变，具体做法如下： ·使用influence_data数据集或其部分创建一个（多个）有向网络的音乐影响，其中影响者连接到追随者。 开发在这个网络中捕获“音乐影响”的参数。 通过创建你的定向影响者网络的子网来探索音乐影响的子集。 描述这个子网。 你的“音乐影响”措施在这个子网中揭示了什么？ ·使用full_music_data和/或两个汇总数据集（与艺术家和年份）的音乐特征，以制定音乐相似性的度量。 用你的衡量标准，流派中的艺术家是否比流派之间的艺术家更相似？ ·比较体裁之间和体裁内部的相似性和影响。 一个体裁的区别是什么，体裁是如何随着时间的推移而变化的？ 有些体裁与其他体裁有关吗？ ·指出data_influence数据集中报告的相似性数据是否表明已识别的影响者实际上影响了各自的艺术家。 “影响者”真的会影响追随者创造的音乐吗？ 有些音乐特征是否比其他音乐更具有“传染性”，或者它们在影响特定艺术家的音乐方面都有类似的作用？ ·从这些数据中确定是否有可能意味着音乐进化的革命（重大飞跃）的特征？ 什么艺术家代表革命者（重大变革的影响者）在你的网络？ ·分析音乐演变的影响过程，随着时间的推移，在一个体裁。 你的团队能找出揭示动态影响者的指标，并解释流派或艺术家是如何随着时间的推移而变化的吗？ ·你的作品如何在时间或环境中表达关于音乐文化影响的信息？ 或者，如何在网络中识别社会、政治或技术变革（如互联网）的影响？ 写一份一页的文件给ICM协会，关于使用你的方法来理解音乐通过网络的影响的价值。考虑到这两个问题数据集仅限于某些类型，然后是两个数据集共同的艺术家，您的工作或解决方案将如何随着更多或更丰富的数据而变化？ 建议进一步研究音乐及其对文化的影响。 来自音乐、历史、社会科学、技术和数学领域的跨学科和多样化的ICM协会期待着您的最后报告。 您的PDF解决方案不超过25页，应包括：·一页汇总表。·目录。·你的解决方案。·提交ICM协会的一页文件。·参考资料清单。 注：2021年新！ ICM竞赛现在有25页的限制。 您提交的所有方面都按25页的限制计算： 摘要表、目录、解决方案主体、图像和表格、一页文档、参考列表和任何附录。附件我们为这个问题提供了以下四个数据文件。 提供的数据文件包含您应该用于此问题的唯一数据。1.influence_data.csv2.full_music_data.csv3.data_by_artist.csv4.data_by_year.csv数据描述1.influence_data.csv(数据以utf-8编码，以便处理特殊字符)： -influencer_id：给被列为影响者的唯一识别号码。 （一串数字）-influencer_name：由追随者或行业专家给出的影响艺术家的名字。 （字符串）-influencer_main_genre：最能描述影响艺术家创作的大部分音乐的体裁。 （如果可用)(字符串）-influencer_active_start：影响艺术家开始音乐生涯的十年。 （整数）-follower_id：给被列为跟随者的艺术家的唯一识别号码。 （一串数字）-follower_name：跟随影响艺术家的艺术家的名字。 （字符串）-follower_main_genre：最能描述以下艺术家创作的大部分音乐的体裁。 （如果可用)(字符串）-follower_active_start：以下艺术家开始音乐生涯的十年。 （整数） 2.full_music_data.csv 3. data_by_artist.csv 4. data_by_year.csv Spotify音频功能来自“full_music_data”、“data_by_artist”、“data_by_year”：-artist_name：表演曲目的艺术家。 （数组）-artist_id：influence_data.csv文件中给出的相同唯一标识号。 （一串数字） 音乐的特点：-舞蹈性：一种基于音乐元素的组合，包括节奏、节奏稳定性、节拍强度和整体规律性，来衡量一个曲目是否适合跳舞。 值0.0是最不可跳舞的，1.0是最可跳舞的。（浮动）-能量：表示对强度和活动的感知的度量。 值0.0是最不强烈/能量的，1.0是最强烈/ 能量的。 通常，充满活力的轨道会感觉快速、响亮和嘈杂。 例如，死亡金属有很高的能量，而巴赫的前奏在量表上得分很低。 这一属性的感知特征包括动态范围、感知响度、音色、起跳率和一般熵。 （浮动）-价态：一种描述曲目所传达的音乐积极性的度量。 值0.0最负，1.0最正。 高价音的轨道更积极(例如。 快乐，开朗，兴高采烈)，而低价音轨听起来更消极(例如。 悲伤，沮丧，愤怒)。 （浮动）-节奏：以每分钟节拍为单位的轨道的总体估计节奏(BPM)。 在音乐术语中，节奏是给定作品的速度或节奏，直接来源于平均节拍持续时间。 （浮动）-响度：轨道的整体响度，单位为分贝(dB)。 值在-60到0db之间的典型范围。 响度值在整个轨道上是平均的，对于比较轨道的相对响度是有用的。 声音是声音的质量， 是身体力量（振幅)的主要心理关联）。 （浮动）-模式：一种轨迹的情态（主要或次要）的指示，它的旋律内容是从其尺度的类型。主修用1表示，辅修用0表示。-关键：估计轨道的总体关键。 整数映射到点，使用标准的Pitch类表示法。 E.g。0=C，1=C♯/D♭，2=D等等。 如果没有检测到键，则键的值为-1。 （整数） 唱腔类型：-声学：衡量轨道是否声学（没有技术增强或电气放大)的置信度）。 值1.0表示高度置信，轨道是声学的。 （浮动）-工具性：预测一个曲目是否包含没有声音。 在这种情况下，“呜”和“啊”的声音被视为工具。 说唱或口语曲目显然是“声乐”。 器乐值越接近1.0，曲目不包含声乐内容的可能性就越大。高于0.5的值意在表示工具轨道，但随着值接近1.0，置信度更高。 （浮动）-活力：在赛道上检测观众的存在。 较高的活性值表示轨道被实时执行的概率增加。高于0.8的值提供了轨道运行的强烈可能性。 （浮动） -言语：在一个轨道上检测口语的存在。 更纯粹的演讲就像录音(例如。 脱口秀，有声书，诗歌)，属性值越接近1.0。 高于0.66的值描述了可能完全由口语构成的音轨。 值在0.33到0.66之间，描述可能包含音乐和语音的曲目，无论是在部分还是分层，包括说唱音乐。 低于0.33的值最有可能代表音乐和其他非语音类曲目。 （浮动）-显式：检测曲目中的显式歌词（真(1)=是的；假(0)=不，它没有或未知）。 （布尔值） 说明：-duration_ms：轨道的持续时间(毫秒。 （整数）-流行：赛道的流行。 值将在0到100之间，其中100是最受欢迎的。 流行度是通过算法来计算的，在很大程度上是基于赛道上的总播放次数和最近的播放次数。 一般来说， 现在播放频率更高的歌曲将比过去播放频率更高的歌曲更受欢迎。 重复轨道(例如。同一曲目来自单一和专辑)是独立的。 艺术家和专辑的流行在数学上来源于曲目的流行。 （整数）-年份：轨道发布的年份。 （1921年至2020年为整数）-release_date：轨道发布的日历日期大多采用yyyy-mm-dd格式，但日期的精度可能会有所不同，有些只是作为yyyy给出的。-song_title（审查）：轨道的名称。 运行软件是为了删除歌曲标题中任何潜在的显式单词。-计数：特定艺术家的歌曲数量表示在full_music_data.csv文件中。 （整数） 思路分析题目要求:（1）根据附件数据influence_data，构建音乐人之间的定向网络模型，或许会根据不同的音乐派别划分成多个子图，进而做出描述性分析与可视化；构建影响力指标表示音乐人的影响程度，可以参考复杂网络、聚类思想中的有向加权度指标。（2）根据附件数据full_music_data 探讨不同流派音乐内部的相似性，可以参考聚类有效性评价指标DB、DUNN等，关键问题是需要我们构建相似性指标。（3）分析不同类别音乐人或音乐之间的类内相似性差异，这些差异随时间是如何变化的，不同类别的音乐人或派系之间是否存在交互（文化背景等因素所导致）。（4）分析不同派系的音乐中，影响者到连接者之间的影响力（Q1指标）是否存在明显差异或共性，如7种音乐特性与5种人声特征之间的规律。（5）从数据中找出重大变革时间点，确定这些时间点的历史背景，确定主要的特征以更有效地表示这些飞跃，找出相关巨大贡献的艺术家。（6）分析不同类型音乐随时间变化的影响过程（内部音乐人互相影响、外部不同派系音乐之间的影响）。提出综合指标来表示这种影响随时间的变化模式。（7）该小问较难，一种可行的建议是从音乐人的作品数据（歌曲名、歌词等）中提取文本信息，获取主题词汇来表示时间、环境特征。（8）模型推广。进一步研究模型的运行模式，以适用于更丰富的音乐数据集等。 结合一篇论文分析（论文来源于网络，侵删）任务1将influencers和fllowers视为结点，构建网络，涉及5603个艺术家和42770个影响（数据集：influence——data.csv）子图筛选出10个顶级艺术家以及每一位艺术家与其追随者相连在影响者和跟随者之间建立了一个有向网络，基于网络理论，提出了三种不同的度量——度中心性、加权度中心性和特征中心性。然后，开发了这些指标的组合，作为音乐影响的综合。之后，创建了一个子网来说明影响力。任务2进行数据清洗，删除违背常识的变量一些音乐特征具有相似的含义，例如“能量”和“响度”都反映了音轨的强度和活动，为了减少相似度计算时的共线性的影响，使用主成分分析(PCA)来减少数据的离散度，同时尽可能保持数据的变化。在主成分分析结果的基础上，选择前7个主成分，忽略REST，新的7个变量保持了80%以上的原始数据信息。首先利用主成分分析对数据进行降维和共线性处理，然后定义和计算不同路径之间的距离，以获得艺术家之间的相似性。通过计算平均相似度，采用Mann-Whitney检验.结果表明，在概率为62.8%，p值小于0.001的情况下，某一体裁中的艺术家比体裁之间的艺术家更相似。所有艺术家之间的相似性如图所示。Mann-Whitney的统计数据显示，以62.8%的概率和小于0.001的p值，流派中的艺术家比流派之间的艺术家更相似。任务3为了比较音乐的相似性和对体裁层次的影响，首先将数据集中的体裁合并为DataSet“datunce_data.csv”和数据集“data_by_artist.csv”，然后根据不同的类型对数据进行分组。Figure 12 shows the Similarity Matrix and Influence Matrix between Genres. Blues andEasy Listening share some similar characteristics in musical features,whereas Electronic andVocal are more similar. As for influence, Pop/Rock significantly influences other genres. Interestingly, Pop/Rock has a strong influence on R&amp;B, but they do not share significant similarityin musical characteristics, which we will discuss the reason later.根据提出的音乐相似性度量， 发现在分析不同类型时，体裁内部和类型之间的相似性和影响差异很大。针对不同类型的不同类型，建立了体裁分类树模型。通过对不同时期影响因素的分析，探索了体裁的演化路径。基于类型尺度的定向网络，发现流行/摇滚与R&amp;B、布鲁斯和民间有着很强的关系。任务4建立了一个基于音乐特征相似性的相似贝叶斯网络来识别真实的跟随者。然后，应用多元二样本法，发现没有强有力的证据(p值&gt;0.1)，任何音乐特征都比其他特征具有更强的“传染性”。任务5首先分析了流派的兴衰，发现了20世纪50年代的音乐革命。提出了一种动态规划算法，用于检测MUSIC特征中与革命相一致的变化点。结果表明，声学、能量、舞步能力和响度可能意味着革命。基于贝叶斯网络，Elvis Presley和Cliff Richard代表了革命者。Our Similarity Bayesian Network is shown in figure.Elvis Presley, an influencer who hasthe most followers in 1950s in Pop/Rock, leads a countercultural movement in the United States.At the same time, Cliff Richard, an English artist, makes a dramatic change on the evolution ofthe genre. These two revolutionaries not only reinvent the existing genre, but also inspire artistslike Moti and Freestyle to create a new genre -Electronic.Therefore,Elvis Presley and CliffRichard represent revolutionaries in 1950s music revolution.任务6为了进一步了解流行音乐/摇滚的演变，提出了一种基于整个流派音乐特征滞后趋势的动态影响指示器。在1960年代到2010年代，有10位动态影响者，每个人对流行/摇滚都有自己独特的影响。此外，还解释了流行/摇滚的演变过程。任务7基于时间序列分析检测出三个重要时期，显示音乐对文化的影响。在所建立的模型基础上，确定了诸如文化运动的社会变化和互联网的扩散等技术变化。"},{"title":"Deep Dream:理解深度神经网络结构及应用","date":"2022-01-17T12:00:00.000Z","url":"/2022/01/17/machine-learning-9/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"Deep Dream 项目简介Deep Dream 技术原理最大化输出层输出的某一类别概率最大化卷积层某一通道激活值经典CNN———AlexNet主要贡献：— 防止过拟合：数据增强（data augmentation）, Dropout— GPU实现：将网络分布在两个GPU上，且GPU之间在某些层能够互相通信。— 非线性激活：ReLU— 大数据训练：120万ImageNet图像数据集模型的加载Tensorflow提供了以下两种方式来存储和加载模型：生成检查点文件（checkpoint file）, 扩展名一般为.ckpt，通过在tf.train.Saver对象上调用Saver.save()生成，通过saver.restore()来加载。生成图协议文件（graph proto file），这是一个二进制文件，扩展名一般为.bp，用tf.train.write_graph()保存，然后使用tf.import_graph_def()来加载图。 图的保存与加载图像预处理——增加维度• 使用的图像数据格式通常是（height,width,channel），只能表示一张图像；• 而Inception模型要求的输入格式却是（batch，height， width, channel），即同时将多张图像送入网络tf.expand_dims(input, dim, name=None)Returns:A Tensor. Has the same type as input. Contains the same data as input, but itsshape has an additional dimension of size 1 added.向tensor中插入维度1，插入位置就是参数代表的位置（维度从0开始）图的基本操作⚫ 建立图、获得默认图、重置默认图(tf.Graph()，tf.get_default_graph()，tf.reset_default_graph())⚫ 获取张量⚫ 获取节点操作 在图像保存的过程中，我遇到一个问题：引用全局变量提示：local variable referenced before assignment.症结归因：在网上查了python3使用toimage()函数的场景，发现这个函数已经弃用（deprecated），很多教程推荐降低第三方scipy的版本来配合toimage()函数的使用，无法从根本上解决问题，况且技术更新迭代是不可避免的，我们需要顺势而为。解决方案：使用matplotlib模块中image下的.imsave()函数，将numpy数组转成图片保存。具体代码如下： 导入模型 Dream图像生成（以噪声为起点） 单通道特征生成较低层单通道生成较高层单通道生成多通道生成所有通道生成生成原始Deep Dream图像通过最大化某一通道的平均值能够得到有意义的图像从浅层到高层越来越抽象从单通道到多通道到所有通道 如何提高生成图像的质量在图像算法中，有高频成分和低频成分的概念。简单的说，高频成分就是图像中灰度、颜色、明度变化比较大的地方，比如图像的边缘和细节部分。低频成分就是图像中变化不大的地方，比如大块色块、整体风格。因此，对于生成的Deep Dream图像，它的高频成分太多，颜色、明度变化都比较剧烈。 而理想是图像的低频成分更多，这样生成的图像才能够更加柔和。在图像分解时，是从金字塔底层开始的，比如从level0 （原图）开始，分解出 level1 高频成分，把当时的低频成分留作level2 ，然后再把level2 分解成高频成分和更低频成分的level3 ，逐步分解，来得到更加低频的信息。在图像生成时，是从金字塔顶层开始的。首先生成低频的图像，把低频成分（level4）放大成跟level3一样的尺寸后，加到level3，得到比较高频的成分，然后再把合成之后的level3’放大，加到level2，逐层相加，得到最后的最大尺寸图像，即合成图像。 完整程序"},{"title":"图像识别问题：卷积神经网络与应用","date":"2022-01-16T12:00:00.000Z","url":"/2022/01/16/machine-learning-8/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"卷积神经网络Convolutional Neural Network(CNN)图像分类ImageNet图像分类错误率（top-5）当卷积神经网络达到152层时，错误率可达到3.57.而同比人类的错误率为5.1卷积神经网络成了机器视觉任务的一个极为重要的工具！认识卷积神经网络-TensorFlow对卷积神经网络的支持-案例：CIFAR-10图像识别 从全连接神经网络到卷积神经网络全连接网络的局限性对于MNIST 手写数字识别，假如第一个隐层的节点数为500，那么一个全连接层的参数个数为：28×28×1×500+500 ≈ 40万；当图片更大时，比如CIFAR数据集中，图片大小为32×32×3，此时全连接层的参数有：32×32×3×500+500 ≈ 150万 当图片分辨率进一步提高时，当隐层数量增加时，例如：600 x 600 图像，各隐层节点数分别为300,200和100，则参数个数为：600 x 600 x 300 + 300 x 200 + 200 x 100 ≈ 1.08亿参数增多会导致：计算速度减慢过拟合 需要更合理的结构来有效减少参数个数 卷积神经网络的提出1962年Hubel和Wiesel通过对猫视觉皮层细胞的研究，提出了感受野(receptive field)的概念。视觉皮层的神经元就是局部接受信息的，只受某些特定区域刺激的响应，而不是对全局图像进行感知。1984年日本学者Fukushima基于感受野概念提出神经认知机(neocognitron)。CNN可看作是神经认知机的推广形式。卷积在介绍卷积神经网络的基本概念之前，我们先做一个矩阵运算：（1）求点积：将5×5输入矩阵中3×3深蓝色区域中每个元素分别与其对应位置的权值（红色数字）相乘，然后再相加，所得到的值作为3×3输出矩阵（绿色）的第一个元素。（2）滑动窗口：将3×3权值矩阵向右移动一个格（即，步长为1）（3）重复操作：同样地，将此时深色区域内每个元素分别与对应的权值相乘然后再相加，所得到的值作为输出矩阵的第二个元素；重复上述“求点积-滑动窗口”操作，直至输出矩阵所有值被填满。卷积核在 2 维输入数据上“滑动”，对当前输入部分的元素进行矩阵乘法，然后将结果汇为单个输出像素值，重复这个过程直到遍历整张图像，这个过程就叫做卷积，这个权值矩阵就是卷积核卷积操作后的图像称为特征图（feature map）卷积层：卷积运算的主要目的是使原信号特征增强，并降低噪音。定义卷积核 → 卷积操作 → 显示卷积运算的主要目的是使原信号特征增强，并降低噪音。对图像用一个卷积核进行卷积运算，实际上是一个滤波的过程。每个卷积核都是一种特征提取方式，就像是一个筛子，将图像中符合条件的部分筛选出来。 0填充(Padding)观察卷积示例，我们会发现一个现象：在卷积核滑动的过程中图像的边缘会被裁剪掉，将5×5特征矩阵转换为 3×3的特征矩阵。如何使得输出尺寸与输入保持一致呢？0填充：用额外的“假”像素（通常值为 0）填充边缘。这样，在滑动时的卷积核可以允许原始边缘像素位于卷积核的中心，同时延伸到边缘之外的假像素，从而产生与输入（5×5蓝色）相同大小的输出（5×5绿色）。 多通道卷积每个卷积核都会将图像生成为另一幅特征映射图，即：一个卷积核提取一种特征。为了使特征提取更充分，可以添加多个卷积核以提取不同的特征，也就是，多通道卷积。每个通道使用一个卷积核进行卷积操作，然后将这些特征图相同位置上的值相加，生成一张特征图。每个通道使用一个卷积核进行卷积操作，然后将这些特征图相同位置上的值相加，生成一张特征图。加偏置。偏置的作用是对每个feature map加一个偏置项以便产生最终的输出特征图。 池化(pooling)在卷积层之后常常紧接着一个降采样层，通过减小矩阵的长和宽，从而达到减少参数的目的。降采样是降低特定信号的采样率的过程。计算图像一个区域上的某个特定特征的平均值或最大值，这种聚合操作就叫做池化（pooling）。卷积层的作用是探测上一层特征的局部连接，而池化的作用是在语义上把相似的特征合并起来，从而达到降维目的。这些概要统计特征不仅具有低得多的维度 (相比使用所有提取得到的特征)，同时还会改善结果(不容易过拟合)。常用的池化方法：(1) 均值池化：对池化区域内的像素点取均值，这种方法得到的特征数据对背景信息更敏感。(2) 最大池化：对池化区域内所有像素点取最大值，这种方法得到的特征对纹理特征信息更加敏感。隐层与隐层之间空间分辨率递减，因此，为了检测更多的特征信息、形成更多不同通道特征的组合，从而形成更复杂的特征，需要逐渐增加每层所含的平面数（也就是特征图的数量）。 步长(stride)步长是卷积操作的重要概念，表示卷积核在图片上移动的格数。通过步长的变换，可以得到不同尺寸的卷积输出结果。步长大于1的卷积操作也是降维的一种方式卷积后图片尺寸： 假如步长为S，原始图片尺寸为[N1,N1]，卷积核大小为[N2,N2]，那么卷积之后图像大小：[(N1-N2)/S+1, (N1-N2)/S+1] 卷积操作的作用：使原信号特征增强（例如，得到显著的边缘特征），并且降低噪音。降采样操作的作用：减少数据处理量的同时保留有用的信息 过程概括：输入图像通过若干个“卷积→降采样”后，连接成一个向量输入到传统的分类器层中，最终得到输出。正则表达式：输入层→（卷积层+→池化层？）+→全连接层+ Tensorflow中CNN的相关函数卷积函数卷积函数定义在tensorflow/python/ops下的nn_impl.py和nn_ops.py文件中:tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None,name=None)tf.nn.depthwise_conv2d(input, filter, strides, padding, name=None)tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides,padding, name=None) • input：需要做卷积的输入数据。注意：这是一个4维的张量（[batch, in_height,in_width, in_channels]），要求类型为float32或float64其中之一。• filter：卷积核。[filter_height, filter_width, in_channels, out_channels]• strides：图像每一维的步长，是一个一维向量，长度为4• padding：定义元素边框与元素内容之间的空间。”SAME”或”VALID”，这个值决定了不同的卷积方式。当为”SAME”时，表示边缘填充，适用于全尺寸操作；当为”VALID”时，表示边缘不填充。• use_cudnn_on_gpu：bool类型，是否使用cudnn加速• name：该操作的名称• 返回值：返回一个tensor，即feature map 池化函数池化函数定义在tensorflow/python/ops下的nn.py和gen_nn_ops.py文件中:最大池化：tf.nn.max_pool(value, ksize, strides, padding, name=None)平均池化：tf.nn.avg_pool(value, ksize, strides, padding, name=None) • value：需要池化的输入。一般池化层接在卷积层后面，所以输入通常是conv2d所输出的feature map，依然是4维的张量（[batch, height, width,channels]）。• ksize：池化窗口的大小，由于一般不在batch和channel上做池化，所以ksize一般是[1,height, width,1]，• strides：图像每一维的步长，是一个一维向量，长度为4• padding：和卷积函数中padding含义一样• name：该操作的名称• 返回值：返回一个tensor 案例：CIFAR-10图像识别CIFAR-10数据集：• CIFAR-10是一个用于识别普适物体的小型数据集，它包含了10个类别的RGB彩色图片。• 图片尺寸： 32 x 32• 训练图片50000张，测试图片10000张 下载数据集导入CIFAR数据集显示数据集信息查看多项images与label数据预处理-图像数据预处理-标签数据预处理——独热编码定义共享函数定义网格结构，建立CIFAR-10图像分类模型构建模型定义准确率启动会话断点续训迭代训练及改进-增加网络层数-增加迭代次数-增加全连接层数-增加全连接层的神经元个数-数据扩增可视化损失值可视化准确率评估模型及预测-计算测试集上的准确率-利用模型进行预测-可视化预测结果"},{"title":"泰坦尼克号旅客生存预测：Keras应用实践","date":"2022-01-14T12:00:00.000Z","url":"/2022/01/14/machine-learning-7/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"泰坦尼克号上的旅客生存概率预测：TensorFlow的高级框架Keras关于使用Keras 案例分析与数据处理下载泰坦尼克号上旅客的数据集 筛选提取字段survival（是否生存）是标签字段，其他是候选特征字段筛选提取需要的特征字段，去掉ticket，cabin等 转换编码 定义数据预处理函数 Keras建模与应用完整程序 通过TensorBoard可视化查看训练过程"},{"title":"MINIST手写数字识别：多层神经网络与应用","date":"2022-01-13T12:00:00.000Z","url":"/2022/01/13/machine-learning-6/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"MNIST手写数字识别进阶 单隐藏层神经网络构建与应用单个神经元模型常见激活函数神经网络模型 全连接单隐藏层网络建模实现1.载入数据2.构建输入层·定义标签数据占位符3.构建隐藏层4.构建输出层5.定义损失函数6.训练模型·设置训练参数·选择优化器·定义准确值·记录训练开始时间·读取批次数据·执行批次训练·验证数据计算误差与准确率·显示运行时间7.训练结果8.评估模型9.应用模型·进行预测·找出预测错误·定义输出错误分类的函数·可视化查看预测错误的样本·定义可视化函数 完整程序如下： 多层网络建模实现 1.构建模型其余程序与步骤都相同完整程序如下：训练结果如下显示：（多数据警告） 训练结果 Train Epoch: 01 Loss= 0.420293599 Accuracy= 0.8820Train Epoch: 02 Loss= 0.314055026 Accuracy= 0.9136Train Epoch: 03 Loss= 0.270856827 Accuracy= 0.9234Train Epoch: 04 Loss= 0.241648629 Accuracy= 0.9310Train Epoch: 05 Loss= 0.221271217 Accuracy= 0.9378Train Epoch: 06 Loss= 0.202354938 Accuracy= 0.9440Train Epoch: 07 Loss= 0.190346196 Accuracy= 0.9464Train Epoch: 08 Loss= 0.176857352 Accuracy= 0.9516Train Epoch: 09 Loss= 0.166498899 Accuracy= 0.9536Train Epoch: 10 Loss= 0.157787085 Accuracy= 0.9546Train Epoch: 11 Loss= 0.153465509 Accuracy= 0.9576Train Epoch: 12 Loss= 0.143812209 Accuracy= 0.9608Train Epoch: 13 Loss= 0.140448615 Accuracy= 0.9594Train Epoch: 14 Loss= 0.134099528 Accuracy= 0.9630Train Epoch: 15 Loss= 0.132911429 Accuracy= 0.9624Train Epoch: 16 Loss= 0.124732591 Accuracy= 0.9660Train Epoch: 17 Loss= 0.119741030 Accuracy= 0.9672Train Epoch: 18 Loss= 0.120224647 Accuracy= 0.9648Train Epoch: 19 Loss= 0.114067823 Accuracy= 0.9676Train Epoch: 20 Loss= 0.111958325 Accuracy= 0.9684Train Epoch: 21 Loss= 0.111831568 Accuracy= 0.9672Train Epoch: 22 Loss= 0.106434591 Accuracy= 0.9700Train Epoch: 23 Loss= 0.105766617 Accuracy= 0.9688Train Epoch: 24 Loss= 0.102649413 Accuracy= 0.9710Train Epoch: 25 Loss= 0.101922378 Accuracy= 0.9712Train Epoch: 26 Loss= 0.100112967 Accuracy= 0.9708Train Epoch: 27 Loss= 0.096768998 Accuracy= 0.9724Train Epoch: 28 Loss= 0.097221516 Accuracy= 0.9732Train Epoch: 29 Loss= 0.095773250 Accuracy= 0.9726Train Epoch: 30 Loss= 0.094824158 Accuracy= 0.9734Train Epoch: 31 Loss= 0.092812546 Accuracy= 0.9734Train Epoch: 32 Loss= 0.092194885 Accuracy= 0.9736Train Epoch: 33 Loss= 0.090828940 Accuracy= 0.9746Train Epoch: 34 Loss= 0.088991299 Accuracy= 0.9748Train Epoch: 35 Loss= 0.089945436 Accuracy= 0.9750Train Epoch: 36 Loss= 0.086703502 Accuracy= 0.9762Train Epoch: 37 Loss= 0.087226808 Accuracy= 0.9758Train Epoch: 38 Loss= 0.086025327 Accuracy= 0.9758Train Epoch: 39 Loss= 0.085212491 Accuracy= 0.9758Train Epoch: 40 Loss= 0.085250698 Accuracy= 0.9762Train Finshied takes: 94.00Test Accurary: 0.9727[ True True True … True True True][8, 104, 247, 259, 320, 340, 381, 445, 449, 495, 582, 610, 613, 659, 684, 691, 707, 720, 740, 810, 813, 844, 890, 938, 951, 956, 959, 965, 982, 1014, 1032, 1039, 1107, 1112, 1114, 1156, 1181, 1182, 1192, 1194, 1224, 1226, 1232, 1242, 1247, 1260, 1319, 1326, 1328, 1378, 1393, 1444, 1500, 1522, 1527, 1530, 1549, 1553, 1570, 1609, 1621, 1671, 1681, 1717, 1754, 1790, 1800, 1850, 1878, 1901, 1911, 1913, 1940, 1941, 1984, 2016, 2035, 2044, 2053, 2070, 2109, 2118, 2129, 2130, 2135, 2145, 2182, 2186, 2195, 2224, 2266, 2272, 2280, 2293, 2333, 2369, 2387, 2406, 2414, 2433, 2455, 2488, 2514, 2573, 2607, 2618, 2648, 2654, 2721, 2863, 2877, 2896, 2915, 2939, 2953, 2979, 2995, 3005, 3030, 3060, 3073, 3117, 3405, 3422, 3503, 3520, 3533, 3549, 3558, 3567, 3575, 3597, 3718, 3751, 3767, 3776, 3780, 3796, 3808, 3811, 3838, 3846, 3853, 3869, 3893, 3902, 3906, 3929, 3941, 3946, 3968, 3985, 3995, 4007, 4065, 4075, 4078, 4163, 4177, 4201, 4211, 4224, 4248, 4289, 4306, 4374, 4437, 4451, 4477, 4497, 4534, 4536, 4578, 4601, 4635, 4668, 4761, 4807, 4814, 4823, 4860, 4874, 4876, 4880, 4956, 4966, 5078, 5265, 5331, 5457, 5600, 5642, 5676, 5734, 5749, 5887, 5936, 5937, 5955, 5973, 5982, 5997, 6023, 6045, 6046, 6059, 6071, 6101, 6166, 6173, 6390, 6400, 6426, 6505, 6532, 6557, 6560, 6571, 6590, 6597, 6608, 6625, 6651, 6783, 6847, 7434, 7451, 7595, 7800, 7821, 7849, 7858, 7886, 7899, 7915, 7991, 8062, 8094, 8272, 8277, 8311, 8325, 8339, 8408, 8520, 8522, 9009, 9015, 9019, 9024, 9071, 9280, 9482, 9587, 9634, 9636, 9662, 9692, 9698, 9729, 9745, 9749, 9768, 9770, 9779, 9811, 9828, 9839, 9879, 9888, 9891, 9944, 9982] 273index=8标签值= 5 预测值= 6index=104标签值= 9 预测值= 5index=247标签值= 4 预测值= 6index=259标签值= 6 预测值= 0index=320标签值= 9 预测值= 8index=340标签值= 5 预测值= 3index=381标签值= 3 预测值= 7index=445标签值= 6 预测值= 0index=449标签值= 3 预测值= 5index=495标签值= 8 预测值= 2index=582标签值= 8 预测值= 2index=610标签值= 4 预测值= 2index=613标签值= 2 预测值= 8index=659标签值= 2 预测值= 8index=684标签值= 7 预测值= 3index=691标签值= 8 预测值= 4index=707标签值= 4 预测值= 9index=720标签值= 5 预测值= 8index=740标签值= 4 预测值= 9index=810标签值= 7 预测值= 2index=813标签值= 9 预测值= 8index=844标签值= 8 预测值= 7index=890标签值= 3 预测值= 5index=938标签值= 3 预测值= 5index=951标签值= 5 预测值= 4index=956标签值= 1 预测值= 2index=959标签值= 4 预测值= 9index=965标签值= 6 预测值= 0index=982标签值= 3 预测值= 2index=1014标签值= 6 预测值= 5index=1032标签值= 5 预测值= 8index=1039标签值= 7 预测值= 8index=1107标签值= 9 预测值= 5index=1112标签值= 4 预测值= 6index=1114标签值= 3 预测值= 8index=1156标签值= 7 预测值= 8index=1181标签值= 6 预测值= 1index=1182标签值= 6 预测值= 8index=1192标签值= 9 预测值= 4index=1194标签值= 7 预测值= 9index=1224标签值= 2 预测值= 6index=1226标签值= 7 预测值= 2index=1232标签值= 9 预测值= 4index=1242标签值= 4 预测值= 9index=1247标签值= 9 预测值= 5index=1260标签值= 7 预测值= 1index=1319标签值= 8 预测值= 3index=1326标签值= 7 预测值= 2index=1328标签值= 7 预测值= 8index=1378标签值= 5 预测值= 6index=1393标签值= 5 预测值= 3index=1444标签值= 6 预测值= 4index=1500标签值= 7 预测值= 1index=1522标签值= 7 预测值= 9index=1527标签值= 1 预测值= 6index=1530标签值= 8 预测值= 7index=1549标签值= 4 预测值= 2index=1553标签值= 9 预测值= 3index=1570标签值= 0 预测值= 6index=1609标签值= 2 预测值= 6index=1621标签值= 0 预测值= 6index=1671标签值= 7 预测值= 3index=1681标签值= 3 预测值= 7index=1717标签值= 8 预测值= 0index=1754标签值= 7 预测值= 2index=1790标签值= 2 预测值= 8index=1800标签值= 6 预测值= 4index=1850标签值= 8 预测值= 3index=1878标签值= 8 预测值= 3index=1901标签值= 9 预测值= 4index=1911标签值= 5 预测值= 6index=1913标签值= 3 预测值= 8index=1940标签值= 5 预测值= 0index=1941标签值= 7 预测值= 8index=1984标签值= 2 预测值= 0index=2016标签值= 7 预测值= 2index=2035标签值= 5 预测值= 3index=2044标签值= 2 预测值= 7index=2053标签值= 4 预测值= 9index=2070标签值= 7 预测值= 9index=2109标签值= 3 预测值= 9index=2118标签值= 6 预测值= 1index=2129标签值= 9 预测值= 2index=2130标签值= 4 预测值= 9index=2135标签值= 6 预测值= 1index=2145标签值= 4 预测值= 2index=2182标签值= 1 预测值= 2index=2186标签值= 2 预测值= 3index=2195标签值= 7 预测值= 2index=2224标签值= 5 预测值= 8index=2266标签值= 1 预测值= 8index=2272标签值= 8 预测值= 0index=2280标签值= 3 预测值= 5index=2293标签值= 9 预测值= 6index=2333标签值= 0 预测值= 2index=2369标签值= 5 预测值= 8index=2387标签值= 9 预测值= 1index=2406标签值= 9 预测值= 1index=2414标签值= 9 预测值= 4index=2433标签值= 2 预测值= 1index=2455标签值= 0 预测值= 6index=2488标签值= 2 预测值= 4index=2514标签值= 4 预测值= 9index=2573标签值= 5 预测值= 8index=2607标签值= 7 预测值= 1index=2618标签值= 3 预测值= 5index=2648标签值= 9 预测值= 0index=2654标签值= 6 预测值= 1index=2721标签值= 6 预测值= 5index=2863标签值= 9 预测值= 4index=2877标签值= 4 预测值= 7index=2896标签值= 8 预测值= 0index=2915标签值= 7 预测值= 3index=2939标签值= 9 预测值= 5index=2953标签值= 3 预测值= 5index=2979标签值= 9 预测值= 7index=2995标签值= 6 预测值= 5index=3005标签值= 9 预测值= 1index=3030标签值= 6 预测值= 2index=3060标签值= 9 预测值= 7index=3073标签值= 1 预测值= 3index=3117标签值= 5 预测值= 9index=3405标签值= 4 预测值= 9index=3422标签值= 6 预测值= 0index=3503标签值= 9 预测值= 1index=3520标签值= 6 预测值= 4index=3533标签值= 4 预测值= 9index=3549标签值= 3 预测值= 2index=3558标签值= 5 预测值= 0index=3567标签值= 8 预测值= 5index=3575标签值= 7 预测值= 8index=3597标签值= 9 预测值= 3index=3718标签值= 4 预测值= 9index=3751标签值= 7 预测值= 2index=3767标签值= 7 预测值= 3index=3776标签值= 5 预测值= 8index=3780标签值= 4 预测值= 6index=3796标签值= 2 预测值= 8index=3808标签值= 7 预测值= 8index=3811标签值= 2 预测值= 0index=3838标签值= 7 预测值= 1index=3846标签值= 6 预测值= 4index=3853标签值= 6 预测值= 2index=3869标签值= 9 预测值= 4index=3893标签值= 5 预测值= 6index=3902标签值= 5 预测值= 3index=3906标签值= 1 预测值= 3index=3929标签值= 5 预测值= 8index=3941标签值= 4 预测值= 2index=3946标签值= 2 预测值= 8index=3968标签值= 5 预测值= 3index=3985标签值= 9 预测值= 4index=3995标签值= 3 预测值= 5index=4007标签值= 7 预测值= 4index=4065标签值= 0 预测值= 7index=4075标签值= 8 预测值= 0index=4078标签值= 9 预测值= 3index=4163标签值= 9 预测值= 0index=4177标签值= 5 预测值= 4index=4201标签值= 1 预测值= 7index=4211标签值= 6 预测值= 5index=4224标签值= 9 预测值= 7index=4248标签值= 2 预测值= 8index=4289标签值= 2 预测值= 8index=4306标签值= 3 预测值= 7index=4374标签值= 5 预测值= 6index=4437标签值= 3 预测值= 2index=4451标签值= 2 预测值= 8index=4477标签值= 0 预测值= 6index=4497标签值= 8 预测值= 2index=4534标签值= 9 预测值= 8index=4536标签值= 6 预测值= 5index=4578标签值= 7 预测值= 9index=4601标签值= 8 预测值= 4index=4635标签值= 3 预测值= 5index=4668标签值= 2 预测值= 3index=4761标签值= 9 预测值= 8index=4807标签值= 8 预测值= 3index=4814标签值= 6 预测值= 0index=4823标签值= 9 预测值= 4index=4860标签值= 4 预测值= 9index=4874标签值= 9 预测值= 6index=4876标签值= 2 预测值= 4index=4880标签值= 0 预测值= 8index=4956标签值= 8 预测值= 4index=4966标签值= 7 预测值= 3index=5078标签值= 3 预测值= 8index=5265标签值= 6 预测值= 4index=5331标签值= 1 预测值= 6index=5457标签值= 1 预测值= 8index=5600标签值= 7 预测值= 9index=5642标签值= 1 预测值= 8index=5676标签值= 4 预测值= 3index=5734标签值= 3 预测值= 2index=5749标签值= 8 预测值= 6index=5887标签值= 7 预测值= 0index=5936标签值= 4 预测值= 9index=5937标签值= 5 预测值= 3index=5955标签值= 3 预测值= 8index=5973标签值= 3 预测值= 8index=5982标签值= 5 预测值= 3index=5997标签值= 5 预测值= 8index=6023标签值= 3 预测值= 8index=6045标签值= 3 预测值= 9index=6046标签值= 3 预测值= 8index=6059标签值= 3 预测值= 9index=6071标签值= 9 预测值= 3index=6101标签值= 1 预测值= 8index=6166标签值= 9 预测值= 3index=6173标签值= 9 预测值= 0index=6390标签值= 5 预测值= 8index=6400标签值= 0 预测值= 6index=6426标签值= 0 预测值= 6index=6505标签值= 9 预测值= 0index=6532标签值= 0 预测值= 5index=6557标签值= 0 预测值= 2index=6560标签值= 9 预测值= 3index=6571标签值= 9 预测值= 7index=6590标签值= 0 预测值= 5index=6597标签值= 0 预测值= 7index=6608标签值= 9 预测值= 5index=6625标签值= 8 预测值= 2index=6651标签值= 0 预测值= 5index=6783标签值= 1 预测值= 6index=6847标签值= 6 预测值= 4index=7434标签值= 4 预测值= 8index=7451标签值= 5 预测值= 6index=7595标签值= 3 预测值= 8index=7800标签值= 3 预测值= 2index=7821标签值= 3 预测值= 2index=7849标签值= 3 预测值= 2index=7858标签值= 3 预测值= 2index=7886标签值= 2 预测值= 4index=7899标签值= 1 预测值= 8index=7915标签值= 7 预测值= 8index=7991标签值= 9 预测值= 8index=8062标签值= 5 预测值= 8index=8094标签值= 2 预测值= 8index=8272标签值= 3 预测值= 5index=8277标签值= 3 预测值= 8index=8311标签值= 6 预测值= 4index=8325标签值= 0 预测值= 6index=8339标签值= 8 预测值= 6index=8408标签值= 8 预测值= 6index=8520标签值= 4 预测值= 8index=8522标签值= 8 预测值= 6index=9009标签值= 7 预测值= 2index=9015标签值= 7 预测值= 2index=9019标签值= 7 预测值= 2index=9024标签值= 7 预测值= 2index=9071标签值= 1 预测值= 8index=9280标签值= 8 预测值= 5index=9482标签值= 5 预测值= 3index=9587标签值= 9 预测值= 4index=9634标签值= 0 预测值= 8index=9636标签值= 3 预测值= 5index=9662标签值= 3 预测值= 2index=9692标签值= 9 预测值= 7index=9698标签值= 6 预测值= 5index=9729标签值= 5 预测值= 6index=9745标签值= 4 预测值= 2index=9749标签值= 5 预测值= 6index=9768标签值= 2 预测值= 0index=9770标签值= 5 预测值= 0index=9779标签值= 2 预测值= 0index=9811标签值= 2 预测值= 8index=9828标签值= 3 预测值= 5index=9839标签值= 2 预测值= 7index=9879标签值= 0 预测值= 2index=9888标签值= 6 预测值= 0index=9891标签值= 9 预测值= 7index=9944标签值= 3 预测值= 8index=9982标签值= 5 预测值= 6总计：273 此外我又尝试了两层隐藏网络的模型效果，事实上网络层数并不是越多越好，在这里不再赘述 重构建模过程构建模型·定义隐藏层神经元数目·输入层：隐藏层参数和偏置项·计算隐藏层结果·计算输出结果 新定义全连接层函数·构建输入层·构建隐藏层(1…n）·构建输出层 训练模型的保存初始化参数和文件目录 训练模型的还原与应用1.定义相同结构的模型2.设置模型文件的存放目录3.读取还原模型 TensorBoard进阶与Tensorflow游乐场TensorBoard进阶 启动tensorboard Tensorflow游乐场TensorFlow游乐场是一个通过网页浏览器就可以训练的简单神经网络并实现了可视化训练过程网址： 数据每个点代表一个样本点的颜色代表样本标签提供了4种不同的数据集，可以设置训练数据比例、噪音、批处理大小 特征提取为了将一个实际问题对应到平面上不同颜色点的划分，需要将实际问题种的实体变成平面上的一个点；点的颜色只有两种，这是一个二分类的问题以判断零件是否合格为例，长度和质量就是特征在机器学习中，所有用于描述实体的数字的组合就是一个实体的特征向量（feature vector） 神经网络神经网络是分层的机构特征向量是神经网络的输入层同一层的节点不会互相连接每一层只和下一层连接最后一层作为输出层得到结果输入和输出层之间的是隐藏层学习率（learning rate）、激活函数（activation）、正则化（regularization） 神经网络训练解读一个小格子代表神经网络中的一个节点边代表节点之间的连接节点和边都有或深或浅的颜色边代表了神经网络的一个参数，可以是任意实数神经网络就是通过对参数的合理设置来解决分类或者回归问题的边的颜色体现了这个参数的取值，颜色越深，绝对值越大节点的颜色代表了区分平面。这个平面上的每个点就代表了（x1,x2）的一种取值，当节点的输出值的绝对值越大时，颜色越深（蓝色）例：x1节点的区分平面就是y轴输出节点除了显示区分平面外，还显示了训练数据 "},{"title":"MINIST手写数字识别：分类应用入门","date":"2022-01-12T12:00:00.000Z","url":"/2022/01/12/machine-learning-5/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"用神经元处理分类问题 MNIST手写数字识别问题分类问题MNIST手写数字识别数据集MNIST 数据集来自美国国家标准与技术研究所, National Institute of Standards and Technology (NIST).数据集由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员训练集 55000 验证集 5000 测试集 10000 数据集读取方法 了解数据集 可视化imageplt.imshow()第二个参数是这个图像的模式参数，“binary”表示以灰度模式显示。plt.imshow()函数中的图像数据参数支持一下数据形状：•（M，N） ：二维数值，代表图像大小为M行N列，值为每个像素点的取值。•（M，N，3） ：三维度数值，代表图像大小为M行N列（即图片的高和宽），每个像素点的取值具有RGB三个通道的值（float或uint8）。• 参数cmap缺省值为none，将把图像数据映射为彩色图显示 独热编码一种稀疏向量，其中：一个元素设为 1，所有其他元素均设为 0独热编码常用于表示拥有有限个可能值的字符串或标识符例如：假设某个植物学数据集记录了 15000 个不同的物种，其中每个物种都用独一无二的字符串标识符来表示。在特征工程过程中，可能需要将这些字符串标识符编码为独热向量，向量的大小为 15000使用独热编码的原因1 将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点2 机器学习算法中，特征之间距离的计算或相似度的常用计算方法都是基于欧式空间的3 将离散型特征使用one-hot编码，会让特征之间的距离计算更加合理 数据集划分构建和训练机器学习模型是希望对新的数据做出良好预测如何去保证训练的实效，可以应对以前未见过的数据呢？一种方法是将数据集分成两个子集：训练集 - 用于训练模型的子集测试集 - 用于测试模型的子集通常，在测试集上表现是否良好是衡量能否在新数据上表现良好的有用指标，前提是：测试集足够大不会反复使用相同的测试集来作假 拆分数据将单个数据集拆分为一个训练集和一个测试集确保测试集满足以下两个条件：规模足够大，可产生具有统计意义的结果能代表整个数据集，测试集的特征应该与训练集的特征相同 工作流程 问题：多次重复执行该流程可能导致模型不知不觉地拟合了特定测试集的特性 新的工作流程前向计算与结果分类 逻辑回归许多问题的预测结果是一个在连续空间的数值，比如房价预测问题，可以用线性模型来描述但也有很多场景需要输出的是概率估算值，例如：• 根据邮件内容判断是垃圾邮件的可能性• 根据医学影像判断肿瘤是恶性的可能性• 手写数字分别是 0、1、2、3、4、5、6、7、8、9的可能性（概率）这时需要将预测输出值控制在 [0，1]区间内二元分类问题的目标是正确预测两个可能的标签中的一个逻辑回归（Logistic Regression）可以用于处理这类问题Sigmod函数逻辑回归中的损失函数 多元分类Softmax 思想逻辑回归可生成介于 0 和 1.0 之间的小数。例如，某电子邮件分类器的逻辑回归输出值为 0.8，表明电子邮件是垃圾邮件的概率为80%，不是垃圾邮件的概率为 20%。很明显，一封电子邮件是垃圾邮件或非垃圾邮件的概率之和为 1.0。Softmax将这一想法延伸到多类别领域。在多类别问题中，Softmax会为每个类别分配一个用小数表示的概率。这些用小数表示的概率相加之和必须是 1.0。交叉熵损失函数交叉熵是一个信息论中的概念，它原来是用来估算平均编码长度的。给定两个概率分布p和q，通过q来表示p的交叉熵为交叉熵刻画的是两个概率分布之间的距离，p代表正确答案，q代表的是预测值，交叉熵越小，两个概率的分布约接近完整程序 训练结果如下显示：（多数据警告） 训练结果 Train Epoch: 01 Loss= 6.405103683 Accuracy= 0.2280Train Epoch: 02 Loss= 3.764361620 Accuracy= 0.4148Train Epoch: 03 Loss= 2.728085279 Accuracy= 0.5380Train Epoch: 04 Loss= 2.206749916 Accuracy= 0.6040Train Epoch: 05 Loss= 1.891477942 Accuracy= 0.6544Train Epoch: 06 Loss= 1.679210782 Accuracy= 0.6842Train Epoch: 07 Loss= 1.525882602 Accuracy= 0.7104Train Epoch: 08 Loss= 1.408041358 Accuracy= 0.7276Train Epoch: 09 Loss= 1.315110087 Accuracy= 0.7408Train Epoch: 10 Loss= 1.239514709 Accuracy= 0.7536Train Epoch: 11 Loss= 1.177493334 Accuracy= 0.7648Train Epoch: 12 Loss= 1.123586535 Accuracy= 0.7726Train Epoch: 13 Loss= 1.077311754 Accuracy= 0.7818Train Epoch: 14 Loss= 1.036843181 Accuracy= 0.7892Train Epoch: 15 Loss= 1.002879262 Accuracy= 0.7950Train Epoch: 16 Loss= 0.971034110 Accuracy= 0.8008Train Epoch: 17 Loss= 0.943259358 Accuracy= 0.8056Train Epoch: 18 Loss= 0.917949319 Accuracy= 0.8094Train Epoch: 19 Loss= 0.894973814 Accuracy= 0.8132Train Epoch: 20 Loss= 0.874269903 Accuracy= 0.8188Train Epoch: 21 Loss= 0.854999602 Accuracy= 0.8222Train Epoch: 22 Loss= 0.837613404 Accuracy= 0.8258Train Epoch: 23 Loss= 0.821559370 Accuracy= 0.8272Train Epoch: 24 Loss= 0.807139874 Accuracy= 0.8294Train Epoch: 25 Loss= 0.792462468 Accuracy= 0.8322Train Epoch: 26 Loss= 0.779907167 Accuracy= 0.8336Train Epoch: 27 Loss= 0.767695904 Accuracy= 0.8372Train Epoch: 28 Loss= 0.756269753 Accuracy= 0.8376Train Epoch: 29 Loss= 0.745229483 Accuracy= 0.8398Train Epoch: 30 Loss= 0.735225081 Accuracy= 0.8424Train Epoch: 31 Loss= 0.725513577 Accuracy= 0.8442Train Epoch: 32 Loss= 0.716079116 Accuracy= 0.8466Train Epoch: 33 Loss= 0.708266139 Accuracy= 0.8472Train Epoch: 34 Loss= 0.700395644 Accuracy= 0.8482Train Epoch: 35 Loss= 0.692030370 Accuracy= 0.8500Train Epoch: 36 Loss= 0.684325218 Accuracy= 0.8524Train Epoch: 37 Loss= 0.677517653 Accuracy= 0.8520Train Epoch: 38 Loss= 0.671082914 Accuracy= 0.8536Train Epoch: 39 Loss= 0.664354265 Accuracy= 0.8546Train Epoch: 40 Loss= 0.658523440 Accuracy= 0.8554Train Epoch: 41 Loss= 0.652254462 Accuracy= 0.8562Train Epoch: 42 Loss= 0.646569550 Accuracy= 0.8564Train Epoch: 43 Loss= 0.640967071 Accuracy= 0.8562Train Epoch: 44 Loss= 0.635610282 Accuracy= 0.8568Train Epoch: 45 Loss= 0.630128682 Accuracy= 0.8588Train Epoch: 46 Loss= 0.625474632 Accuracy= 0.8600Train Epoch: 47 Loss= 0.620498955 Accuracy= 0.8608Train Epoch: 48 Loss= 0.616365254 Accuracy= 0.8608Train Epoch: 49 Loss= 0.612295687 Accuracy= 0.8622Train Epoch: 50 Loss= 0.607778609 Accuracy= 0.8626Train Epoch: 51 Loss= 0.603170395 Accuracy= 0.8644Train Epoch: 52 Loss= 0.599933922 Accuracy= 0.8648Train Epoch: 53 Loss= 0.595565438 Accuracy= 0.8670Train Epoch: 54 Loss= 0.591758490 Accuracy= 0.8680Train Epoch: 55 Loss= 0.588303804 Accuracy= 0.8694Train Epoch: 56 Loss= 0.585322618 Accuracy= 0.8696Train Epoch: 57 Loss= 0.581881523 Accuracy= 0.8698Train Epoch: 58 Loss= 0.577679873 Accuracy= 0.8706Train Epoch: 59 Loss= 0.574802935 Accuracy= 0.8716Train Epoch: 60 Loss= 0.572479665 Accuracy= 0.8724Train Epoch: 61 Loss= 0.568615794 Accuracy= 0.8730Train Epoch: 62 Loss= 0.566718578 Accuracy= 0.8728Train Epoch: 63 Loss= 0.563213229 Accuracy= 0.8730Train Epoch: 64 Loss= 0.559740126 Accuracy= 0.8732Train Epoch: 65 Loss= 0.556897581 Accuracy= 0.8732Train Epoch: 66 Loss= 0.554247439 Accuracy= 0.8750Train Epoch: 67 Loss= 0.551319838 Accuracy= 0.8754Train Epoch: 68 Loss= 0.548921108 Accuracy= 0.8742Train Epoch: 69 Loss= 0.546500266 Accuracy= 0.8752Train Epoch: 70 Loss= 0.543983519 Accuracy= 0.8758Train Epoch: 71 Loss= 0.542061985 Accuracy= 0.8758Train Epoch: 72 Loss= 0.539074361 Accuracy= 0.8768Train Epoch: 73 Loss= 0.536839545 Accuracy= 0.8766Train Epoch: 74 Loss= 0.534821987 Accuracy= 0.8764Train Epoch: 75 Loss= 0.533168137 Accuracy= 0.8774Train Epoch: 76 Loss= 0.530308068 Accuracy= 0.8784Train Epoch: 77 Loss= 0.528434753 Accuracy= 0.8778Train Epoch: 78 Loss= 0.526309192 Accuracy= 0.8788Train Epoch: 79 Loss= 0.524245977 Accuracy= 0.8782Train Epoch: 80 Loss= 0.522068501 Accuracy= 0.8792Train Epoch: 81 Loss= 0.520250797 Accuracy= 0.8794Train Epoch: 82 Loss= 0.517979860 Accuracy= 0.8794Train Epoch: 83 Loss= 0.516227961 Accuracy= 0.8806Train Epoch: 84 Loss= 0.514664233 Accuracy= 0.8802Train Epoch: 85 Loss= 0.512204289 Accuracy= 0.8802Train Epoch: 86 Loss= 0.510789692 Accuracy= 0.8814Train Epoch: 87 Loss= 0.509589970 Accuracy= 0.8810Train Epoch: 88 Loss= 0.507072687 Accuracy= 0.8816Train Epoch: 89 Loss= 0.505876541 Accuracy= 0.8824Train Epoch: 90 Loss= 0.504153013 Accuracy= 0.8822Train Epoch: 91 Loss= 0.502235591 Accuracy= 0.8832Train Epoch: 92 Loss= 0.501242876 Accuracy= 0.8828Train Epoch: 93 Loss= 0.498941988 Accuracy= 0.8836Train Epoch: 94 Loss= 0.497406751 Accuracy= 0.8832Train Epoch: 95 Loss= 0.496222556 Accuracy= 0.8838Train Epoch: 96 Loss= 0.494214565 Accuracy= 0.8840Train Epoch: 97 Loss= 0.493356436 Accuracy= 0.8832Train Epoch: 98 Loss= 0.491570175 Accuracy= 0.8846Train Epoch: 99 Loss= 0.489944965 Accuracy= 0.8840Train Epoch: 100 Loss= 0.488946378 Accuracy= 0.8842Train Epoch: 101 Loss= 0.487533092 Accuracy= 0.8844Train Epoch: 102 Loss= 0.485964954 Accuracy= 0.8852Train Epoch: 103 Loss= 0.484404892 Accuracy= 0.8852Train Epoch: 104 Loss= 0.483356357 Accuracy= 0.8846Train Epoch: 105 Loss= 0.482004821 Accuracy= 0.8860Train Epoch: 106 Loss= 0.480790615 Accuracy= 0.8854Train Epoch: 107 Loss= 0.479260147 Accuracy= 0.8858Train Epoch: 108 Loss= 0.478474498 Accuracy= 0.8860Train Epoch: 109 Loss= 0.476907611 Accuracy= 0.8862Train Epoch: 110 Loss= 0.475418597 Accuracy= 0.8868Train Epoch: 111 Loss= 0.474131107 Accuracy= 0.8870Train Epoch: 112 Loss= 0.473437607 Accuracy= 0.8874Train Epoch: 113 Loss= 0.471899986 Accuracy= 0.8874Train Epoch: 114 Loss= 0.470724523 Accuracy= 0.8882Train Epoch: 115 Loss= 0.469866693 Accuracy= 0.8880Train Epoch: 116 Loss= 0.468689829 Accuracy= 0.8872Train Epoch: 117 Loss= 0.468155473 Accuracy= 0.8878Train Epoch: 118 Loss= 0.466815442 Accuracy= 0.8886Train Epoch: 119 Loss= 0.465229392 Accuracy= 0.8882Train Epoch: 120 Loss= 0.464131504 Accuracy= 0.8890Train Epoch: 121 Loss= 0.462855667 Accuracy= 0.8886Train Epoch: 122 Loss= 0.462298840 Accuracy= 0.8880Train Epoch: 123 Loss= 0.461265326 Accuracy= 0.8894Train Epoch: 124 Loss= 0.460113525 Accuracy= 0.8894Train Epoch: 125 Loss= 0.459472865 Accuracy= 0.8892Train Epoch: 126 Loss= 0.458169192 Accuracy= 0.8894Train Epoch: 127 Loss= 0.456787854 Accuracy= 0.8898Train Epoch: 128 Loss= 0.456247598 Accuracy= 0.8892Train Epoch: 129 Loss= 0.455326319 Accuracy= 0.8898Train Epoch: 130 Loss= 0.453999221 Accuracy= 0.8904Train Epoch: 131 Loss= 0.453291893 Accuracy= 0.8900Train Epoch: 132 Loss= 0.452165246 Accuracy= 0.8900Train Epoch: 133 Loss= 0.451443315 Accuracy= 0.8906Train Epoch: 134 Loss= 0.450718075 Accuracy= 0.8898Train Epoch: 135 Loss= 0.449742883 Accuracy= 0.8906Train Epoch: 136 Loss= 0.448562503 Accuracy= 0.8908Train Epoch: 137 Loss= 0.447787493 Accuracy= 0.8912Train Epoch: 138 Loss= 0.447070807 Accuracy= 0.8920Train Epoch: 139 Loss= 0.446267515 Accuracy= 0.8922Train Epoch: 140 Loss= 0.445026428 Accuracy= 0.8930Train Epoch: 141 Loss= 0.444738477 Accuracy= 0.8918Train Epoch: 142 Loss= 0.443849623 Accuracy= 0.8916Train Epoch: 143 Loss= 0.442687124 Accuracy= 0.8924Train Epoch: 144 Loss= 0.442112952 Accuracy= 0.8924Train Epoch: 145 Loss= 0.441391319 Accuracy= 0.8924Train Epoch: 146 Loss= 0.440162748 Accuracy= 0.8926Train Epoch: 147 Loss= 0.439293742 Accuracy= 0.8934Train Epoch: 148 Loss= 0.438909620 Accuracy= 0.8930Train Epoch: 149 Loss= 0.437819093 Accuracy= 0.8934Train Epoch: 150 Loss= 0.437358975 Accuracy= 0.8934Train Epoch: 151 Loss= 0.436241060 Accuracy= 0.8936Train Epoch: 152 Loss= 0.436019480 Accuracy= 0.8940Train Epoch: 153 Loss= 0.435418934 Accuracy= 0.8940Train Epoch: 154 Loss= 0.434051841 Accuracy= 0.8948Train Epoch: 155 Loss= 0.433334082 Accuracy= 0.8946Train Epoch: 156 Loss= 0.432984859 Accuracy= 0.8944Train Epoch: 157 Loss= 0.432401121 Accuracy= 0.8946Train Epoch: 158 Loss= 0.431591660 Accuracy= 0.8944Train Epoch: 159 Loss= 0.430783927 Accuracy= 0.8950Train Epoch: 160 Loss= 0.430064738 Accuracy= 0.8954Train Epoch: 161 Loss= 0.428908020 Accuracy= 0.8958Train Epoch: 162 Loss= 0.428390443 Accuracy= 0.8956Train Epoch: 163 Loss= 0.428132027 Accuracy= 0.8962Train Epoch: 164 Loss= 0.427713633 Accuracy= 0.8958Train Epoch: 165 Loss= 0.426613778 Accuracy= 0.8960Train Epoch: 166 Loss= 0.425659865 Accuracy= 0.8964Train Epoch: 167 Loss= 0.425214738 Accuracy= 0.8964Train Epoch: 168 Loss= 0.424902737 Accuracy= 0.8960Train Epoch: 169 Loss= 0.424074769 Accuracy= 0.8966Train Epoch: 170 Loss= 0.423162699 Accuracy= 0.8974Train Epoch: 171 Loss= 0.422749072 Accuracy= 0.8974Train Epoch: 172 Loss= 0.422401696 Accuracy= 0.8964Train Epoch: 173 Loss= 0.421341300 Accuracy= 0.8976Train Epoch: 174 Loss= 0.420944721 Accuracy= 0.8964Train Epoch: 175 Loss= 0.419966906 Accuracy= 0.8976Train Epoch: 176 Loss= 0.419914395 Accuracy= 0.8968Train Epoch: 177 Loss= 0.418810934 Accuracy= 0.8970Train Epoch: 178 Loss= 0.418342829 Accuracy= 0.8968Train Epoch: 179 Loss= 0.417744726 Accuracy= 0.8970Train Epoch: 180 Loss= 0.417116791 Accuracy= 0.8978Train Epoch: 181 Loss= 0.416599661 Accuracy= 0.8976Train Epoch: 182 Loss= 0.415944725 Accuracy= 0.8982Train Epoch: 183 Loss= 0.415637791 Accuracy= 0.8978Train Epoch: 184 Loss= 0.414836079 Accuracy= 0.8980Train Epoch: 185 Loss= 0.414405614 Accuracy= 0.8978Train Epoch: 186 Loss= 0.413655072 Accuracy= 0.8982Train Epoch: 187 Loss= 0.413016111 Accuracy= 0.8982Train Epoch: 188 Loss= 0.412935942 Accuracy= 0.8988Train Epoch: 189 Loss= 0.412001669 Accuracy= 0.8984Train Epoch: 190 Loss= 0.411753237 Accuracy= 0.8992Train Epoch: 191 Loss= 0.411443263 Accuracy= 0.8990Train Epoch: 192 Loss= 0.410422057 Accuracy= 0.8984Train Epoch: 193 Loss= 0.410251558 Accuracy= 0.8996Train Epoch: 194 Loss= 0.409084409 Accuracy= 0.8988Train Epoch: 195 Loss= 0.408952326 Accuracy= 0.8998Train Epoch: 196 Loss= 0.408304602 Accuracy= 0.8998Train Epoch: 197 Loss= 0.407955080 Accuracy= 0.8992Train Epoch: 198 Loss= 0.407745570 Accuracy= 0.8992Train Epoch: 199 Loss= 0.406976283 Accuracy= 0.8994Train Epoch: 200 Loss= 0.406263798 Accuracy= 0.8996Train Epoch: 201 Loss= 0.406233251 Accuracy= 0.8994Train Epoch: 202 Loss= 0.405623794 Accuracy= 0.9000Train Epoch: 203 Loss= 0.404749840 Accuracy= 0.8998Train Epoch: 204 Loss= 0.404725671 Accuracy= 0.9006Train Epoch: 205 Loss= 0.403718770 Accuracy= 0.9002Train Epoch: 206 Loss= 0.403355181 Accuracy= 0.9006Train Epoch: 207 Loss= 0.402798116 Accuracy= 0.9008Train Epoch: 208 Loss= 0.402724475 Accuracy= 0.9002Train Epoch: 209 Loss= 0.402527630 Accuracy= 0.9006Train Epoch: 210 Loss= 0.401481658 Accuracy= 0.9006Train Epoch: 211 Loss= 0.401139230 Accuracy= 0.9006Train Epoch: 212 Loss= 0.400432110 Accuracy= 0.9004Train Epoch: 213 Loss= 0.400378883 Accuracy= 0.9004Train Epoch: 214 Loss= 0.399567872 Accuracy= 0.9002Train Epoch: 215 Loss= 0.399531484 Accuracy= 0.9006Train Epoch: 216 Loss= 0.399013489 Accuracy= 0.9008Train Epoch: 217 Loss= 0.397973686 Accuracy= 0.9012Train Epoch: 218 Loss= 0.397794008 Accuracy= 0.9008Train Epoch: 219 Loss= 0.397283971 Accuracy= 0.9012Train Epoch: 220 Loss= 0.397037268 Accuracy= 0.9014Train Epoch: 221 Loss= 0.396341175 Accuracy= 0.9020Train Epoch: 222 Loss= 0.396116793 Accuracy= 0.9002Train Epoch: 223 Loss= 0.395749956 Accuracy= 0.9014Train Epoch: 224 Loss= 0.395612061 Accuracy= 0.9008Train Epoch: 225 Loss= 0.394687176 Accuracy= 0.9012Train Epoch: 226 Loss= 0.394498616 Accuracy= 0.9016Train Epoch: 227 Loss= 0.394306749 Accuracy= 0.9012Train Epoch: 228 Loss= 0.393707484 Accuracy= 0.9008Train Epoch: 229 Loss= 0.393643707 Accuracy= 0.9008Train Epoch: 230 Loss= 0.393045634 Accuracy= 0.9008Train Epoch: 231 Loss= 0.392508119 Accuracy= 0.9022Train Epoch: 232 Loss= 0.391976804 Accuracy= 0.9018Train Epoch: 233 Loss= 0.391808689 Accuracy= 0.9006Train Epoch: 234 Loss= 0.391237199 Accuracy= 0.9014Train Epoch: 235 Loss= 0.390866280 Accuracy= 0.9006Train Epoch: 236 Loss= 0.390418172 Accuracy= 0.9010Train Epoch: 237 Loss= 0.389898777 Accuracy= 0.9024Train Epoch: 238 Loss= 0.389768124 Accuracy= 0.9020Train Epoch: 239 Loss= 0.389144570 Accuracy= 0.9020Train Epoch: 240 Loss= 0.389046282 Accuracy= 0.9030Train Epoch: 241 Loss= 0.388572276 Accuracy= 0.9024Train Epoch: 242 Loss= 0.388103604 Accuracy= 0.9018Train Epoch: 243 Loss= 0.387917459 Accuracy= 0.9026Train Epoch: 244 Loss= 0.387428015 Accuracy= 0.9024Train Epoch: 245 Loss= 0.387170792 Accuracy= 0.9032Train Epoch: 246 Loss= 0.386788905 Accuracy= 0.9014Train Epoch: 247 Loss= 0.386517614 Accuracy= 0.9030Train Epoch: 248 Loss= 0.386347741 Accuracy= 0.9020Train Epoch: 249 Loss= 0.385344386 Accuracy= 0.9036Train Epoch: 250 Loss= 0.384959489 Accuracy= 0.9026Train Epoch: 251 Loss= 0.384517908 Accuracy= 0.9028Train Epoch: 252 Loss= 0.384441257 Accuracy= 0.9030Train Epoch: 253 Loss= 0.383821398 Accuracy= 0.9028Train Epoch: 254 Loss= 0.383479148 Accuracy= 0.9022Train Epoch: 255 Loss= 0.383624524 Accuracy= 0.9032Train Epoch: 256 Loss= 0.383323073 Accuracy= 0.9022Train Epoch: 257 Loss= 0.382732749 Accuracy= 0.9028Train Epoch: 258 Loss= 0.382320166 Accuracy= 0.9030Train Epoch: 259 Loss= 0.381811768 Accuracy= 0.9028Train Epoch: 260 Loss= 0.381984144 Accuracy= 0.9026Train Epoch: 261 Loss= 0.381227553 Accuracy= 0.9026Train Epoch: 262 Loss= 0.380849212 Accuracy= 0.9022Train Epoch: 263 Loss= 0.380524695 Accuracy= 0.9040Train Epoch: 264 Loss= 0.380101144 Accuracy= 0.9028Train Epoch: 265 Loss= 0.380186915 Accuracy= 0.9024Train Epoch: 266 Loss= 0.379487067 Accuracy= 0.9030Train Epoch: 267 Loss= 0.379631072 Accuracy= 0.9034Train Epoch: 268 Loss= 0.378731072 Accuracy= 0.9034Train Epoch: 269 Loss= 0.378665388 Accuracy= 0.9030Train Epoch: 270 Loss= 0.378257990 Accuracy= 0.9032Train Epoch: 271 Loss= 0.377845258 Accuracy= 0.9038Train Epoch: 272 Loss= 0.377908498 Accuracy= 0.9034Train Epoch: 273 Loss= 0.377575696 Accuracy= 0.9038Train Epoch: 274 Loss= 0.377002954 Accuracy= 0.9034Train Epoch: 275 Loss= 0.376586318 Accuracy= 0.9034Train Epoch: 276 Loss= 0.376820147 Accuracy= 0.9040Train Epoch: 277 Loss= 0.376379251 Accuracy= 0.9036Train Epoch: 278 Loss= 0.375819713 Accuracy= 0.9036Train Epoch: 279 Loss= 0.375545800 Accuracy= 0.9046Train Epoch: 280 Loss= 0.375153273 Accuracy= 0.9044Train Epoch: 281 Loss= 0.375212997 Accuracy= 0.9034Train Epoch: 282 Loss= 0.374674737 Accuracy= 0.9042Train Epoch: 283 Loss= 0.374336630 Accuracy= 0.9046Train Epoch: 284 Loss= 0.373770863 Accuracy= 0.9042Train Epoch: 285 Loss= 0.373896927 Accuracy= 0.9040Train Epoch: 286 Loss= 0.373323888 Accuracy= 0.9046Train Epoch: 287 Loss= 0.373260558 Accuracy= 0.9046Train Epoch: 288 Loss= 0.372779518 Accuracy= 0.9044Train Epoch: 289 Loss= 0.372727990 Accuracy= 0.9046Train Epoch: 290 Loss= 0.372335255 Accuracy= 0.9046Train Epoch: 291 Loss= 0.372065455 Accuracy= 0.9048Train Epoch: 292 Loss= 0.371591061 Accuracy= 0.9046Train Epoch: 293 Loss= 0.371365815 Accuracy= 0.9046Train Epoch: 294 Loss= 0.371317178 Accuracy= 0.9048Train Epoch: 295 Loss= 0.370640844 Accuracy= 0.9052Train Epoch: 296 Loss= 0.370910048 Accuracy= 0.9044Train Epoch: 297 Loss= 0.370179355 Accuracy= 0.9050Train Epoch: 298 Loss= 0.370271593 Accuracy= 0.9054Train Epoch: 299 Loss= 0.369908154 Accuracy= 0.9054Train Epoch: 300 Loss= 0.369609326 Accuracy= 0.9054Train Epoch: 301 Loss= 0.369176894 Accuracy= 0.9054Train Epoch: 302 Loss= 0.369029045 Accuracy= 0.9056Train Epoch: 303 Loss= 0.368548244 Accuracy= 0.9058Train Epoch: 304 Loss= 0.368454874 Accuracy= 0.9058Train Epoch: 305 Loss= 0.368041515 Accuracy= 0.9056Train Epoch: 306 Loss= 0.367680520 Accuracy= 0.9058Train Epoch: 307 Loss= 0.367845863 Accuracy= 0.9056Train Epoch: 308 Loss= 0.367201656 Accuracy= 0.9060Train Epoch: 309 Loss= 0.366869599 Accuracy= 0.9062Train Epoch: 310 Loss= 0.366847694 Accuracy= 0.9060Train Epoch: 311 Loss= 0.366336137 Accuracy= 0.9058Train Epoch: 312 Loss= 0.366104513 Accuracy= 0.9060Train Epoch: 313 Loss= 0.366278410 Accuracy= 0.9058Train Epoch: 314 Loss= 0.365434617 Accuracy= 0.9064Train Epoch: 315 Loss= 0.365378529 Accuracy= 0.9064Train Epoch: 316 Loss= 0.365533412 Accuracy= 0.9058Train Epoch: 317 Loss= 0.365197808 Accuracy= 0.9068Train Epoch: 318 Loss= 0.365037024 Accuracy= 0.9068Train Epoch: 319 Loss= 0.364381582 Accuracy= 0.9064Train Epoch: 320 Loss= 0.364105165 Accuracy= 0.9072Train Epoch: 321 Loss= 0.364401549 Accuracy= 0.9066Train Epoch: 322 Loss= 0.363872409 Accuracy= 0.9066Train Epoch: 323 Loss= 0.363355607 Accuracy= 0.9072Train Epoch: 324 Loss= 0.363645494 Accuracy= 0.9064Train Epoch: 325 Loss= 0.362869859 Accuracy= 0.9070Train Epoch: 326 Loss= 0.362704009 Accuracy= 0.9066Train Epoch: 327 Loss= 0.362510622 Accuracy= 0.9072Train Epoch: 328 Loss= 0.362664402 Accuracy= 0.9064Train Epoch: 329 Loss= 0.362214684 Accuracy= 0.9070Train Epoch: 330 Loss= 0.361763179 Accuracy= 0.9068Train Epoch: 331 Loss= 0.361724973 Accuracy= 0.9072Train Epoch: 332 Loss= 0.361439168 Accuracy= 0.9072Train Epoch: 333 Loss= 0.361238927 Accuracy= 0.9072Train Epoch: 334 Loss= 0.360923618 Accuracy= 0.9070Train Epoch: 335 Loss= 0.360640526 Accuracy= 0.9070Train Epoch: 336 Loss= 0.360468805 Accuracy= 0.9074Train Epoch: 337 Loss= 0.360336691 Accuracy= 0.9070Train Epoch: 338 Loss= 0.360181004 Accuracy= 0.9066Train Epoch: 339 Loss= 0.359589636 Accuracy= 0.9078Train Epoch: 340 Loss= 0.359605044 Accuracy= 0.9074Train Epoch: 341 Loss= 0.359191000 Accuracy= 0.9068Train Epoch: 342 Loss= 0.359398872 Accuracy= 0.9072Train Epoch: 343 Loss= 0.358821988 Accuracy= 0.9072Train Epoch: 344 Loss= 0.358554870 Accuracy= 0.9072Train Epoch: 345 Loss= 0.358417094 Accuracy= 0.9074Train Epoch: 346 Loss= 0.358287454 Accuracy= 0.9082Train Epoch: 347 Loss= 0.358403027 Accuracy= 0.9074Train Epoch: 348 Loss= 0.357745498 Accuracy= 0.9076Train Epoch: 349 Loss= 0.357700974 Accuracy= 0.9074Train Epoch: 350 Loss= 0.357280284 Accuracy= 0.9074Train Epoch: 351 Loss= 0.357116640 Accuracy= 0.9072Train Epoch: 352 Loss= 0.356674671 Accuracy= 0.9076Train Epoch: 353 Loss= 0.356743395 Accuracy= 0.9080Train Epoch: 354 Loss= 0.356310487 Accuracy= 0.9080Train Epoch: 355 Loss= 0.356106997 Accuracy= 0.9080Train Epoch: 356 Loss= 0.356351852 Accuracy= 0.9076Train Epoch: 357 Loss= 0.356079578 Accuracy= 0.9078Train Epoch: 358 Loss= 0.355536312 Accuracy= 0.9080Train Epoch: 359 Loss= 0.355534166 Accuracy= 0.9080Train Epoch: 360 Loss= 0.355263025 Accuracy= 0.9070Train Epoch: 361 Loss= 0.355054259 Accuracy= 0.9086Train Epoch: 362 Loss= 0.354682177 Accuracy= 0.9086Train Epoch: 363 Loss= 0.354584038 Accuracy= 0.9084Train Epoch: 364 Loss= 0.354257971 Accuracy= 0.9076Train Epoch: 365 Loss= 0.354202539 Accuracy= 0.9084Train Epoch: 366 Loss= 0.354148418 Accuracy= 0.9080Train Epoch: 367 Loss= 0.353945524 Accuracy= 0.9072Train Epoch: 368 Loss= 0.353680283 Accuracy= 0.9084Train Epoch: 369 Loss= 0.353619725 Accuracy= 0.9088Train Epoch: 370 Loss= 0.353221059 Accuracy= 0.9086Train Epoch: 371 Loss= 0.353100061 Accuracy= 0.9084Train Epoch: 372 Loss= 0.352835447 Accuracy= 0.9090Train Epoch: 373 Loss= 0.352596819 Accuracy= 0.9084Train Epoch: 374 Loss= 0.352595180 Accuracy= 0.9080Train Epoch: 375 Loss= 0.352259755 Accuracy= 0.9090Train Epoch: 376 Loss= 0.352301449 Accuracy= 0.9086Train Epoch: 377 Loss= 0.352147013 Accuracy= 0.9088Train Epoch: 378 Loss= 0.351966113 Accuracy= 0.9088Train Epoch: 379 Loss= 0.351528525 Accuracy= 0.9088Train Epoch: 380 Loss= 0.351112694 Accuracy= 0.9088Train Epoch: 381 Loss= 0.350878716 Accuracy= 0.9088Train Epoch: 382 Loss= 0.351027966 Accuracy= 0.9094Train Epoch: 383 Loss= 0.350885630 Accuracy= 0.9092Train Epoch: 384 Loss= 0.350694865 Accuracy= 0.9086Train Epoch: 385 Loss= 0.350394100 Accuracy= 0.9084Train Epoch: 386 Loss= 0.349927545 Accuracy= 0.9090Train Epoch: 387 Loss= 0.350075901 Accuracy= 0.9092Train Epoch: 388 Loss= 0.349799097 Accuracy= 0.9086Train Epoch: 389 Loss= 0.349439144 Accuracy= 0.9098Train Epoch: 390 Loss= 0.349565566 Accuracy= 0.9100Train Epoch: 391 Loss= 0.349208802 Accuracy= 0.9102Train Epoch: 392 Loss= 0.349225044 Accuracy= 0.9100Train Epoch: 393 Loss= 0.349022955 Accuracy= 0.9106Train Epoch: 394 Loss= 0.348788708 Accuracy= 0.9090Train Epoch: 395 Loss= 0.348476678 Accuracy= 0.9092Train Epoch: 396 Loss= 0.348356456 Accuracy= 0.9096Train Epoch: 397 Loss= 0.348270595 Accuracy= 0.9094Train Epoch: 398 Loss= 0.348110557 Accuracy= 0.9100Train Epoch: 399 Loss= 0.347774327 Accuracy= 0.9114Train Epoch: 400 Loss= 0.347881585 Accuracy= 0.9108Train Epoch: 401 Loss= 0.347597957 Accuracy= 0.9106Train Epoch: 402 Loss= 0.347694665 Accuracy= 0.9110Train Epoch: 403 Loss= 0.347311169 Accuracy= 0.9116Train Epoch: 404 Loss= 0.347082287 Accuracy= 0.9106Train Epoch: 405 Loss= 0.346681118 Accuracy= 0.9104Train Epoch: 406 Loss= 0.346817166 Accuracy= 0.9100Train Epoch: 407 Loss= 0.346650332 Accuracy= 0.9102Train Epoch: 408 Loss= 0.346380711 Accuracy= 0.9116Train Epoch: 409 Loss= 0.346278042 Accuracy= 0.9116Train Epoch: 410 Loss= 0.346026599 Accuracy= 0.9106Train Epoch: 411 Loss= 0.345805764 Accuracy= 0.9102Train Epoch: 412 Loss= 0.345602989 Accuracy= 0.9100Train Epoch: 413 Loss= 0.345262617 Accuracy= 0.9100Train Epoch: 414 Loss= 0.345551938 Accuracy= 0.9114Train Epoch: 415 Loss= 0.345476508 Accuracy= 0.9114Train Epoch: 416 Loss= 0.345072389 Accuracy= 0.9110Train Epoch: 417 Loss= 0.345159948 Accuracy= 0.9100Train Epoch: 418 Loss= 0.344624996 Accuracy= 0.9112Train Epoch: 419 Loss= 0.344826609 Accuracy= 0.9112Train Epoch: 420 Loss= 0.344251603 Accuracy= 0.9098Train Epoch: 421 Loss= 0.344352007 Accuracy= 0.9110Train Epoch: 422 Loss= 0.344260216 Accuracy= 0.9110Train Epoch: 423 Loss= 0.343791753 Accuracy= 0.9102Train Epoch: 424 Loss= 0.343953133 Accuracy= 0.9108Train Epoch: 425 Loss= 0.343585521 Accuracy= 0.9108Train Epoch: 426 Loss= 0.343450099 Accuracy= 0.9106Train Epoch: 427 Loss= 0.343184948 Accuracy= 0.9112Train Epoch: 428 Loss= 0.343215674 Accuracy= 0.9110Train Epoch: 429 Loss= 0.343070745 Accuracy= 0.9108Train Epoch: 430 Loss= 0.342824966 Accuracy= 0.9110Train Epoch: 431 Loss= 0.342438400 Accuracy= 0.9114Train Epoch: 432 Loss= 0.342611969 Accuracy= 0.9114Train Epoch: 433 Loss= 0.342214018 Accuracy= 0.9110Train Epoch: 434 Loss= 0.342429280 Accuracy= 0.9108Train Epoch: 435 Loss= 0.341949135 Accuracy= 0.9112Train Epoch: 436 Loss= 0.342126906 Accuracy= 0.9110Train Epoch: 437 Loss= 0.341858774 Accuracy= 0.9110Train Epoch: 438 Loss= 0.341584474 Accuracy= 0.9110Train Epoch: 439 Loss= 0.341345459 Accuracy= 0.9112Train Epoch: 440 Loss= 0.341049880 Accuracy= 0.9120Train Epoch: 441 Loss= 0.340980768 Accuracy= 0.9116Train Epoch: 442 Loss= 0.340953112 Accuracy= 0.9114Train Epoch: 443 Loss= 0.340954691 Accuracy= 0.9116Train Epoch: 444 Loss= 0.340899229 Accuracy= 0.9110Train Epoch: 445 Loss= 0.340938568 Accuracy= 0.9110Train Epoch: 446 Loss= 0.341198325 Accuracy= 0.9110Train Epoch: 447 Loss= 0.340240568 Accuracy= 0.9112Train Epoch: 448 Loss= 0.340235293 Accuracy= 0.9112Train Epoch: 449 Loss= 0.340209186 Accuracy= 0.9114Train Epoch: 450 Loss= 0.339898020 Accuracy= 0.9116Train Epoch: 451 Loss= 0.339793384 Accuracy= 0.9114Train Epoch: 452 Loss= 0.339602858 Accuracy= 0.9120Train Epoch: 453 Loss= 0.339292139 Accuracy= 0.9116Train Epoch: 454 Loss= 0.339383274 Accuracy= 0.9120Train Epoch: 455 Loss= 0.339096963 Accuracy= 0.9114Train Epoch: 456 Loss= 0.339157701 Accuracy= 0.9114Train Epoch: 457 Loss= 0.338972569 Accuracy= 0.9120Train Epoch: 458 Loss= 0.338898569 Accuracy= 0.9118Train Epoch: 459 Loss= 0.338588566 Accuracy= 0.9120Train Epoch: 460 Loss= 0.338383496 Accuracy= 0.9116Train Epoch: 461 Loss= 0.338389367 Accuracy= 0.9118Train Epoch: 462 Loss= 0.338129699 Accuracy= 0.9120Train Epoch: 463 Loss= 0.338002205 Accuracy= 0.9120Train Epoch: 464 Loss= 0.337737024 Accuracy= 0.9118Train Epoch: 465 Loss= 0.337787151 Accuracy= 0.9116Train Epoch: 466 Loss= 0.337848932 Accuracy= 0.9118Train Epoch: 467 Loss= 0.337492615 Accuracy= 0.9112Train Epoch: 468 Loss= 0.337512612 Accuracy= 0.9120Train Epoch: 469 Loss= 0.337134123 Accuracy= 0.9116Train Epoch: 470 Loss= 0.336997300 Accuracy= 0.9118Train Epoch: 471 Loss= 0.336745471 Accuracy= 0.9120Train Epoch: 472 Loss= 0.337093860 Accuracy= 0.9124Train Epoch: 473 Loss= 0.336436093 Accuracy= 0.9120Train Epoch: 474 Loss= 0.336423337 Accuracy= 0.9116Train Epoch: 475 Loss= 0.336435258 Accuracy= 0.9116Train Epoch: 476 Loss= 0.336220562 Accuracy= 0.9120Train Epoch: 477 Loss= 0.336083412 Accuracy= 0.9112Train Epoch: 478 Loss= 0.336012244 Accuracy= 0.9116Train Epoch: 479 Loss= 0.335907310 Accuracy= 0.9116Train Epoch: 480 Loss= 0.336151212 Accuracy= 0.9126Train Epoch: 481 Loss= 0.335842371 Accuracy= 0.9114Train Epoch: 482 Loss= 0.335602969 Accuracy= 0.9126Train Epoch: 483 Loss= 0.335610300 Accuracy= 0.9124Train Epoch: 484 Loss= 0.335590661 Accuracy= 0.9120Train Epoch: 485 Loss= 0.335201710 Accuracy= 0.9124Train Epoch: 486 Loss= 0.334876269 Accuracy= 0.9124Train Epoch: 487 Loss= 0.334915966 Accuracy= 0.9120Train Epoch: 488 Loss= 0.335007548 Accuracy= 0.9124Train Epoch: 489 Loss= 0.334643126 Accuracy= 0.9124Train Epoch: 490 Loss= 0.334648579 Accuracy= 0.9124Train Epoch: 491 Loss= 0.334624171 Accuracy= 0.9126Train Epoch: 492 Loss= 0.334362388 Accuracy= 0.9134Train Epoch: 493 Loss= 0.334085405 Accuracy= 0.9132Train Epoch: 494 Loss= 0.333826602 Accuracy= 0.9128Train Epoch: 495 Loss= 0.333956271 Accuracy= 0.9128Train Epoch: 496 Loss= 0.334147930 Accuracy= 0.9126Train Epoch: 497 Loss= 0.333745390 Accuracy= 0.9128Train Epoch: 498 Loss= 0.333584398 Accuracy= 0.9130Train Epoch: 499 Loss= 0.333650619 Accuracy= 0.9126Train Epoch: 500 Loss= 0.333045900 Accuracy= 0.9126Train Finshied!Test Accurary: 0.9094 学习效果极好："},{"title":"多元线性回归：波士顿房价预测问题","date":"2022-01-11T12:00:00.000Z","url":"/2022/01/11/machine-learning-4/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"数据与问题分析 问题介绍波士顿房价数据集包括506个样本，每个样本包括12个特征变量和该地区的平均房价房价（单价）显然和多个特征变量相关，不是单变量线性回归（一元线性回归）问题选择多个特征变量来建立线性方程，这就是多变量线性回归（多元线性回归）问题 采用上一节所讲：机器学习步骤 数据集解读CRIM: 城镇人均犯罪率ZN：住宅用地超过 25000 sq.ft. 的比例INDUS: 城镇非零售商用土地的比例CHAS: 边界是河流为1，否则0NOX: 一氧化氮浓度RM: 住宅平均房间数AGE: 1940年之前建成的自用房屋比例DIS：到波士顿5个中心区域的加权距离RAD: 辐射性公路的靠近指数TAX: 每10000美元的全值财产税率PTRATIO: 城镇师生比例LSTAT: 人口中地位低下者的比例MEDV: 自住房的平均房价，单位：千美元 读取数据（想快速读取常规大小的数据文件时，通过创建读缓存区和其他的机制可能会造成额外的开销。此时建议采用Pandas库来处理。） 准备建模采用多元线性回归模型多变量线性方程的矩阵运算表示 第一个版本的模型构建数据准备 构建模型定义训练数据占位符shape中 None 表示行的数量未知，在实际训练时决定一次代入多少行样本，从一个样本的随机SDG到批量SDG都可以 定义模型结构定义命名空间Tensorflow计算图模型中常有数以千计节点，在可视化过程中很难一下子全部展示出来，因此可用name_scope为变量划分范围，在可视化中，这表示在计算图中的一个层级。 训练模型设置训练超参数定义均方差损失函数声明会话/启动会话迭代训练 运行结果出现异常！！ 探究训练结果异常的原因：从梯度下降讲起二元两个变量（或多变量）要比一元单变量复杂得多，要考虑不同特征值取值范围大小的影响采用方法：归一化( 特征值- 特征值 最小值)/ ( 特征值最大值- 特征值最小值） 后续版本的持续改进特征数据归一化 模型应用 可视化训练过程中的损失值 可视化损失值 加上TensorBoard可视化代码为可视化准备数据 创建摘要的文件写入器 添加代码 "},{"title":"单变量线性回归","date":"2022-01-10T12:00:00.000Z","url":"/2022/01/10/machine-learning-3/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"监督式机器学习的基本术语标签和特征标签是我们要预测的真实事物：y 线性回归中的y变量特征是指用于描述数据的输入变量：xi 线性回归中的 {x1 ,x2 ,…,xn }变量 样本和模型样本是指数据的特定实例：x有标签样本具有{特征，标签}： {x ，y} //用于训练模型无标签样本具有{特征，？}： {x ，?} //用于对新数据做出预测模型可将样本映射到预测标签：y’ //由模型的内部参数定义，这些内部参数值是通过学习得到的 训练训练模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值 在监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试找出可最大限度地减少损失的模型这一过程称为经验风险最小化 损失损失是对糟糕预测的惩罚： 损失是一个数值，表示对于单个样本而言模型预测的准确程度如果模型的预测完全准确，则损失为零，否则损失会较大训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差 定义损失函数L1损失 ：基于模型预测的值与标签的实际值之差的绝对值 平方损失 ：一种常见的损失函数，又称为 L2损失均方误差 ( MSE) 指的是每个样本的平均平方损失 训练模型的迭代方法 模型训练要点首先对权重w和偏差b进行初始猜测，然后反复调整这些猜测，直到获得损失可能最低的权重和偏差为止 收敛在学习优化过程中，机器学习系统将根据所有标签去重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。通常，您可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已收敛 梯度下降法梯度：一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大 梯度是矢量:具有方向和大小 学习率用梯度乘以一个称为学习速率（有时也称为 步长）的标量，以确定下一个点的位置 超参数在机器学习中，超参数是在开始学习过程 之前设置值的参数，而不是通过训练得到的参数数据 通常情况下，需要对超参数进行优化，选择一组好的超参数，可以提高学习的性能和效果 超参数是编程人员在机器学习算法中用于调整的旋钮 典型超参数：学习率、神经网络的隐含层数量…… 线性回归问题TensorFlow实战核心步骤（1）准备数据（2）构建模型（3）训练模型（4）进行预测 人工数据集生成 （可以使用tab键进行代码补齐） 构建模型定位训练数据的占位符，x是特征值，y是标签值 定义模型函数 定义模型结构创建变量 训练模型设置训练参数 定义损失函数• 损失函数用于描述预测值与真实值之间的误差，从而指导模型收敛方向• 常见损失函数：均方差（Mean Square Error, MSE）和交叉熵（cross-entropy） 定义优化器定义优化器Optimizer，初始化一个GradientDescentOptimizer设置学习率和优化目标：最小化损失 创建会话声明会话变量初始化 迭代训练模型训练阶段，设置迭代轮次，每次通过将样本逐个输入模型，进行梯度下降优化操作每轮迭代后，绘制出模型曲线 可视化 最终可视化结果如下： 利用模型 进行预测结果显示，模型预测结果极佳 小结通过一个简单的例子介绍了利用Tensorflow实现机器学习的思路，重点讲解了下述步骤：（1）生成人工数据集及其可视化（2）构建线性模型（3）定义损失函数（4）定义优化器、最小化损失函数（5）训练结果的可视化（6）利用学习到的模型进行预测 完整程序附下： 进阶显示损失值 随机梯度下降在梯度下降法中， 批量指的是用于在单次迭代中计算梯度的样本总数 假定批量是指整个数据集，数据集通常包含很大样本（数万甚至数千亿），此外， 数据集通常包含多个特征。因此，一个批量可能相当巨大。如果是超大批量，则单次迭代就可能要花费很长时间进行计算 随机梯度下降法 ( SGD) 每次迭代只使用一个样本（批量大小为 1），如果进行足够的迭代，SGD 也可以发挥作用。“随机”这一术语表示构成各个批量的一个样本都是随机选择的 小批量随机梯度下降法（量 小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效"},{"title":"TensorFlow编程基础","date":"2022-01-09T04:00:00.000Z","url":"/2022/01/09/machine-learning-2/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"TensorFlow的基础概念A machine learning platform for everyone to solve real problems. 开放源码软件库，用于进行高性能数值计算 Tensorflow的Hello WorldTensor 张量 数据结构：多维数组Flow 流 计算模型： 张量之间通过计算而转换的过程 通过计算图的形式表述计算的编程系统，计算为节点，边描述计算之间的关系 计算图是一个有向图，由以下内容构成：• 一组节点，每个节点都代表一个操作，是一种运算• 一组有向边，每条边代表节点之间的关系（数据传递和控制依赖） TensorFlow有两种边：• 常规边（实线）：代表数据依赖关系。一个节点的运算输出成为另一个节点的输入，两个节点之间有tensor流动（ 值传递）• 特殊边（虚线）：不携带值，表示两个节点之间的 控制相关性。比如， happens- - before 关系，源节点必须在目的节点执行前完成执行 计算图的实例输出一个张量结构： 建立对话显示运行结果： 张量的概念在TensorFlow中，所有的数据都通过张量的形式来表示• 从功能的角度，张量可以简单理解为多维数组零阶张量表示标量（scalar），也就是 一个数；一阶张量为向量（vector），也就是 一维数组；n n 阶张量可以理解为一个 n 维数组；• 张量并没有真正保存数字，它保存的是计算过程 张量的属性 名字（ name ）“node:src_output”：node 节点名称，src_output 来自节点的第几个输出形状（ shape ）张量的维度信息， shape=() ，表示是标量类型（ type ）每一个张量会有一个唯一的类型TensorFlow会对参与运算的所有张量进行类型的检查，发现类型不匹配时会报错 张量的形状三个术语描述张量的维度： 阶（rank）、 形状（shape） 、 维数（dimension number） （数组下标从0开始） 张量的类型TensorFlow支持14种不同的类型实数 tf.float32, tf.float64整数 tf.int8, tf.int16, tf.int32, tf.int64, tf.uint8布尔 tf.bool复数 tf.complex64, tf.complex128张量的类型默认类型：不带小数点的数会被默认为int32带小数点的会被默认为float32 操作计算图中的 节点就是 操作（Operation）• 一次加法是一个操作• 一次乘法也是一个操作• 构建一些变量的初始值也是一个操作• 每个运算操作都有 属性，它在构建图的时候需要确定下来• 操作可以和计算 设备绑定，指定操作在某个设备上执行• 操作之间存在顺序关系，这些操作之间的 依赖就是“ 边”• 如果操作A的输入是操作B执行的结果，那么这个操作A就依赖于操作B TensorFlow的基本运算会话会话1会话2在交互式环境下，Python脚本或者Jupyter编辑器下，通过设置默认会话来获取张量的取值更加方便tf.InteractiveSession 使用这个函数会自动将生成的会话注册为默认会话 常量和变量常量 在运行过程中值不会改变的单元，在TensorFlow中无须进行初始化操作创建语句：变量 在运行过程中值会改变的单元，在TensorFlow中须进行初始化操作创建语句：个别变量初始化：所有变量初始化： 变量的赋值变量更新语句： 占位符TensorFlow中的 Variable 变量类型，在定义时需要初始化，但有些变量定义时并不知道其数值，只有当真正开始运行程序时，才由外部输入，比如训练数据，这时候需要用到占位符tf.placeholder 占位符，是TensorFlow中特有的一种数据结构，类似动态变量，函数的参数、或者C语言或者Python语言中格式化输出时的“%”占位符 TensorFlow占位符Placeholder，先定义一种数据，其参数为数据的Type和Shape占位符Placeholder的函数接口如下： Feed提交数据和Fetch提取数据 TensorBoard可视化初步• TensorBoard是TensorFlow的可视化工具• 通过TensorFlow程序运行过程中输出的日志文件可视化TensorFlow程序的运行状态• TensorBoard和TensorFlow程序跑在不同的进程中TensorBoard不需要额外安装，在TensorFlow安装时已自动完成在 Anaconda Prompt中先进入日志存放的目录（ 非常重要！！！）再运行TensorBoard，并将日志的地址指向程序日志输出的地址命令："},{"title":"深度学习简介及开发环境搭建","date":"2022-01-08T13:07:09.000Z","url":"/2022/01/08/machine-learning-1/","tags":[["DL","/tags/DL/"]],"categories":[["DL","/categories/DL/"]],"content":"从今天开始我便正式进入到针对深度学习应用开发的学习中，主要课程资源为中国大学慕课,附网址如下： 今天所学为第一讲 人工智能导论（略），第二讲 深度学习简介及开发环境搭建，其中第二讲中包括：人工智能，机器学习与深度学习，深度学习网路与深度学习框架，Anaconda和Tensorflow开发环境搭建 人工智能，机器学习与深度学习首先介绍为19世纪50年代以来人工智能的发展，人工智能包含机器学习，机器学习是实现人工智能的一个分支，是实现人工智能的一个核心技术，机器学习中又包含深度学习，深度学习则是机器学习的一种实现方法，常见的有图片分类等应用。 机器学习的形式化的描述对于某类任务T和性能度量P，如果一个计算机程序在T上以P衡量的性能，随着经验E而自我完善，那么就称这个计算机程序从经验E学习 大致就是通过历史数据训练出一个模型，将新的数据输入，通过模型的预测，则去预测其未知属性 机器学习的学习形式分类：有监督学习，无监督学习，半监督学习，强化学习 有监督学习（supervised learning),事先准备好输入与正确输出相配套的训练数据，让计算机进行学习，以便当他被输入某个数据时能够得到正确的输出无监督学习（unsupervised learning)，仅提供输入用数据，需要计算机自己找出数据内在结构，目的是让计算机从数据中抽取其中所包含的模式及规则半监督学习 (semi_supersived learning)，训练数据一部分有标记，另一部分没有标记，而没有标记的数量常常极大于有标记数据的数量，基本规律：数据的分布必然不是完全随机的，通过结合有标记数据的局部特征，以及大量没标记数据的整体分布，可以得到比较好的分类结果强化学习 (reinforcement learning),解决计算机从感知到决策控制的问题，强化学习是目标导向的，从白纸一张的状态开始，经由许多个步骤来实现某一维度上的目标最大化。最简单的理解是在训练过程中，不断去尝试，错误就惩罚，正确就奖励，由此训练得到的模型在各个状态环境中都最好。对强化学习来说，它虽然没有标记，但有一个延迟奖励与训练相关，通过学习过程中的激励函数获得某种从状态到行动的映射。强化学习强调如何基于环境而行动，以取得最大化的预期利益。 无监督机器学习的典型应用模式：聚类 常见算法： k-means 关联规则抽取 有监督机器学习的典型应用模式：预测 针对连续数据 常见算法： 线性回归 Gradient Boosting AdaBoost 神经网络预测 针对离散数据 常见算法： 逻辑回归 决策树 随机森林 朴素贝叶斯 神经网络 神经网络包括输入层，隐含层，输出层常见激活函数：S型(Sigmoid)将加权和转换为介于0和1之间的值修正线性单元激活函数(简称为ReLU)易于计算 深度神经网路与深度学习框架卷积神经网络卷积神经网络（Convolutional Neural Network, CNN）是深度学习中最重要的概念之一20世纪60年代，Hubel和Wiesel在研究猫脑皮层中用于局部敏感和方向选择的神经元时发现，其独特的网络结构可以有效降低神经网络的复杂性1998年，Yann LeCun提出了LeNet神经网络，标志着第一个采用卷积思想的神经网络面世 Anaconda和Tensorflow开发环境搭建鉴于课程提供的安装版本我的电脑并不支持，于是我自行从官网进行下载并进行了环境配置，其中遇到很多问题，面对众多的error报错，确实磨练了我的耐心，和解决问题的能力，安装过程就不再赘述了，总之大约花了我三四个小时吧 下面是我安装完成后的程序测试：而我得到的运算结果如下："},{"title":"胡适散文（一）","date":"2021-12-12T09:00:00.000Z","url":"/2021/12/12/211212/","tags":[["Literature","/tags/Literature/"]],"categories":[["Literature","/categories/Literature/"]],"content":"我们要深信：今日的失败，都由于过去的不努力。我们要深信：今日的努力，必定有将来的大收成。 胡适《赠与今年的大学毕业生》 今日，读胡适先生的散文《赠与今年的大学毕业生》有感，遂写此文。 此文胡适先生最早发表于1932年7月《独立评论》第7号，目的在于勉励当年的即将大学毕业的青年学子。 这一两个星期里，各地的大学都有毕业的班次，都有很多的毕业生离开学校去开始他们的成人事业。学生的生活是一种享有特殊优待的生活，不妨幼稚一点，不妨吵吵闹闹，社会都能纵容他们，不肯严格的要他们负行为的责任。现在他们要撑起自己的肩膀来挑他们自己的担子了。在这个国难最紧急的年头，他们的担子真不轻！ 我们祝他们的成功，同时也不忍不依据我们自己的经验，赠与他们几句送行的赠言——虽未必是救命毫毛，也许作个防身的锦囊罢！ 你们毕业之后，可走的路不出这几条：绝少数的人还可以在国内或国外的研究院继续作学术研究；少数的人可以寻着相当的职业；此外还有做官，办党，革命三条路；此外就是在家享福或者失业闲居了。第一条继续求学之路，我们可以不讨论。走其余几条路的人，都不能没有堕落的危险。堕落的方式很多，总括起来，约有这两大类： 第一是容易抛弃学生时代的求知识的欲望。你们到了实际社会里，往往所用非所学，往往所学全无用处，往往可以完全用不着学问，而一样可以胡乱混饭吃，混官做。在这种环境里，即使向来抱有求知识学问的决心的人，也不免心灰意懒，把求知的欲望渐渐冷淡下去。况且学问是要有相当的设备的；书籍，试验室，师友的切磋指导，闲暇的工夫，都不是一个平常要糊口养家的人所能容易办到的。没有做学问的环境，又谁能怪我们抛弃学问呢？ 第二是容易抛弃学生时代的理想的人生的追求。少年人初次与冷酷的社会接触，容易感觉理想与事实相去太远，容易发生悲观和失望。多年怀抱的人生理想，改造的热诚，奋斗的勇气，到此时候，好像全不是那么一回事。渺小的个人在那强烈的社会炉火里，往往经不起长时期的烤炼就镕化了，一点高尚的理想不久就幻灭了。抱着改造社会的梦想而来，往往是弃甲曳兵而走，或者做了恶势力的俘虏。你在那俘虏牢狱里，回想那少年气壮时代的种种理想主义，好像都成了自误误人的迷梦！从此以后，你就甘心放弃理想人生的追求，甘心做现成社会的顺民了。 要防御这两方面的堕落，一面要保持我们求知识的欲望，一面要保持我们对于理想人生的追求。有什么好法子呢？依我个人的观察和经验，有三种防身的药方是值得一试的。 第一个方子只有一句话：“总得时时寻一两个值得研究的问题！”问题是知识学问的老祖宗；古今来一切知识的产生与积聚，都是因为要解答问题，——要解答实用上的困难或理论上的疑难。所谓“为知识而求知识”，其实也只是一种好奇心追求某种问题的解答，不过因为那种问题的性质不必是直接应用的，人们就觉得这是“无所为”的求知识了。我们出学校之后，离开了做学问的环境，如果没有一个两个值得解答的疑难问题在脑子里盘旋，就很难继续保持追求学问的热心。可是，如果你有了一个真有趣的问题天天逗你去想他，天天引诱你去解决他，天天对你挑衅笑你无可奈他，——这时候，你就会同恋爱一个女子发了疯一样，坐也坐不下，睡也睡不安，没工夫也得偷出工夫去陪她，没钱也得撙衣节食去巴结她。没有书，你自会变卖家私去买书；没有仪器，你自会典押衣服去置办仪器；没有师友，你自会不远千里去寻师访友。你只要能时时有疑难问题来逼你用脑子，你自然会保持发展你对学问的兴趣，即使在最贫乏的智识环境中，你也会慢慢的聚起一个小图书馆来，或者设置起一所小试验室来。所以我说：第一要寻问题，脑子里没有问题之日，就是你的智识生活寿终正寝之时！古人说，“待文王而兴者，凡民也。若夫豪杰之士，虽无文王犹兴。”试想葛理略（Galieo）和牛敦(Newton)有多少藏书？有多少仪器？他们不过是有问题而已。有了问题而后，他们自会造出仪器来解答他们的问题。没有问题的人们，关在图书馆里也不会用书，锁在试验室里也不会有什么发现。 第二个方子也只有一句话：“总得多发展一点非职业的兴趣。”离开学校之后，大家总得寻个吃饭的职业。可是你寻得的职业未必就是你所学的，或者未必是你所心喜的，或者是你所学而实在和你的性情不想近的。在这种状况之下，工作就往往成了苦工，就不感觉兴趣了。为糊口而作那种非“性之所近而力之所能勉”的工作，就很难保持求知的兴趣和生活的理想主义。最好的救济方法只有多多发展职业以外的正当兴趣与活动。一个人应该有他的职业，又应该有他的非职业的顽艺儿，可以叫做业余活动。凡一个人用他的闲暇来做的事业，都是他的业余活动。往往他的业余活动比他的职业还更重要，因为一个人的前程往往会靠他怎样用他的闲暇时间。他用他的闲暇来打马将，他就成个赌徒；你用你的闲暇来做社会服务，你也许成个社会改革者；或者你用你的闲暇去研究历史，你也许成个史学家。你的闲暇往往定你的终身。英国十九世纪的两个哲人，弥儿（J.S.Mill）终身做东印度公司的秘书，然而他的业余工作使他在哲学上，经济学上，政治思想史上都占一个很高的位置；斯宾塞（Spencer）是一个测量工程师，然而他的业余工作使他成为前世纪晚期世界思想界的一个重镇。古来成大学问的人，几乎没有一个不是善用他的闲暇时间的。特别在这个组织不健全的中国社会，职业不容易适合我们性情，我们要想生活不苦痛或不堕落，只有多方发展业余的兴趣，使我们的精神有所寄托，使我们的剩余精力有所施展。有了这种心爱的顽艺儿，你就做六个钟头的抹桌子工夫也不会感觉烦闷了，因为你知道，抹了六点钟的桌子之后，你可以回家去做你的化学研究，或画完你的大幅山水，或写你的小说戏曲，或继续你的历史考据，或做你的社会改革事业。你有了这种称心如意的活动，生活就不枯寂了，精神也就不会烦闷了。 第三个方子也只有一句话：“你总得有一点信心。”我们生当这个不幸的时代，眼中所见，耳中所闻，无非是叫我们悲观失望的。特别是在这个年头毕业的你们，眼见自己的国家民族沉沦到这步田地，眼看世界只是强权的世界，望极天边好像看不见一线的光明，——在这个年头不发狂自杀，已算是万幸了，怎么还能够希望保持一点内心的镇定和理想的信心呢？我要对你们说：这时候正是我们培养我们的信心的时候！只要我们有信心，我们还有救。古人说：“信心（Faith）可以移山。”又说：“只要工夫深，生铁磨成绣花针。”你不信吗？当拿破仑的军队征服普鲁士占据柏林的时候，有一位穷教授叫做菲希特（Fichte）的，天天在讲堂上劝他的国人要有信心，要信仰他们的民族是有世界的特殊使命的，是必定要复兴的。菲希特死的时候（1814），谁也不能预料德意志统一帝国何时可以实现。然而不满五十年，新的统一的德意志帝国居然实现了。 一个国家的强弱盛衰，都不是偶然的，都不能逃出因果的铁律的。我们今日所受的苦痛和耻辱，都只是过去种种恶因种下的恶果。我们要收将来的善果，必须努力种现在的新因。一粒一粒的种，必有满仓满屋的收，这是我们今日应该有的信心。 我们要深信：今日的失败，都由于过去的不努力。 我们要深信：今日的努力，必定有将来的大收成。 佛典里有一句话：“福不唐捐。”唐捐就是白白的丢了，我们也应该说：“功不唐捐！”没有一点努力是会白白的丢了的。在我们看不见想不到的时候，在我们看不见想不到的方向，你瞧！你下的种子早已生根发叶开花结果了！ 你不信吗？ 法国被普鲁士打败之后，割了两省地，赔了五十万万佛郎的赔款。这时候有一位刻苦的科学家巴斯德（Pasteur）终日埋头在他的试验室里做他的化学试验和微菌学研究。他是一个最爱国的人，然而他深信只有科学可以救国。他用一生的精力证明了三个科学问题：（1）每一种发酵作用都是由于一种微菌的发展；（2）每一种传染病都是由于一种微菌在生物体中的发展；（3）传染病的微菌，在特殊的培养之下，可以减轻毒力，使它从病菌变成防病的药苗。——这三个问题，在表面上似乎都和救国大事业没有多大的关系。然而从第一个问题的证明，巴斯德定出做醋酿酒的新法，使全国的酒醋业每年减除极大的损失。从第二个问题的证明，巴斯德教全国的蚕丝业怎样选种防病，教全国的畜牧农家怎样防止牛羊瘟疫，又教全世界的医学界怎样注重消毒以减除外科手术的死亡率。从第三个问题的证明，巴斯德发明了牲畜的脾热瘟的疗治药苗，每年替法国农家灭除了二千万佛郎的大损失；又发明了疯狗咬毒的治疗法，救济了无数的生命。所以英国的科学家赫胥黎（Huxley）在皇家学会里称颂巴斯德的功绩道：“法国给了德国五十万万佛郎的赔款，巴斯德先生一个人研究科学的成绩足够还清这一笔赔款了。” 巴斯德对于科学有绝大的信心，所以他在国家蒙奇辱大难的时候，终不肯抛弃他的显微镜与试验室。他绝不想他的显微镜底下能偿还五十万万佛郎的赔款，然而在他看不见想不到的时候，他已收获了科学救国的奇迹了。 朋友们，在你最悲观最失望的时候，那正是你必须鼓起坚强的信心的时候。你要深信：天下没有白费的努力。成功不必在我，而功力必不唐捐。 如今，处于全球范围的疫情下，国家经济受到严重影响，企业缩减经营成本，我们大学学子对未来也同样感到迷茫，现在正值考研的最后冲刺阶段，看着周围忙碌的学长学姐们，我也曾陷入焦虑之中。昨日去书店，我买了这本书，这篇文章也引起我的兴趣，它带给我的，也并不是什么大彻大悟，而是让我的心境能够平静下来的信心。 文中，胡适先生提出两大堕落的方式： 第一是容易抛弃学生时代的求知识的欲望。第二是容易抛弃学生时代的理想的人生的追求。而为了防御这两方面的堕落，也向我们开出三个“方子”： 总得时时寻一两个值得研究的问题！总得多发展一点非职业的兴趣。你总得有一点信心。这三个方子，从学术到生活再到精神，如今看来仍充满智慧，这些道理从不过时。 后来我查阅资料，在两年之后的毕业季，胡适先生又给出了第四个方子：“你得先自己反省：不可专责备别人，更不必责备社会。” 文章附录如下： 两年前的六月底，我在《独立评论》（第七号）上发表了一篇”赠与今年的大学毕业生”，在那篇文字里我曾说，我要根据我个人的经验，赠与三个防身的药方给那些大学毕业生： 第一个方子是：”总得时时寻一个两个值得研究的问题。”一个青年人离开了做学问的环境，若没有一个两个值得解答的疑难问题在脑子里打旋，就很难保持学生时代的追求知识的热心。”可是，如果你有了一个真有趣的问题天天逗你去想他，天天引诱你去解决他，天天对你挑衅笑，你无可奈何他——这时候，你就会同恋爱一个女子发了疯一样，没有书，你自会变卖家私去买书；没有仪器，你自会典押衣服去置办仪器；没有师友，你自会不远千里去寻师访友”没有问题可以研究的人，关在图书馆里也不会用书，锁在试验室里也不会研究。 第二个方子是：”总得多发展一点业余的兴趣。”毕业生寻得的职业未必适合他所学的；或者是他所学的，而未必真是他所心喜的。最好的救济是多发展他的职业以外的正当兴趣和活动。一个人的前程往往全看他怎样用他的闲暇时间。他在业余时间做的事业往往比他的职业还更重要。英国哲人弥儿（J.S.Mill）的职业是东印度公司的秘书，但他的业余工作使他在哲学上，经济学上，政治思想上都有很重要的贡献。乾隆年间杭州魏之琇在一个当铺了做了二十几年的伙计，”昼营所职，至夜篝灯读书”。后来成为一个有名的诗人与画家（有柳州遗稿，岭云集）。 第三个方子是：”总得有一点信心。”我们应该信仰：今日国家民族的失败都由于过去的不努力；我们今日的努力必定有将来的大收成。一粒一粒的种，必有满仓满屋的收。成功不必在我，而功力必然不会白费。 这是我对两年前的大学生说的话，今年又到各大学办毕业的时候了。前两天我在北平参加了两个大学的毕业典礼，我心里要说的话，想来想去，还只是这三句话：要寻问题，要培养业余兴趣，要有信心。 但是，我记得两年前，我发表了那篇文字之后，就有一个大学毕业生写信来说：“胡先生，你错了。我们毕业之后，就失业了！吃饭的问题不能解决，那能谈到研究的问题？职业找不到，那能谈到业余？求了十几年的学，到头来不能糊自己一张嘴，如何能有信心？所以你的三个药方都没有用处！” 对于这样失望的毕业生，我要贡献第四个方子：“你得先自己反省：不可专责备别人，更不必责备社会。”你应该想想：为什么同样一张文凭，别人拿了有效，你拿了就无效呢？还是仅仅因为别人有门路有援助而你没有呢？还是因为别人学到了本事而你没学到呢？为什么同叫做“大学”，他校的文凭有价值，而你的母校的文凭不值钱呢？还是仅仅因为社会只问虚名而不问实际呢？还是因为你的学校本来不够格呢？还是因为你的母校的名誉被你和你的同学闹得毁坏了，所以社会厌恶轻视你的学堂呢？——我们平心观察，不能不说今日中国的社会事业已有逐渐上轨道的趋势，公私机关的用人已渐渐变严格了。凡功课太松，管理太宽，教员不高明，学风不良的学校，每年尽管送出整百的毕业生，他们在社会上休想得着很好的位置。偶然有了位置，他们也不会长久保持的。反过来看那些认真办理而确能给学生一种良好训练的大学——尤其是新兴的清华大学与南开大学——他们的毕业生很少寻不着好的位置的。我知道一两个月之前，几家大银行早就有人来北方物色经济学系的毕业人才了。前天我在清华大学，听说清华今年工科毕业的的四十多人早已全被各种工业预聘去了。现在国内有许多机关的主办人真肯留心选用各大学的人才。两三年前，社会调查所的陶孟和先生对我说：“近年北大的经济系毕业生远不如清华毕业的，所以这两年我们没有用一个北大经济系毕业生。”刚巧那时我在火车上借得两本杂志，读了一篇研究，引起了我的注意；后来我偶然发现那篇文字的作者是一个北大未毕业的经济系学生，我叫他把他做的几篇研究送给陶孟和先生看看。陶先生看了大高兴，叫他去谈，后来那个学生毕业后就在社会调查所工作到如今，总算替他的母校在陶孟和先生的心目中恢复了一点已失的信用。这一件事应该使我们明白社会上已渐渐有了严格的用人标准了；在一个北大老教员主持的学术机关里，若没有一点可靠的成绩，北大的老招牌也不能帮谁寻着工作。 在蔡元培先生主持的中央研究院里，去年我看见傅斯年先生在暑假前几个月就聘定了一个北大国文系将毕业的高材生。今年我又看见他在暑假前几个月就要和清华大学抢一个清华史学系将毕业的高材生。这些事都应该使我们明白，今日的中国社会已不是一张大学文凭就能骗得饭吃的了。拿了文凭而找不着工作的人们，应该要自己反省：社会需要的是人才，是本事，是学问，而我自己究竟是不是人才，有没有本领？从前在学校挑容易的功课，拥护敷衍的教员，打倒严格的教员，旷课，闹考，带夹带，种种躲懒取巧的手段到此全失了作用。躲懒取巧混来的文凭，在这新兴的严格用人的标准之下，原来只是一张废纸。即使这张文凭能够暂时混得一支饭碗，分得几个钟点，终究是靠不住保不牢的，终究要被后起的优秀人才挤掉的。打不破“铁饭碗”不是父兄的势力，不是阔校长的荐书，也不是同学党派的援引，只是真实的学问与训练。能够如此，才是反省。能够如此反省，方才有救援自己的希望。 “毕了业就失业”的人们怎样才可以救援自己呢？没有别的法子，只有格外努力，自己多学一点可靠的本事。二十多岁的青年，若能自己勉力，没有不能长进的。这个社会是最缺乏人才又是需要人才的。一点点的努力往往就有十倍百倍的奖励，一分的成绩往往可以得着十分百分的虚声。社会上的奖掖只有远超过我们所应得的，决没有真正的努力而不能得着社会的承认的。没有工作机会的人，只有格外努力训练自己可以希望得着工作，有工作机会的人而嫌待遇太薄地位太低的人，也只有格外努力工作可以靠成绩来抬高他的地位。只有责已是生路，因为只有自己的努力最靠得住。 在这篇文章里，胡适先生没有留一点情面，直击痛点，引人反思。大学四年里，我同样要自问，我有没有刻苦努力，有没有求学求知的态度，我不想在两年之后，得到一个使自己痛心疾首的答案。 最后，我想再细读一下先生的话，送给自己，也送给屏幕前的你。 朋友们，在你最悲观最失望的时候，那正是你必须鼓起坚强的信心的时候。你要深信：天下没有白费的努力。成功不必在我，而功力必不唐捐。（唐捐：白白地失去） "},{"title":"致emo的Stellarium","date":"2021-12-05T14:16:19.000Z","url":"/2021/12/05/211204/","tags":[["Philosophy","/tags/Philosophy/"]],"categories":[["Philosophy","/categories/Philosophy/"]],"content":"当你失望的时候，可能是因为你太过于盼望而沮丧。 人嘛，经常会有一些疲倦和灰心，在追求让任何事物的过程中，我们也不可避免的会有一些灰心和失望。有些事情呢，会让你感到灰心，会让你感到失望，也许你甚至还会怀疑，自己当初的选择，那我一直是觉得，当你灰心的时候呢，希望能够有一种力量，能够帮你擦去落在心中的灰尘，能够让这种热情，继续在你的心中燃烧。有的时候你对某些事物失望，是因为你对此太看重，所以你才会沮丧。但是看见的不用去相信，看不见的采用去相信。在可见的视野中呢，我们经常看到不公平和不正义，这让我对看不见的公平和正义呢，就更加充满向往。 人总要对焦于某种超越生活的存在，才能告别习以为常的平庸与肤浅。我们可以把失望看成比失望短一截的那个“矢望”（“矢望”一词节选“矢志不渝”的“矢”，“望”则是一种对事物的期盼。），你可以把失望看成矢志不渝的盼望，指向的是盼望。正是因为你对你的使命，生活的这种矢志不渝的盼望，我们可以忍受路途中，一切暂时的这个灰心和失望，因为很多时候灰心会让你怀疑，怀疑无非有两条路，一条是往左，倒向彻底的虚无，在虚无中堕落，放纵，一种可怕的怀疑主义，因为他让你放弃，让你走向彻底的堕落，让你走向彻底的空虚，让你走向彻底的放纵，因为觉得一切没有意义；但是还有一种怀疑是往右走，这种怀疑是让你去修正，是让你更加的确信，每天都有很多很多的怀疑，当你走出怀疑主义的迷雾，你会看到璀璨的明天。所以我们希望每一个人都能够把自己的失望，变成矢志不渝的一种盼望，能够尽快从你的灰心中走出，当作为普通人，我们经常会有灰心和失望，我也时常会感到会感到灰心和失望，人的情绪万千种种，它们不应该被一个emo全部覆盖掉。"},{"title":"遍身罗绮者，不是养蚕人","date":"2021-07-04T01:16:30.000Z","url":"/2021/07/04/0704/","tags":[["Literature","/tags/Literature/"]],"categories":[["Literature","/categories/Literature/"]],"content":"昨日入城市,归来泪满巾。遍身罗绮者，不是养蚕人。——张俞（北宋） 诗句著于北宋时期，后两句最为出名，大概是说身上穿着丝绸的人，不会是养蚕的人。丝绸制成的衣物在当时是非常昂贵的，一般只有达官显贵才可以穿得起，而养蚕的人则是从事底层劳动的贫苦百姓。百姓养一辈子的蚕，但终究不会有一件丝制的衣服可以着在自己的身上，而富人则可能从未见过蚕的样子，却遍身罗绮。如果把养蚕视作一般的劳动过程的话，那么我们可以这样理解：一部分人终身劳作了一辈子，但是劳动的成果自己却未享受；而另外一部分人没有动过一次锄头，却享有了前者所劳得的一切。这里暗含着后者对前者的剥削，以及作者对食利者的鄙视和对劳动者的同情。 作品著于吃人的封建社会，在当时的环境下，整体的社会话语权被士大夫牢牢掌握，所以他们可以制定任何有利于自己的规则，肆意的剥削养蚕人。而普通的劳动者，一是没有知识，连自己的劳动成果被谁剥夺了都不知道。他们想不明白为什么自己蚕丝的收购价格这么低，也不知道为什么朝廷一再喊赈灾，今年粮食价格还是高了这么多。只能似懂非懂的听村口的王老爷说朝廷在跟北辽打战，“皇上要大家紧一紧腰带，好日子在后面呢“。二是没有体制内发声渠道，就算部分幸运的穷人弄懂了自己的收入这么低的原因，他也找不到渠道为养蚕人争取利益。因为封建制度下，一方面，所有的中央政策由大臣们商议，皇帝拍板，要收多少税，要收多少丝绸，什么价格收，养蚕人是没有话语权的。另一方面，政策的执行者（地方官吏）也是由上面选派的，这个丝要怎么收，什么质量合格，养蚕人也只有听令的份。一个群体缺乏知识，缺乏体制内的发声渠道，怎么可能会有机会享受劳动成果呢？他们唯一的发声机会就是聚成一团，然后燃烧成火罢了。 对我国“基建狂魔”称号背后的深思.听我学土木或道桥的同学说 隔三岔五就要被老板派到工地上去，七八月份的时候桥上的温度能到五十度，就这样趴在地上测量，记录数据……回来的时候阳光能晒到的地方全部乌里透红，这时候你要是赞美他为人民做了贡献他肯定要来抽你。仅仅是过去实习的研究生就如此辛苦，一线工人的劳苦可想而知。而一些危险性高的项目，死个把人都是再正常不过的事，大概上面还会定一个指标，多少个之内安全性是达标的。 评：我们称赞农村田园牧歌，种豆南山式的生活，但真正土里刨食的农民们何尝希望自己的儿女像自己一样辛劳，面朝黄土背朝天一年也只刨出两三万的收入。 我们称赞中国是基建狂魔，但谁又注意到中午那些在地铁口纳凉，浑身是泥的民工。又或者他们带着大包小包上公交时，周围的“上等人们”投去的嫌弃的目光，连他们坐过的位置也不愿意做？ 慨他人以慷是容易的，事实上大部分人都很难体面地生活。王小波曾经在一篇杂文里戏谑地写过：痛苦是艺术的源泉，但不必是艺术家的痛苦。柴可夫斯基自己不是小伊万；唱黄土高坡的都打扮的珠光宝气；演秋菊的卸了妆一点也不悲惨。。。种种事实都说明，别人的痛苦才是你艺术的源泉，而你去受苦，只会成为别人艺术的源泉。剪几个镜头，配上激昂的音乐与“基建狂魔”的标题，这样的自豪感是廉价的，这样的赞美也没有什么意义。真正有意义的，是从事基建的工人能自豪地说出这是我们造的桥，这是我们修的路。是让农民说出，这是我们种的粮。是让建设国家的人感觉到自己是国家的主人，而不是靠出卖劳动力，养家糊口的底层人。 关于《商君书》简述《商君书》也称《商子》，现存26篇 ，是战国时期法家学派的代表作之一，是中国家喻户晓的人物商鞅及其后学的著作汇编。《商君书》着重论述商鞅一派在当时秦国施行的变法理论和具体措施。《商君书》论述了“强国弱民”。例如：《商君书》中有“民弱国强，国强民弱。故有道之国务在弱民。朴则强，淫则弱。弱则轨，淫则越志。弱则有用，越志则强。故曰：以强去强者，弱；以弱去强者，强。”能够战胜强敌、称霸天下的国家，必须控制本国的人民，使之成为“弱民”，而不是“淫民”。商鞅认为国家的强势和人民的强势是对立的。只有使人民顺从法律、朴实忠厚，人民才不易结成强大的力量来对抗国家和君主，这样国家才会容易治理，君主的地位才会牢固。但是，不要把“弱民”理解成“愚民”。《商君书》中认为人性本恶，必须承认人之恶性，治理国家要以恶治善才能使国家强大。《商君书》中主张重刑轻赏，他认为加重刑罚，减少奖赏，是君主爱护民众，民众就会拼命争夺奖赏；增加奖赏，减轻刑罚，是君主不爱护民众，民众就不会为奖赏而拼死奋斗。"},{"title":"何以为家？","date":"2021-06-17T02:41:46.000Z","url":"/2021/06/17/family/","tags":[["Emotion","/tags/Emotion/"]],"categories":[["undefined",""]],"content":"什么是家？对我而言，家的组成部分是人，人是构成家的主体，爱是家永恒的主体。有我，我爱和爱我的女孩，我们的至亲，这便是一个家。 这样的组成，无论我们在什么地方，农村也好，城市也罢；中国也好，美国也罢；我们都是一个家。家对我而言，不是一栋多大的房子，有多少家产与家业，而是简简单单的人和其中联络的情感。或许居无定所，也或许衣食无忧，家是那个你劳累疲惫的时候，能充分给你依靠的地方，有你充分信任的人，永远会信任和爱你的人。同样，家也是一个我愿意为之付出，为之努力，愿意为它付出一切哪怕是生命的地方。这里有我爱的人和爱我的人。 此心安处是吾家（乡）。———苏轼《定风波·南海归赠王定国侍人寓娘》 有时我也会想，人存在的意义是什么，茫茫人海，广阔世间，我们究竟在追求些什么？或许，家便是一个可以让我活下去的存在，一个让我愿为之奋斗的存在吧。 家是渴了有水喝，饿了有饭吃，冷了有衣穿。家是桌上摆着我喜欢的菜，对面坐着我爱的人。家是不管如何也会满心欢喜的期待着要回去的地方。家是柴米油盐酱醋茶，家也是琴棋书画诗酒花。 家是一个能让你摘下面具的地方。在学校，在社会，我们承担着众多的角色，或为一个学生，一个职员，一个钢琴家，一个工程师~~在外人看来，我们平易近人，乖巧和善，慷慨大方；而在家里，我就是我，我和我爱的人互相了解，互相信任，不需要虚伪的面具，家包容了你我的缺点，也包含了甜蜜的爱。 那里寄托了你对生活的希望，和努力的勇气，这就叫做家。 "},{"title":"孙少平与孙少安 ——《平凡的世界》","date":"2021-06-15T14:55:40.000Z","url":"/2021/06/15/615/","tags":[["Literature","/tags/Literature/"]],"categories":[["Literature","/categories/Literature/"]],"content":"前言我们每个人无法选择自己的出身，也无法选择与之相对应的责任使命。 《中国现当代文学经典赏读》这门课结课了，我提交的论文是关于《平凡的世界》的，于是就像在这里记录一下当晚的所思所想所感与所悟。本篇文章主要就少安与少平两人选择的不同道路进行探讨。 就脚踏实地而言，显然少安是务实的典型代表。他不甘心命运给他的安排，想方设法地去改变这一切，冒着批判的风险去私分猪饲料地，然后被戴上“走资本主义道路”的帽子；不顾家人反对，毅然开设砖厂，带着村里人发家致富；当他自己的腰包鼓起来后，并没有希图“独乐乐”，心中的目标依然是让全家人住进新房，否则宁愿住牲口棚。 就仰望星空而言，显然少平适合那个变革者的角色。少平性格的明确性在于他对理想生活和理想自我的执著追求，理想主义的他最终也逝于时代的洪流。 讨论：何以为家？More info: 另一篇博客 后记但我们可以去尝试改变，脚踏实地又不忘仰望星空。少平少安或许是悲壮的，但我也拼尽全力还会去尝试一下如果我最终平凡，我就是少安少平的其中一个；如果我不凡，那是因为有千千万万失败的少平少安的衬托。 "},{"title":"”凡是过往，皆为序章“","date":"2021-06-02T06:21:08.000Z","url":"/2021/06/02/602/","tags":[["Emotion","/tags/Emotion/"]],"categories":[["undefined",""]],"content":"原文出自《暴风雨》（莎士比亚创作戏剧） 该剧讲述米兰公爵普洛斯彼罗被弟弟安东尼奥篡夺了爵位，只身携带襁褓中的独生女米兰达逃到一个荒岛，并依靠魔法成了岛的主人。后来，他制造了一场暴风雨，把经过附近的那不勒斯国王和王子斐迪南及陪同的安东尼奥等人的船只弄到荒岛，又用魔法促成了王子与米兰达的婚姻。结局是普洛斯彼罗恢复了爵位，宽恕了敌人，返回家园。 这句话原文出处如下：She that is Queen of Tunis;she that dwellsTen leagues beyond man’s life;she that from NaplesCan have no note,unless the sun were post,—The Man i’ th’ Moon’s too slow,—till newborn chinsBe rough and razorable;she that from whomWe all were sea-swallow’d, though some cast again,And by that destiny,to perform an actWhereof what’s past is prologue, what to comeIn yours and my discharge.——ACTII SCENE I Another Part of the Island 我将“凡是过往，皆为序章”这句话作为我的博客副标题，它预示着凡是未来，皆有可期我很喜欢简书上的一段话：“每个人的时区都不一样纽约比加州早三小时，但它并没有使加州变慢。 有人22岁毕业， 但直到5年后才找到一份好工作。有人25岁成为首席执行官，但50岁就去世了；有人50岁时成为首席执行官，一直活到90岁。有些人单身，别人却成家了。奥巴马55岁退休，特朗普70岁才当总统。这个世界上的人，都以自己的时区工作。环顾左右，有些人比你早，有些人比你晚。每个人都按自己的轨迹奔跑，有自己的时间。不要嫉妒，嘲笑他们。他们有他们的时区，你有你的。生活就是等待合适的机会行动。所以，放松：你既不晚，也不早，你正当其时。”"},{"title":"Romantic","date":"2021-05-23T13:18:47.000Z","url":"/2021/05/23/523/","tags":[["Emotion","/tags/Emotion/"]],"categories":[["undefined",""]],"content":"爱情是一种高尚的情感，爱情意味着欣赏与尊重，更需要责任和能力。———-七下政治 什么是爱情？罗翔老师所说：真正的爱是一种宽容之爱。很多人寻找爱情，在不同的人中周旋探索，最终感觉自己越来越孤独。任何一个人都无法达到你对她的全部预设。一个可爱的人人人都会去爱她，但她不再像以前这么可爱，你还愿去爱她，并且去发掘她的可爱之处，那这可能就是真正的爱了。英语中有两个单词表达爱，一个是love，而另一个是charity，love是一种情感的爱，而真正的爱，也许是charity。charity意味着你对他是一种宽容之爱，也许这听起来太难了。可我们每天都在这样爱着一个人，你也许会恨他犯下许多错误，但你依然爱着这个人，而我相信你还会爱他一辈子，这个人就是我们自己。不管我们多么讨厌自己的自负，贪婪，胆怯，虚伪，伪善，我其实依然这样爱着我自己。事实上，我恨自己的那些缺点，正是因为我爱这个人。正因为我爱我自己，所以我才为自己做出那些事情感到难过。爱人如己。 哥林多前书第十三章第四节爱是恒久忍耐，又有恩慈；爱是不嫉妒；爱是不自夸；爱是不狂妄，不做害羞的事，不求自己的益处，不轻易发怒，不计算他人的恶，不喜欢不义，只喜欢真理；凡事包容，凡事盼望，凡事忍耐；爱是永不止息。 尾声枕上诗书闲处好，门前风景雨来佳。相识相知相遇相爱。你是冰镇后的可乐，入暮时的微风，漫天的星河。幸天光乍破遇，愿慕雪到白头。 "},{"title":"寂静之城","date":"2021-05-22T13:32:35.000Z","url":"/2021/05/22/522/","tags":[["Anecdote","/tags/Anecdote/"]],"categories":[["Literature","/categories/Literature/"]],"content":"马伯庸《寂静之城》未删节版本全文 发表在 2005 年第 5 期《科幻世界》上的本文由于众所周知的原因进行了删节，这是发在马伯庸 个人网站上的原文。愿终有一日，我们能在光明下相见。 And in the naked light I saw ten thousand people, maybe more.People talking without speaking, people hearing without listening.People writing songs that voices never shared, no one dared disturb the sound of silence. — The sound of silence 美利坚合众国，2015年，纽约。 当电话响起来的时候，阿瓦登正趴在电脑前面睡觉。电话铃声十分急促，尖锐，每一次振动都让他的耳膜难受好久。他揉了揉干涩的眼睛，十分不情愿地爬起来，觉得脑子沉滞无比。 其实他的脑子一直就很沉滞，这种感受既然是生理上的也是心理上的。他身处的房间很狭窄，空气不很好，唯一的两扇窗户紧闭着——即使打开窗户也没用，外面的空气更加浑浊。这是一间大约只有三十平米的小屋子，屋子墙壁上泛黄的墙纸有好几处开始剥落，天花板上的水渍渗成奇怪的形状；一张老式的军绿色行军床摆在墙角，床腿用白漆写着编号；紧挨着行军床的是一张三合板制成的电脑桌，桌上摆着一台浅白色的电脑，机箱后面五颜六色的电线纠缠在一起，把它们自己打成一个古怪的死结，杂乱无章地蔓延到地板与墙角，仿佛常春藤一样。 阿瓦登走到电话前，慢慢坐到地板上，目光呆滞地盯着电话，手却没有动。这部古怪的东西是老式的按键式电话，大概是十几年前的款式，这是阿瓦登有一次去费城出差时偶尔在一家杂货店里买到的；他拿回家以后稍微修理了一下，发现居然还能用，这让他当时小小地兴奋了一阵子。 电话继续在响着，已经是第七声。阿瓦登意识到自己不得不去接听了。于是他弓下腰，用两个指头拈起电话，慢慢把电话放到耳边。 “请说出你的网络编号？”话筒里传来的声音并不急噪，事实上它也不带其他任何的感情色彩，因为这是电脑合成的人工智能语音系统。“19842015”阿瓦登熟练地报出一连串数字，同时开始觉得胸有些更闷了。说实话他并不喜欢这些空洞的电子声音。 他有时候想，假如打过电话来的是一位声音圆润的女性该多好。阿瓦登知道这是一种不切实际的幻想，不过这幻想会让他的身体得到几秒钟的舒缓。话筒里的声音仍旧在继续着。“关于你在十月四日提交的网络论坛用户注册申请已经被受理，经有关部门审查后确认资格无误，请在三日内持本人身份证件、网络使用许可证及相关文件前往办理登记手续，并领取用户名及密码。”“知道了，谢谢。”阿瓦登谨慎地选择词语，同时努力挤出一副满足的微笑，好象话筒的另一侧有人在看着自己一样。放下电话，阿瓦登先是茫然地盯着它看了大约两分钟，然后站起来活动一下手腕，坐回到电脑前面，缓慢地推动了一下鼠标。电脑屏幕“啪”地一声亮了起来，显示出一个登陆的界面，还有一行英文：“请输入你的网络编号和姓名。”阿瓦登将那八位数字敲进去，又输入了自己的名字，点击“登陆”。随即机箱的指示灯开始频繁地闪动起来，整个机器发出细微的噪音。 每一个使用互联网的人都有一个网络编号，没有这个编号，就无法连接进互联网络。每一个编号都是独一无二的，每个人只有一个；这是使用者在网上的唯一代号，既不能修改，也不能取消。这些编号分别对应着使用者身份证上的名字，因此 19842015 就是阿瓦登，阿瓦登就是 19842015。阿瓦登知道有些记忆力不好的人会把自己的编号印在衣服的后面，那看起来颇为滑稽，也容易引发一些不正当的联想。 有关部门说使用网络实名制是为了规范网络秩序方便管理，杜绝因匿名使用网络而产生的一系列重大问题和混乱。阿瓦登不太清楚那一系列重大问题会是什么，他自己没试过用假名上网，他所认识的任何人里也不曾有人尝试过——事实上，从技术角度来说，他根本没办法匿名登陆互联网络，没有编号就没有权限上网，而编号则连接着他的详细档案，换句话说，没人能在网上隐藏自己。有关部门把这一切都考虑的很周详。“有关部门”，这是一个语意模糊、但却有着权威与震慑力的词组。它既是泛指，又是确指，其所涵盖的意义相当广泛。有时候，它指的是为阿瓦登颁发网络编号的美国联邦网络管理委员会；有时候它是将最新通告及法规发到阿瓦登 EMAIL 信箱的服务器；还有时候它是监察网络的 FBI 特属网络调查科；总之一句话，有关部门是无处不在，无职不司的，总会在适当的时候出现，给予指导、监控或者警告，无论你是在网上还是网下。简直就象是老大哥一样无微不至。 电脑仍旧在持续运转着，阿瓦登知道这得花上一阵子。这台电脑是有关部门配发给他的，具体型号和配置阿瓦登并不清楚，机箱是被焊死的，无法打开。于是他拿出一小瓶清凉油，用右手小拇指的指甲挑出一点抹在自己的太阳穴，然后从脚下堆积如山的杂物里翻出一个塑料杯子，从桌子旁的饮水机里接了半杯蒸馏水，就着一片镇痛片一饮而尽。蒸馏水穿过喉咙和狭长的食道滑进胃里，空泛的味道让他有些恶心。 音响里忽然传来一阵美国国歌的旋律，阿瓦登放下杯子，重新把目光投到电脑上去。这是已经连入互联网络的标志。屏幕上首先跳出来的是有关部门的通告，白底黑色四号字，里面陈述了使用互联网的意义以及最新的规章制度。“缔造健康的互联网络，美国万岁！”音响里传来激昂的男性呼声，阿瓦登不大情愿地跟着大声念了一遍。“缔造健康的互联网络，美国万岁！” 这段呼号持续了三十秒钟，然后消失，取而代之的是写着“缔造健康的互联网络”标语的桌面背景。另外一个窗口慢慢浮上开，上面开列出几个选项：工作、娱乐、电子信箱和 BBS 论坛。其中 BBS 选项呈现灰色，说明这项功能还没有开通。整个操作系统简洁明了，这台电脑的浏览器没有地址输入栏，只是在收藏夹里有几个无法修改的的网站地址。理由很简单，这些网站都是健康向上的，假如其他站点和这些网站一样，那么只保留这些网站就够了；假如其他站点与这些网站不一样，那么就是不健康的，是低级趣味，不能保留。这是有关部门精心设计的，是为了公民的精神健康着想，生怕他们受到不良信息的侵染。 阿瓦登首先点开了“工作”，一连串和他工作相关的站点列表与相关软件在电脑上显示出来。阿瓦登是一名程序员，他每天的工作就是根据上级的要求编写程序。这份工作很无聊，不过可以保证他有稳定的收入。他不知道自己的源代码会被用到哪里去，上级也从来没有跟他说过。他打算继续昨天的工作，但是很快发现自己很难继续下去。阿瓦登觉得今天的情绪比以前要烦躁，无法集中精神，大脑还是很呆滞，胸口仍旧发闷。他试图娱乐自己，但是他发现“娱乐”选项里只有纸牌与挖地雷，根据有关部门的说法，这是两个健康的游戏，没有暴力，没有色情，不会让人产生犯罪冲动，也不涉及任何政治色彩。据说美国境外也是有互联网络站点的，不过无法连上去，因为本国的互联网络自成格局，独立自主，普通人无法直接连接到国外——IE 浏览器没有地址栏，就算知道地址也没有用处。 “您有一封新邮件。”系统忽然跳出来提示，阿瓦登终于找到了可以暂停工作的理由，他很快移动鼠标到电子信箱的选项上，点开，很快一个新的界面出现了。 “To: 19842015From：10045687Subject: 模块、已经、完成、当前、项目、是否、开始。” 阿瓦登微微地叹了一口气，觉得有些失望。每一次他收到新的电子邮件，都希望能够有一次新鲜的刺激来撞击他日益迟钝的脑神经，每一次他都失望了。其实他早就知道这一点，只不过他觉得保持期待至少能够享受到几秒钟快感。就好象他期待着打电话过来的是一个圆润温柔的女性声音一样。不给自己一些渺茫的希望，阿瓦登觉得自己迟早会疯掉的。 这封信很简短，但是内容很充实。19842015 是阿瓦登的网络编号，而 10045687 则是他的一位同事的编号，这种工作性质的信件通常都以编号相称。信的内容是几个不连续的英文单词，这是有关部门所提倡的一种电子邮件书写方式，因为这样可以方便软件检查信件中是否含有敏感词汇。阿瓦登打开回信的页面，同时另开了一个窗口，打开一份名字叫做“网络健康语言词汇列表”的 TXT 文档。这是有关部门要求每一位网民所必须使用的词汇。当他们书写电子邮件或者使用论坛服务的时候，都得从这个词汇列表中寻找适合的名词、形容词、副词或者动词来表达自己想要说的话。一旦过滤软件发现网民使用了列表以外的词，那么这个词就会被自动屏蔽，取而代之的是“请使用健康语言”。“屏蔽”是个专有名词，被屏蔽的词将不允许再度被使用，无论是在书信里还是口头都不允许。讽刺的是，“屏蔽”一词本身也是被屏蔽的词汇之一。这个列表是经常更新的，每一次更新都会有几个词在列表上消失，于是阿瓦登不得不费劲脑汁寻找其他词语来代替那个被屏蔽掉的词语或者单字。比如在以前，“运动”这个词是可以使用的，但后来有关部门宣布这也是一个敏感词汇，阿瓦登只好使用“质点位移”来表达相同的意思。 他对照着这份列表，很快就完成了一封文字风格与来信差不多的 EMAIL——健康词汇表迫使人们不得不用最短的话来表达最多意思，而且要尽量减少不必要的修辞，所以这些信件就好象是那杯蒸馏水一样，淡而无味，阿瓦登有时候想，他早晚也会和这些水和信一样腐烂，因为这些信是他写的，水是他喝的。接下来阿瓦登启动检查软件先扫了一遍，确保自己没无意中加入什么敏感词汇。等这一切都完成后，他按下了发送键，邮件被送出去了。阿瓦登没有留下备份，因为他的机器里没有硬盘，也没有软驱、光驱或者USB接口。这个时代宽带技术已经得到了很大发展，应用软件可以集中在统一的一个服务器中，个人用户调用时的速度丝毫不会觉得迟滞。因此个人不需要硬盘，也不需要本地存储，他们在自己电脑里写的每一份文档、每一段程序、甚至每一个动作都会被自动传送到有关部门的公共服务器中，这样便于管理。换句话说，阿瓦登所使用的电脑，仅仅具备输入和输出两种功能。完成了这封信后，阿瓦登再度陷入了软绵绵的焦躁状态，这是一个连续工作了三天的程序员的正常反应。这种情绪很危险，因为它让人效率低下精神低迷，而且没有渠道发泄。“疲劳”、“烦躁”以及其他负面词汇都属于危险词汇，如果他写信给别人抱怨的话，那么对方收到的将会是一封写满“请使用健康语言”的 EMAIL。 这就是阿瓦登每天的生活，今天比昨天更糟糕，但应该比明天还稍微好一点。事实上这个叙述也很模糊，因为阿瓦登自己并不清楚什么是“好一点”，什么是“更糟糕”。“好”与“坏”是两个变量，而他的生活就是一个定量，只有一个常数叫“压抑”。阿瓦登推开鼠标，把脑袋向后仰去，长长地呼了一口气。（至少“呼”这个字还没有被屏蔽）这是空虚的表现，他想哼些歌，但却又不记得什么，转而吹了几下口哨，但那听起来与一只生了肺结核的狗差不多，只得做罢。有关部门象幽灵一样充斥在整个房间里，让他无法舒展自己的烦闷。就好象一个人在泥沼里挣扎，刚一张口就被灌入泥水，甚至无法大声呼救。他的头不安分地转了几转，眼神偶尔撇到了摆在地板上的老式电话机，他忽然想到还必须要去有关部门申请自己的 BBS 论坛浏览许可证。于是他关掉“工作”和“电子邮件”窗口，退出了网络登陆。阿瓦登在做这些事的时候毫不犹豫，他很高兴能够暂时摆脱互联网络，在那上面他只是一串枯燥的数字和一些“健康词汇”的综合体。阿瓦登找出一件破旧的黑色呢子大衣，那件大衣继承自他的父亲，袖口和领子已经磨损的很严重，个别地方有灰色的棉花露出来，但还是很耐寒。他把大衣套到身上，戴上一副墨绿色的护镜，用过滤口罩捂住嘴。他犹豫了一下，拿起“旁听者”别在耳朵上，然后走出家门去。 纽约的街上人很少，在这个时代，互联网的普及率相当地高，大部分事务在网上就可以解决，有关部门并不提倡太多的户外活动。太多的户外活动会导致和其他人发生物理接触，而两个人发生物理接触后会发生什么事则很难控制。“旁听者”就是为了防止这种事而发生的，这是一种便携式的语言过滤器，当携带者说出敏感词汇的时候，它就会自动发出警报。每一位公民外出前都必须要携带这个装置，以便随时检讨自己的言语。当人们意识到旁听者存在的时候，他们往往会选择沉默，至少阿瓦登是如此。有关部门正逐步试图让网络和现实生活统一起来，一起“健康”。这时候正是11月份，寒风凛冽，天空漂浮着令人压抑的铅灰阴云，街道两旁的电线杆仿佛落光了叶子的枯树，行人们都把自己包裹在黑色或灰色的大衣里面，浓缩成空旷街道上的一个个黑点飞快移动着。一层若有若无的烟雾将整个纽约笼罩起来，不用过滤口罩在这样的空气里呼吸将会是一件很有挑战的事情。距离上一次离开家门已经有两个月了吧，阿瓦登站在公共汽车站的站牌下，不无感慨地想，周围的一切看起来很陌生，泛黄，而且干燥。那是上一次沙尘暴的痕迹。不过沙尘暴这个词也已经被屏蔽了，因此阿瓦登的脑海里只是闪过那么一下，思想很快就转移到别的事情上去了。 站在阿瓦登旁边的是一个穿着蓝色制服的高个男人。他先是狐疑地看了阿瓦登一眼，看到后者沉默地沉在黑色大衣里，他的两只脚交替移动，缓慢地凑了过去，装做漫不经心对阿瓦登说：“烟，有吗？”男人说，每一个字都说的很清晰，而且词与词之间间隔也足够长。这“旁听者”还没有精密到能够完全捕捉到每一个人语速和语调的程度，因此有关部门要求每一位公民都要保持这种说话风格，以方面检测发言人是否使用了规定以外的词汇。阿瓦登转过头去，看了他一眼，舔舔自己干裂的嘴唇，回答说：“没有。”男人很失望，又一次不甘心地张开嘴。“酒，有吗？”“没有。”阿瓦登又重复了一次这个词，他也已经很久没有收到烟和酒了，也许是缺货的关系吧，这是常有的事。不过有一点很奇怪，“旁观者”这一次却没有发出警报。以阿瓦登的经验，以往一旦烟、酒或者其他生活必需品发生短缺现象，这个词就会暂时成为被屏蔽掉的敏感词汇，直到恢复供给为止。这个男人看起来很疲惫，红肿的眼睛是这个时代的人们普遍的特征，这是长时间挂在网上的关系。他的头发蓬乱，嘴边还留着青色的胡子碴，制服下的衬衣领口散发着刺鼻的霉味。能看的出，他也很久不曾到街上来了。 阿瓦登这时候才注意到，他的耳朵上空荡荡的，没有挂着那个银灰色的小玩意“旁听者”，这实在是一件严重的事情。不携带“旁听者”外出，就意味着语言不会再被过滤，一些不健康的思想和言论就有可能孳生，因此有关部门相当严厉地规定公民上街必须携带旁听者。而这个男人的耳朵旁却什么也没有。阿瓦登暗暗吃惊，一时间不知道是该去提醒还是装做没看到。他暗自想，也许向有关部门举报会更好。这时候那个男人又朝他靠近了一点，眼神变的饥渴起来。阿瓦登心里一阵紧张，下意识地向后退去。这难道是一次抢劫？还是说他是个压抑太久的同性恋者？那个男人忽然扯住他的袖子，阿瓦登狼狈地挣扎却没有挣开。出乎他的意料，那个男人并没有进一步的动作，而是大吼一声，用一种阿瓦登已经不太习惯了的飞快语速向他倾泻起话语来。阿瓦登被这突如其来的爆发弄的莫名其妙，不知所措。 “我只是想和你多几句话，就几句，我很久没有说过话了。我叫斯多葛，今年三十二岁，记得，是三十二岁。我一直梦想有一套在湖边的房子，有一副钓鱼竿和一条小艇；我讨厌网络，打倒网管；我妻子是个可恶的网络中毒者，她只会用枯燥乏味的话叫我的网络编号；这个城市就是一个大疯人院，里面大疯子管着小疯子，并且把所有没疯的人变的和他们同样疯狂；敏感词汇都去他X的，老*受够了……”男人的话仿佛一瓶摇晃了很久然后突然打开的罐装碳酸饮料，迅猛，爆裂，而且全无条理。阿瓦登惊愕地望着这个突然狂躁起来的家伙，却不知道如何应对；更可怕的是，他发现自己居然对他产生了一点同情，那种“同病相怜”式的同情。男人的话这时候已经从唠叨变成了纯粹谩骂，全部都是最直抒胸臆的那种。阿瓦登已经有五、六年不曾说过这些脏话，最后一次听到这些也是四年前。有关部门认为这都有碍精神文明，于是全部都屏蔽掉了。而现在这个男人就在公众场合对着他大吵大嚷，似乎要将被屏蔽掉的敏感词汇一口气全倒出来。他的目光和手势并不针对任何人，甚至也不针对阿瓦登，更象是在一个人在自说自话。阿瓦登的耳膜似乎不习惯这种分贝，开始有些隐隐做痛，他捂着耳朵，拿不定主意是干脆逃掉还是……这时候，远处街道出现两辆警车，一路闪着警灯直直冲着这座公共汽车站而来。警车开到站台旁时，男人仍旧在痛骂着。警车门开了，涌出了五、六名全副武装的联邦警察。他们扑过去将那个男子按在地上，用橡皮棍痛打。男人两条腿挣扎着，嘴里的语速更快了，骂出来的话也越来越难听。其中一名警察掏出一卷胶带，“嚓”地一声扯下一条向男人的嘴贴去。男人在嘴被胶带封住之前，突然提高嗓门，冲着警察痛快无比地喊了一句：“FUXKYOU, YOU SONOFBITCH！”阿瓦登看到他的表情由疯狂变成享受，面带着微笑，似乎完全陶醉在那一句话所带来的无上快感和解脱感中。 联邦警察们七手八脚地将男人送进了警车，这时才有一名警察走到了阿瓦登的跟前。“他，是，你朋友？”“我，不，认识。”警察盯了他一阵，取下他耳朵上的“旁观者”查看记录，发现他并没有提及任何敏感词汇，于是重新给他戴回去，警告他说那名男子说的全部都是极度反动的词汇，要求他立刻忘掉，然后转身押着那男子离开了。阿瓦登松了一口气，其实刚才他有一瞬间涌现出一种冲动，也想在这空旷的街道上大喊一声“FUXKYOU, YOUSONOFBITCH”那一定很爽快，他心里想，因为那男子说出这句话的时候表情很享受。不过他也知道，这也是妄想的一种，“旁观者”紧帖在耳朵上的冰凉感觉时刻提醒着他。 街上很快就恢复了冷清，十分钟后，一辆公共汽车慢吞吞地开进站里，锈迹斑斑的车门哗啦一声打开，一个电子女声响彻整个空荡荡的车厢：“请乘客注意文明用语，严格按照健康词汇发言。”阿瓦登把自己缩进大衣，压抑住自己异样的兴奋，决定继续保持沉默下去。大约过了一个小时，公共汽车到了目的地。从破碎的车窗玻璃里吹进来的寒风让阿瓦登脸上挂起一层暗灰色的霜气，面部被风中的沙砾和煤渣刮的生疼。他听到电子女声报出了站名，就站起身来，象一条狗一样抖抖身上的土，走下车去。 车站对面就是阿瓦登要去的地方，那是有关部门负责受理BBS论坛申请的网络部。这是一间五层的大楼，正方形，全水泥混凝土结构，外表泛灰。如果没有那几个窗户的话，那么它的外貌将与水泥块没有任何区别：生硬、死气沉沉，让蚊子和蝙蝠都退避三舍。BBS 论坛是一种奇特的东西，从理论上来讲它完全多余，BBS 的功能完全可以由 EMAIL 新闻组来取代，后者更容易管理和审查。而且申请使用 BBS 论坛资格不是件容易的事，申请人必须要通过十几道手续和漫长的审查才能有浏览资格，浏览资格三个月才会被允许在指定论坛发布帖子，至于自己开设 BBS 则几乎是不可能。因此真正对 BBS 有兴趣的人少之有少。阿瓦登当初之所以决定申请 BBS 论坛资格，纯粹是因为他那种模糊但却顽强的怀旧心态，就好象他从杂货店里买的那部老式电话一样。他也不知道自己为什么会自找麻烦，也许是为了给生活带来些刺激，还是说为了强调自己和曾经旧时代的那么一点点联系，也许两者兼有之。阿瓦登恍惚记得在他小的时候，互联网与现在并不太一样。并不是指技术上的不同，而是一种人文的感觉。他希望能通过使用 BBS 论坛回想起一些当年的事情。 阿瓦登走进网络部的大楼，大楼里和外面一样寒冷，而且阴森。走廊里没有路灯，蓝白色调的两侧墙壁贴满了千篇一律的网络规章条文与标语，冰冷的空气呼吸到肺里，让阿瓦登一阵痉挛。只有走廊尽头的小门缝隙里流泻出一丝光亮，小门的上面挂着一块牌子，上面写的是“网络部 BBS 论坛科。”一走进这间屋子，阿瓦登立刻感觉到一阵温洋洋的热气。屋子里的暖气（或者是空调）开的很大，让阿瓦登冻麻了的手脚和脸麻酥酥的，有些发痒，他不禁想伸出手去挠挠。 “公民，请您站在原地不要动。”一个电子女声忽然从天花板上的喇叭里传来，阿瓦登触电似地把手放下，恭敬地站在原地不同。他借这个机会观察了一下这间屋子。这屋子准确来说应该是一个狭长形的大厅，一道拔地而起的大理石柜台象长城一样将房间割裂成两部分，柜台上还装着一排银白色的圆柱形栅栏，直接连到天花板。屋子里没有任何装饰，没有观赏植物，没有塑料鲜花，甚至没有长椅和饮水机。“缔造健康的互联网络，美国万岁。”阿瓦登跟着声音重复了一遍。“请前往八号窗口。”电子女声的语调很流畅，因为这是电脑制作出来的，因此没有敏感词汇的限制。 阿瓦登转头看到在自己右手边的不远处，大理石柜台上的液晶屏幕显示着八号的字样。他走过去，拼命抬起头，因为柜台实在太高了，他只能勉强看到边缘，而无法看到柜台另一侧的情形。不过他能听到，一个人走到柜台对面，坐下去，并有翻动纸张与敲击键盘的声音。“请把文件放入盒子里。”柜台上的喇叭传来命令。出乎意料，这一次在喇叭里的声音却变了。虽然同样冷漠枯燥，但阿瓦登还是能分辨出它与电子女声的不同——这是一个真正的女性的声音。他惊讶地抬头望去，却什么都看不到，柜台太高了。“请把文件放入盒子里。”声音又重复了一次，语气里带着一丝烦躁，似乎对阿瓦登的迟钝很不满。“是的，这是真正的女声……”阿瓦登想，电子女声永远是彬彬有礼不带任何感情色彩的。他把相关的电子身份证、网络许可证、网络编号和敏感词汇犯罪记录等一系列个人资料卡片一起放进柜台外的一个小金属盒子里，然后把盒子插进柜台上一个同样大小的凹槽中，关好门。很快他听到“唰”的一声，他猜测这也许是对面的人——也许是个女人——将盒子抽出去的声音。 “你申请BBS服务的目的是什么？”喇叭后的女声浸满了纯粹事务性的腔调。“为了、提高、互联网络、工作效率、为了、缔造、一个、健康、的网络、环境，更好地、为、祖国、做出、贡献。”阿瓦登一字一句地回答，心里知道这只是一道官方程序，只需要按标准回答就可以。对面很快就陷入沉默，大约过了十五分钟，喇叭再度响起。“最后手续确认，你已经获得BBS论坛浏览权。”“谢谢。”“砰”的一声，金属盒子从柜子里弹了出来，里面除了阿瓦登的证件以外还多了五张小尺寸光盘。“这是有关部门核发给你的 BBS 论坛统一用户名与密码，BBS 论坛列表、互联网BBS论坛使用指南及相应法规、以及最新健康网络词汇列表。”阿瓦登向前踏了一步，从盒子里将这些东西一股脑全拿出来，揣进大衣的大兜里。那些东西其实是可以全部放在同一张光盘里的，不过有关部门认为每一张光盘装一份文件有助于用户理解这些文件的严肃性和重要性，并产生敬畏。他心里盼望着那个喇叭能再说两句。让他失望的是，对面传来的是一个人起身并且离开的声音，从脚步声的韵律判断，阿瓦登愈发相信这是一名女性。“手续办理完毕，请离开网络部回到自己的工作岗位上。” 甜美空洞的电子女声从天花板上传来，阿瓦登厌恶地抽动鼻翼，拿手揉了揉，转身离开这间温暖的大厅，重新进入到寒冷的走廊。在回家的路上，阿瓦登蜷缩在公共汽车上一动不动，顺利申请到 BBS 的使用权让他有些虚无缥缈的兴奋。他闭着眼睛，找了一个合适的角度躲开破窗而入的寒风，右手在兜里不断摩挲那一系列光盘，还在怀念着那一个神秘的女声。如果能再一次听到该多好，他不能抑制自己这样的想法，同时用拇指的指肚在光盘上轻轻地摩擦，幻想这几张光盘也曾经被她的手触摸过。他兴奋的几乎也想破口大骂一句“FUXKYOU, YOUSONOFBITCH”，真奇怪，那名男子的骂声在他的记忆里根深蒂固，并时不时不自觉地滑到唇边。忽然，他的手指在光盘上发觉到一丝异常的感觉。阿瓦登下意识地朝四周望去，确认周围一个乘客也没有后，他小心翼翼地把光盘全拿出来，就着窗外的光亮仔细端详。阿瓦登很快注意到，在装有 BBS 论坛列表的光盘背面，被人用指甲轻轻地划了一道刮痕。这条刮痕很轻，如果不是阿瓦登仔细地抚摩光盘的话，是很难发觉到的。这条刮痕很奇特，是一条直线，而在这条直线末端的不远处，则是另外一条极短的刮痕，似乎刻意想弯成一个圆点。整体看上去就好象是一个叹号，或者倒过来说，象是字母i。很快他在其他四张光盘上也发现了类似的刮痕，它们造型都不同，但都似乎代表着某种符号。阿瓦登回想起喇叭里那个女声最后一句提到过的文件顺序，于是把这五张光盘按照 BBS 论坛统一用户名与密码、BBS 论坛列表、互联网 BBS 论坛使用指南、相应法规、以及最新健康网络词汇列表的顺序排列好，接着依次把那五道刮痕用手指临摹到汽车窗户上。很快那些刮痕构成了一个英文单词：title 题目？这是什么意思？阿瓦登看着这个单词莫名其妙，这究竟是纯属无意的痕迹，还是有人刻意为之？如果是有人刻意为之，他这么做的用意是什么？这时候汽车停住了，又有几名乘客走上车来。阿瓦登挪动一下身体，不让他们看到自己在车窗上写出来的字迹，然后装做打呵欠的样子抬起袖子，轻轻把那五个字母擦掉。阿瓦登暗自庆幸，如果他没有在现在发现这些光盘上的痕迹，那么以后就永远没有机会发觉了。按照规定，个人电脑是不允许使用任何存储存设备的，因此阿瓦登的电脑并没有光驱。他下一步所要做的是将这些光盘送交到管区网络安全部，由他们将光盘内资料登陆到服务器中，再转发给阿瓦登。这是为了防止个人私自在家里制造、阅读或者传播黄色或者反动信息，网络安全部发出的通告是这么解释的。联邦的网络警察经常会突入到个人家中进行临时检查，看用户是否非法拥有信息贮存设备，阿瓦登曾经亲眼见过一个邻居被警察带走，原因仅仅是因为他私自藏了一张光盘在家里——其实他只是打算拿那个当茶杯垫用。那个邻居再没回来过。无论这些符号代表的是什么意思，它都是一种全新的体验，这让阿瓦登感觉到兴奋。怀旧与渴望新奇是阿瓦登生活在这个时代的两根精神支柱，否则他会与这座城市一样变的僵硬，然后窒息而死。他先来到网络安全分部，将光盘交给那里的负责人，负责人反复地检查光盘和阿瓦登的表情，好象所有使用BBS论坛的人都不可信赖一样。末了负责人终于找不到什么破绽，只得将光盘收下，然后举起右手，阿瓦登和他一起高呼“缔造健康的互联网络”。这句话是唯一被允许可以连贯着被说出来的句子。 回到家里，阿瓦登脱掉大衣，摘了过滤口罩，将旁观者扔到了行军床上，然后整个人也倒进枕头里。每次出去外面都会让他疲劳，这一半是因为他孱弱的肉体已经不大适合室外活动；另外一半原因是因为他必须花费大量的精力来应付旁观者。过了四十分钟，他才悠悠地醒过来，头还是和平常一样地疼，胸口还是一如既往地闷。胡乱吃了一点东西以后，阿瓦登爬到电脑桌前，打开电脑，按程序登陆上网络，习惯性地先检查了一遍信箱。信箱里有七、八封新的信件，其中两封是同事发来的事务信。另外五封则是网络安全部发给他的，内容就是他送交的那几张光盘。阿瓦登打开了包含有 BBS 论坛的用户名、密码和 BBS 论坛列表的两封信。他看到自己的论坛通用用户名叫做 19842015，和自己的网络编号完全一样，不由得有些失望。他依稀记得在小的时候，BBS 论坛的用户名是可以自己决定的，而且每一个论坛都可以不同，一个人在网上并不单只是一串枯燥数字。小时候的记忆往往是跟童话和幻想混杂在一起，未必与实际相符。现实中你只能使用有关部门指定的用户名和密码，理由很简单，用户名和密码内也可能含有敏感词汇。 阿瓦登又打开了那份 BBS 列表，全部都是有关部门开设的官方论坛，没有私人的——事实上个人能够合法持有的电脑设备从技术上来说也无法架设新 BBS——这些论坛的主题各有侧重点不同，但基本上是围绕着如何更好响应国家号召，缔造健康互联网络来说的。比如其中一个电脑技术论坛，主题就是如何更好地屏蔽掉敏感词汇。居然在这些论坛中还有一个是关于游戏的。里面正在讨论的是一个如何帮助别人使用健康词汇的网络游戏，玩家可以操纵一名小男孩在街上侦察，看是否有人使用了敏感词汇，小男孩可以选择上前指责或者通知警察，抓到的人越多，小男孩得到的褒奖就越高。阿瓦登随便打开了几个论坛，里面的人都彬彬有礼，说话很“健康”，就好象街上的那些行人一样。不，准确地说，比街上的气氛还要压抑。在街上的人也许还有机会保留一下自己的小动作，比如阿瓦登刚才在公共汽车上就偷偷地写了 TITLE 五个字母；而在网上论坛，人的最后的一点隐私也全被暴露出来，有关部门随时可以调看你的一切行动，无从遁形，这就是科学技术发展所带来的进步。 一阵失落和失望袭上阿瓦登的心头，他合上眼睛，把鼠标甩开，重重地向后靠去。原来他天真地以为 BBS 论坛也许会少许宽松一些，现在看来甚至比现实中更叫人窒息，他感觉到自己好象陷入沉滞的电子淤泥之中，艰于呼吸。“FUXKYOU,YOUSONOFBITCH”再一次涌现到他的唇边，强烈无比，要化好大的力气才控制住。忽然，他又想到了那个神秘的 title，那究竟是什么意思？那五张光盘里或许隐藏着什么？也许这跟 title 有关系？阿瓦登想到这里，把目光重新转向电脑屏幕，仔细去看网络安全部发来的五封信的 title 部分。五张光盘各隐藏着一个字母，凑到一起就是 title，那么按照这个方式，那五封 EMAIL 的 title 凑到一起，就变成了一句话：去用户学习论坛。”阿瓦登记得刚才他确实看到其中一个论坛的名字叫做“用户学习”，于是他抱着姑且一试的心态连接到这个论坛去。他希望这并不是一个巧合。 用户论坛是一个事务性论坛，里面是一些关于 BBS 用户资料的投诉帖和管理帖，斑竹的是一个叫 19387465 的人；发帖的人和回帖的人数量都很少，里面冷冷清清的。阿瓦登打开帖子列表，按照刚才的规律去搜寻每一个帖子的标题，并把它们综合到一起，很快他就得到了另外一句话： “每周日辛普森大楼5层B户。” 又是一个谜团，阿瓦登想。但这却坚定了他的信心，这其中必定隐藏着玄机。光盘、EMAIL和BBS论坛，连续三次都可以通过首词组组合的方式得到暗示，绝非巧合。究竟是什么人会在有关部门的官方文件中隐藏着这样的信息呢？每周日在效率大楼5层B户又会发生什么事情呢？阿瓦登终于找到了久违的兴奋感，未知事物的新奇刺激着他麻木很久的神经。更重要的是，这种在有关部门正式文件中玩弄的文字技巧，叫他有一种喘息的快感，仿佛一个密不透风的铁面罩上几个透进空气的小孔。 营造健康的互联网络。FUXKYOU, YOUSONOFBITCH。阿瓦登盯着屏幕上的桌面背景，用嘴唇比出了那句粗话的口型，并且比出了中指。 在接下来的日子里，阿瓦登一直处于一种潜藏的兴奋状态，就象是一个摆出无辜表情嘴里却藏着糖果的小孩子，在大人转身过去之后露出狡黠的笑容，尽情享受心中藏有秘密的乐趣。日子一天一天地过去，健康词汇在列表里又少了几个，窗外的空气又浑浊了几分，这已经是生活的常态。阿瓦登自己已经开始拿网络健康词汇表当日历来用，划掉三个词就证明过了三天，划掉七个就证明过了一周，于是周日终于到来了。阿瓦登抵达辛普森大楼的时间是中午，暗示的句子里并没有指明时间，阿瓦登认为在中午前往应该是比较可以接受的。当穿着深绿军大衣，耳朵上别着旁观者的阿瓦登来到辛普森大楼的入口时，他的心开始忐忑不安地跳跃起来。他在上一星期设想了无数种可能发生的情景，而现在这个谜底就要揭晓了。无论在周日效率大楼会发生什么，也不会比现在的生活更加糟糕，阿瓦登心里想，所以他并不怎么害怕。 他走进大楼内部，发现这里的人也很少，空旷的走廊里只听到他哒哒的脚步声与回音。一部老电梯里贴着“缔造美好网络家园”的广告，以及一个充满了正义感的男性头像海报，背景是星条旗，他在纸里用右手食指指向观看者，头上写着一行字是“公民，请使用健康词汇。”阿瓦登厌恶地转过身去，发现另外一侧也贴着同样的海报，避无可避。值得庆幸的是五楼很快就到了，电梯的门一开，对面的门上就赫然挂着B户的牌子。门是掉了漆的绿色，门框上还点了几滴墨水，一部简易的电子门铃挂在右上角。 阿瓦登深深吸了一口气，伸手去按电纽。电铃响起，很快屋子里传来脚步声。阿瓦登觉得这脚步的韵律很熟悉，似乎是在哪里见到过。门“咔拉”一声被打开一半，一名年轻女子一手握着把手，把身体前倾望着阿瓦登，警惕地说：“你，找谁？”女子疑惑地问道。阿瓦登一下子就认出了她的声音，就是那个在网络部BBS论坛科柜台后面的女性。她很漂亮，穿着墨绿色绒线衫，头上梳着这时代流行的短发，皮肤特别的白，只有嘴唇能看到一些血色。看着女子的眼神，一瞬间阿瓦登不知道该说什么好，犹豫了一下，他举起右手，轻声回答说：“title。”阿瓦登不知道这句话能否奏效，也不知道自己是否真的找对了地方，但这是他唯一能想到的回答了。他紧张地望着那女子，假如那女子忽然报警，那么自己就会被抓起来仔细审问为什么无缘无故跑到陌生人家里。“肆意游走罪”只比“使用敏感词汇罪”轻那么一点。女子听到他这么说，脸上还是毫无表情，只是把头幅度很轻微地点了一下，右手谨慎地做了一个“进来”的手势。阿瓦登刚要张口，那女子严厉地瞪了他一眼，吓的他把话又吞回去了，乖乖地跟着她进了屋子。 一进屋子，女子首先做的就是把门关好，然后拉起来一层铅灰色的门帘挡在门口。阿瓦登不安地眨着眼睛，趁她拉门帘的时候环顾四周。这屋子是标准的两室一厅，在厅里摆放的是一套双人沙发与一个茶几，茶几上居然还有几束红紫色的塑料花。靠墙是电脑桌和电脑，墙上挂着普通的白色日历，但被主人用粉红色的纸套了边，看起来颇为温馨。一盏粗笨的日光灯从天花板上垂下来，上面象是恶作剧一样挂了几缕绿色的电线，象是垂下藤蔓的葡萄架。阿瓦登注意到厅口的鞋架上有四双鞋，尺码不同，说明今天的客人并不只他一个。阿瓦登正踌躇不安，忽然女子从背后拍了拍他的肩膀，示意他朝里面走。于是两个人穿过客厅另一侧的短小回廊来到其中一间卧室。卧室上挂着同样质地的铅灰色帘子，女子伸手举起帘布，推开了门。阿瓦登迈了进去，首先映入眼帘的是三名面带微笑的人类，以及一间用真正的鲜花装点的房间。屋子里有很多旧日记忆里的古老物品，比如一幅印象派的油画、一尊乌干达木雕，甚至还有一个银烛台，唯独没有电脑。他正在迟疑，女子也进了屋子。她谨慎地拉好门帘关上门，将耳边的旁观者取下，回过身来对阿瓦登用曼妙的声音说道：“欢迎加入说话会！”“说话会？”出于习惯，阿瓦登并没有把这三个字说出口，因为他不确定是否“健康”，只是用眼神表示自己的疑惑。“在这里你可以随便说话，这个该死的东西不会起作用的。”女子把自己的旁观者晃了晃，那个小东西象死掉了一样，对女子句子里两个敏感词汇“随便”和“该死”充耳不闻。 阿瓦登一下子想到上星期在公共汽车站前碰到的男子，如果他摘下旁观者，会不会也会落到同一境地呢？那女子见他犹豫不决，指了指门口的铅灰色门帘说：“放心好了，这里是可以屏蔽掉旁观者信号的，不会有人觉察到。”“你们，是什么，人，这，是，哪里？”阿瓦登一边摘下耳朵上的旁观者，一边小声说道，语调还是改不了那种有关部门规定的说话方式。“这里是说话会，是一个完全自由场所，在这里你可以畅所欲言，请不要拘束。”另外一个人起身对他说道，这是一名瘦高的中年男子，鼻梁上的眼镜非常地厚。阿瓦登嗫嚅着，却找不到发音的焦点，在四个人的注视下显得窘迫不堪，脸都要红起来。女子同情地看了他一眼：“可怜的家伙，不用太紧张，每一个刚到这里的人都是这样。慢慢就习惯了。”她把手搭到阿瓦登的肩上：“我们其实见过的，当然，我见过你，而你没见过我。”她一边说，一边将自己的头发解下来，原来她留的是一头齐肩的乌黑长发，头发披下来的一瞬间阿瓦登觉得她真的很美。“我……我记得你，记得你的声音。”阿瓦登终于说出了一句完整的话，虽然不够流畅。“是吗，那可太好了。”女子笑起来，拉着他的手，让他坐到沙发上，递给他一杯水。阿瓦登注意到这是一个款式古老的茶杯，上面还刻着花纹，杯子里的水带着浓郁的香气，阿瓦登尝了一点，那种甜丝丝的味道对喝惯纯净水的舌头来说刺激格外地大。让他觉得浑身一下子被注进了许多活力。“弄到这个可不容易，我们也不是每周都能喝到。”女子坐到他身边，两只乌黑的眼睛注视着他，“你是怎么知道这个集会的？”阿瓦登把发现光盘暗示的过程说了一遍，其他四个人都赞许地点了点头。“果然是个聪明人，脑筋还没被陈腐的空气腐蚀掉。”一个三十多岁的胖子称赞道，他的嗓门大的要命。那个戴眼镜的中年人把两只手交叉在一起，表示赞同。“这正是天生的说话会成员，聪明、敏锐，而且不甘屈从于沉默。”“那么。”胖子提议，“先让我们鼓掌欢迎说话会的新成员吧。”于是四个人鼓起掌来，小小的屋子里响起一片掌声。阿瓦登羞涩地举起杯子做回应，他还不太习惯这样的场面。等到掌声稍息，他抬起头怯生生地问道：“可以问个问题吗？说话会到底是什么？” 带他进屋的女子伸出食指，在他鼻子前两公分的地方比了一比，解释道：“说话会，就是可以畅所欲言的集会。在这里你不必顾忌什么，说出任何你想说的东西。这里没有敏感词汇，也没有健康互联网络。这里是绝对自由的空间，你可以尽情释放你的灵魂，舒展你的身体，没有任何禁锢与束缚。”说着说着，她的声音变的高亢、奔放，里面饱含了许多早已经被屏蔽掉的词汇，阿瓦登不曾听到这样流畅连贯的话语很久了。“我们的宗旨就是，说话，就这么简单。”中年人扶扶眼镜，补充道。“可是，要说些什么呢？”阿瓦登又问道。“任何事情，你心里想的任何事情都可以说出来。”中年人露出宽和的笑容，“尤其是那些被美国政府限制的思想。”这可真是一个大胆的集会啊，这分明就是犯罪，阿瓦登心想，但他发觉自己却被这种犯罪慢慢地吸引住了。“当然，有件事我们会事先说明。说话会是危险的，每一个成员都冒着被有关部门拘捕的风险。联邦执法人员也随时可能破门而入，以非法集会以及非法使用不合法词语的名义把我们抓起来。你现在有权拒绝加入，并且离开。”阿瓦登听到女子的警告，心里一度犹豫起来。但一想到此刻离去的话，那么又要开始持续那种窒息的泥沼生活，他就难以压抑自己的烦闷。阿瓦登第一次发现，原来“说话”对他来说是一个致命的诱惑，他先前并不知道自己原来是如此地渴望着说话。“我不会离开的，我要加入你们，说话。”“那太好了。唔，那么不妨就从自我介绍开始吧。”女子高兴地说，同时站起身来，把右手搭到胸前，“从我开始。我的名字叫阿尔特弥斯，至于网络编号和身份证号码，让他们见鬼去吧！谁会去管那个！我有我自己的名字，我不是数字。” 她的话让所有人包括阿瓦登都笑了起来。接着她继续说道：“不过，这其实只是一个假名，这是希腊神话里的女神。”“假名？”“是的，和我户籍本上的名字是不同的。”“可是，为什么？”“你不会对自己在档案里的名字厌倦吗？我想起一个自己喜欢的名字，哪怕只有一次机会也好，自己想叫什么就叫什么。在这个说话会里的每一个人都有一个自己喜欢的名字，我们彼此拿这个称呼。”阿瓦登若有所思地点点头，他很理解阿尔特弥斯的想法。事实上当他在使用网络论坛的时候，也希望能够自己取一个称心如意的名字，而不是被分配一个用户名。通过介绍，阿瓦登了解到阿尔特弥斯是网络部BBS论坛管理科的职员，今年23岁，未婚，最讨厌蟑螂和蜘蛛，喜欢缝纫与园艺，屋子里的花就是她偷偷从城市边缘摘回来的。 接下来是那名中年人，他自我介绍说名字叫兰斯洛特，41岁，是城市电厂的一名工程师；兰斯洛特这名字出自英国的亚瑟王传说，是一名忠贞的骑士。他有自己的老婆和两个孩子，一男一女；男孩三岁，女孩四岁，他们最喜欢吃的就是柠檬味道的水果糖。说到这里，兰斯洛特说希望下次聚会能把他们也带了，孩子们正是学说话的时候，他想教给他们真正的说话。那个三十多岁的胖子是网络部的一名网管，叫瓦格纳。这个身份让阿瓦登吃惊不已，他的印象里网管都是些绷着脸全无表情的冷漠生物，但眼前的瓦格纳脸圆滚滚的，油光锃亮，嘴边两条翘起的小胡子神气十足。他喜欢的是雪茄和歌剧，利用网管的特权这两样东西都不难弄到。“这个能屏蔽掉信号的门帘就是他弄的。”阿尔特弥斯补充说，瓦格纳冲她做了个“乐意为您效劳”的手势，然后点燃了雪茄，把它放到嘴里，很快屋子里就笼罩起一片稀薄的烟雾。说话会的第四名成员是一位穿着黑色制服的女性，今年刚满三十。她的名字是杜拉丝，城市日报（那个时代的报纸已经全部都数字化了）的编辑，她比阿尔特弥斯还瘦，颧骨高高耸起，眼窝身陷，两片薄薄的嘴唇即使在最说话的时候也很少分开，看不到牙齿。爱好是饲养狗和猫，尽管她并没有养。“那么，到你了。”阿尔特弥斯对阿瓦登说。阿瓦登想了想，结结巴巴地把自己的情况说了一遍，当谈到自己的爱好时候，他一时间居然想不到自己喜好什么，似乎什么都没有，在那之前他甚至从来没想过。“那，你最想做的是什么事呢？”阿尔特弥斯把手再一次放在他肩上，诱导着问道。“真的什么都可以？”“什么都可以，在这里没有任何限制。” 阿瓦登觉得自己终于找到机会了，他咳了一声，抓抓头，脱口而出一句响亮的叫喊：“FUXKYOU, YOUSONOFBITCH！” 在一瞬间，在座的四个人都被他这句话震惊了。瓦格纳率先反应了过来，他先叼住雪茄，用力鼓掌，然后用右手把雪茄取下来，张嘴大声地赞叹道：“真棒，痛快，这简直是最完美的入会誓词。”“我宁可听十遍这样的脏话，也不想再去碰那个乏味的电子女声。”兰斯洛特也是一脸陶醉，毫不掩饰自己对电子女声的厌恶。而阿尔特弥斯和杜拉丝全都咯咯地笑起来，杜拉丝发现自己的笑容幅度大了一点，不好意思地把嘴掩住。阿瓦登觉得他们与其说是觉得新奇，不如说是在享受这句脏话所带来的对体制的蔑视与挑战。“那你叫希望自己叫什么名字呢？” 阿尔特弥斯歪着头问。“唔……王二。“ 阿瓦登沉吟了一下，回答说。这是一个中式的名字，他以前有一个中国人朋友，喜欢讲故事，故事里的主角名字总是叫王二。屋子里的气氛现在完全融洽了，大家都开始谈些比较自然的话题，每个人都摆出了最舒服的姿势，阿尔特弥斯不时拿起茶壶来为大家续水。阿瓦登紧张的心情逐渐放松下来，他感觉到自己的脑子前所未有地轻松。 “你知道的。”阿尔特弥斯又给他倒了一杯甜水，“我们一直想把说话会保持在一定规模，平日是没有办法畅所欲言的，我们需要空间。麻烦的是，我们没办法公开征集会员，又不可能直接通过物理接触去寻找，那风险太大。于是兰斯洛特就设计了一套暗示系统，只有发现这些暗示的人才能知道本会的存在。”“这套系统考虑到的还不止是安全问题。”兰斯洛特把自己的眼镜摘下来仔细擦拭了一下，得意地说，“这其实也是一个会员资格验证。说话会所吸纳的成员，必须有智慧，有头脑，内心渴望激情，并且对自由有着渴望。”瓦格纳用两根指头夹着雪茄，在事先准备好的烟灰缸里弹了弹烟灰，大声说道：“据我的经验，申请BBS论坛服务的人，大多数都是为了怀旧，或者说渴望一些新鲜的东西，这样的人往往都怀有激情，认为BBS论坛也许能给他们一些与现实不一样的东西——当然，事实上并非如此，美国政府对BBS论坛的管理甚至严厉过电子邮件——这暗示着他们心里渴望解脱束缚。因此我们将暗示隐藏在申请BBS论坛的光盘之中，只有申请人才有机会接触到这些暗示。而只有那些有智慧、观察敏锐的人才会发觉到这些暗示的存在，并顺利解读出来，找到这里。”“归根到底，说话会也不过是一群渴望自由说话的秘密小团体罢了。”兰斯洛特笑道。“你是第二个找到说话会的人，第一个是杜拉丝小姐。”阿尔特弥斯告诉阿瓦登。阿瓦登敬佩地看了杜拉丝一眼，后者淡淡地回答道：“这没什么，这是我的工作，我的工作就是摆弄文字。” 阿瓦登想到上一周在公共汽车站碰到的那个疯狂男子，于是把这件事讲给其他成员听。听完之后，兰斯洛特摇了摇头，从嘴唇里滑出一声叹息：“这样的事情我也是见过的，我的一个同事就是如此。所以说话会的存在是必要的，这是缓解压力的阀门。长时间的敏感词汇限制会让人都疯掉的，因为他们既无法思考又没办法表达。”“这正是美国政府有关部门所希望看到的，这样只有傻瓜能够存活下来，一个全是傻瓜的社会是稳定的。” 瓦格纳费力地把自己肥胖的身躯挪了一下位置，轻蔑地说。“你也是有关部门的一分子，瓦格纳先生。”阿尔特弥斯一边往茶杯里续了些热水，一边抬头轻声说道。“阿尔特弥斯小姐，我只是一个能比普通人多使用几个敏感词汇的普通人而已。”大家都笑了起来。阿瓦登从来没有见过这么多的人说这么多的话，这是前所未有的奇妙经验。他惊讶地发现自己居然很快就融进了这个小圈子里，隔阂与陌生感很快就消失了；同时消失的还有胸闷与头晕等习惯性的毛病。很快话题就从说话会本身扩展到了更加宽泛随意的话题，阿尔特弥斯唱支歌，兰斯洛特说了几个笑话，杜拉丝则给大家讲了美国南部诸州的风土人情；瓦格纳甚至还唱了一段歌剧，虽然阿瓦登一个字也听不懂，但他一点也不吝惜掌声。在这个城市的某一个被屏蔽掉的角落里，五个不甘沉默的人正在享受着在这个时代视为奢侈品的事情——说话。 “王二，你可曾看过《1984》？”阿尔特弥斯忽然问道，她就靠着阿瓦登坐下，阿瓦登摇摇头，反问道：“这是网络编号的一段么？”“这是一本书的名字。”“书？”阿瓦登听到这个名词，头摇的更大了。这是个古老的名词，在这个电脑技术非常发达的时代，网络可以承载一切信息，任何人都可以在网上图书馆查到电子版；因此有关部门认为实体书籍变成了一种没有必要存在的浪费，实体书也就逐渐消亡了。瓦格纳对此的评论是：“有关部门喜欢电子书籍的心情是可以理解的，电子书籍的话，只需要FIND和REPLACE两个命令就可以消灭掉全部不健康词汇，替一本书消毒；而实体书籍的校对与修订却是件旷日持久的工作。”“这是一本伟大的书，是旧世界哲人们对我们这个时代的预言。”阿尔特弥斯认真地说。“它很早以前就洞察到了肉的束缚与解脱，灵的束缚与解脱，这是说话会的基石。” 阿瓦登不无惊奇地发现他的网络编号开头恰好是这这本书名字：19842015。“那么，该怎么样才能看到呢？”阿瓦登盯着阿尔特弥斯乌黑色的眼睛问。“我们也无法找到纸质版，网络图书馆不可能存在这样的书。”兰斯洛特摇摇头，然后重新露出笑容，左手向着杜拉丝摆了个请的姿势，“但我们的杜拉丝小姐应该为她的记忆力而自豪，她在很早已经有幸阅读过这两本书，并且能够记得里面的大部分文字。”“太好了，然后她写下来了，对吗？”“那太危险，这时代持有实体书是个大罪过，也容易让说话会暴露。我们只是在每次聚会的时候请杜拉丝小姐为我们背诵。既然是说话会，那么把这两个故事讲出来不是更名符其实吗？”大家都安静下来，杜拉丝站起来走到屋子中央，其他四个人坐在旁边看着她。阿瓦登不经意地把手搂在阿尔特弥斯肩上，后者微微朝这边靠过来，女性头发的幽香“咝咝”地划过他的鼻子，让他的心里一阵荡漾。屋子里非常暖和，他分不清这是花香还是阿尔特弥斯的味道。杜拉丝的声音并不高，不过却很清晰有力；她的记忆力确实惊人，不仅记得情节，包括一些细节和句子都可以复述下来。杜拉丝讲到了朱丽亚假装摔倒，然后偷偷递给温斯顿一张写着“我爱你”的纸条，绘声绘色，这让听众们都听的入神了，阿尔特弥斯听的尤其认真，以至于都没有注意到阿瓦登一直注视着她。 “1984的作者预见到了专制的进步，却没有预见到技术的进步。”瓦格纳在杜拉丝停下来喝水的时候发表自己的评论，阿瓦登觉得他与外貌不太相称，是个很有洞察力的技术官僚。“在大洋国人们还可以靠传递纸条来偷偷表达自己的想法，但是现在不一样了。美国政府有关部门把我们全赶到了网上，而在网络技术发达的今天，我们即使想发一条短信都会被系统或者网管看的一清二楚，无从遁形。现实里呢，还有旁观者在。”瓦格纳在腿上敲了敲雪茄根部，“一句话，技术是中性的，但技术的进步会让自在的世界更自在，集权的世界更加集权。”“这句话说的很有哲学家的味道哟。”阿尔特弥斯冲瓦格纳挤了挤眼睛，从抽屉里取来一把饼干和曲奇散发给大家。“就好象同样是0和1，有的人就能写出工具软件，有人却拿那个编出恶性病毒？”阿瓦登想到一个类似的比喻，瓦格纳听了以后满意地打了个响指。“很不错的比喻，王二，就是如此，真不愧是程序员。”谈话持续了不知道有多久，杜拉斯看了一眼墙壁上的挂钟，连忙提醒谈兴正浓的四个人时间快到了。说话会不能持续很长时间，旁听者被屏蔽的越久，暴露的危险就越大。 “那么好吧，我们就抓紧最后半个小时来完成今天的活动。”阿尔特弥斯一边说着，一边将桌子上的空杯子收走。兰斯洛特和瓦格纳也都站起身来，活动一下已经有些酸疼的肩膀和腰，只有杜拉丝坐在位子上没有动。“活动？还有什么活动？”阿瓦登奇怪地问道，说话会除了说话还有其他活动？“唔，对啊，我们还有其他活动。”阿尔特弥斯撩起额前的长发，对他妩媚一笑：“我们还会和对方完全交流。”“完全交流？”“就是intercourse”“………………”阿瓦登一下子变的脸色苍白，呼吸急促起来，仿佛胃里被灌进去零下三十度的寒风，他几乎不相信自己的耳朵。“说话会有说话的自由，也有选择与谁上床的自由。”阿尔特弥斯毫不羞涩地说，“我们互相谈话，然后选择合适的人做爱，就象我们选择我们喜欢的词汇说话一样。”兰斯洛特看阿瓦登很窘迫，走过去拍拍他的肩膀，慢慢地说：“当然，我们不会强迫任何人，这完全是在自愿的基础上。今天我还要早点回去照顾小孩，你们人数正好合适。” 阿瓦登的脸色涨红，热的仿佛夏季的电脑CPU，他甚至不敢多看阿尔特弥斯一眼。他憧憬过女性很长时间，但如此接近还是第一次。还要回家去照顾小孩子的兰斯洛特向大家道别后就先行离去了，阿尔特弥斯将房间留给瓦格纳与杜拉丝，然后带着惶恐不安的阿瓦登来到了另外一间房间。这间显然是阿尔特弥斯的卧室，屋子里很简单，但却收拾的十分干净，在床上枕头旁还摆着一个手制的布娃娃，床单和窗帘都是粉红色的。最初的是由阿尔特弥斯主动开始的，丝毫没心理准备的阿瓦登只是被动地任她摆布。经过了几轮挑逗，阿瓦登才逐渐放开，任由潜藏在自己心内的原始欲望奔流出来，那种期待听到圆润女声的青春憧憬本来只是苦闷生活的意淫，而在今天它加倍实现了。很快这种憧憬与他在现实中被压抑的郁闷合流，转化成了猛烈的冲动，让他一次又一次与阿尔特弥斯融为一体。阿瓦登不知道这种冲动和他想大声说出“FUXKYOU, YOUSONOFBITCH”冲动有什么不同，不过现在不是考虑这个的时候，他现在脑子里想的只有尽情地、全无束缚地让自己释放激情，完全没有任何束缚。强烈的刺激一波波地冲击着兴奋中枢，最终一阵快感浪潮在狂暴洋面扬起头来，达到了一个极高的顶端。阿瓦登在那一瞬间感觉到了前所未有的自由，那种轻盈无比的自由，以及因自由而生的快乐与疲惫。浑身是汗的他喘息着倒在了阿尔特弥斯身上，一阵舒畅的倦意如潮水般淹没了他的身体…… ……当他醒来的时候，发现阿尔特弥斯躺在自己身边，赤裸的身体好象一尊白玉雕像，睡姿恬美静谧。他侧过身子去，慵懒地打了个呵欠，然后阿尔特弥斯睁开了眼睛。“很舒服，对不对？”她问道。“是啊……”阿瓦登不知道该说什么好，他顿了顿，犹豫地说道：“你以前和兰斯洛特、瓦格纳他们也……呃，我是说，象刚才那样子过吗？”“是的。”阿尔特弥斯温柔地回答，她半支起胳膊，长发从肩膀披到了胸口。她的大方坦白反而让阿瓦登有些不知所措。屋子里出现了一段时间的沉默，然后阿尔特弥斯忽然开口问道：“还记得今天杜拉丝讲的那段故事吗？女主角偷偷递给男主角写着“我爱你”的纸条。”“唔，还记得。”阿瓦登回答，很高兴终于能从那个拙劣的话题摆脱出来了。“在有关部门的健康互联网络词汇列表里，没有爱这个字呢。在我们这个时代，我爱你也是一个敏感词汇，被屏蔽掉了。”阿尔特弥斯的眼神里似乎是感慨，又象是失落。“我爱你。”阿瓦登不禁脱口而出，他知道在这间屋子里可以说出任何自己想说的话，不必顾忌。“谢谢你。”阿尔特弥斯听到之后只是笑了笑，起身穿上衣服，催促阿瓦登时间差不多了。阿瓦登有些失望，因为她没有预期反应的热烈，仿佛他刚才说的只是有些无关紧要的东西。这时候杜拉丝和瓦格纳已经离开了，屋子里只剩他们两个人。阿尔特弥斯把他送到门口，将旁观者交给他，然后叮嘱他说：“记得在外面绝对不要提及说话会的任何事情或者任何人，我们在说话会以外的地方是完全不认识的。”“我记住了。”阿瓦登回答，然后转身要走。“王二。”阿尔特弥斯忽然叫道，阿瓦登连忙转身，还没等他反应过来，两片柔软温暖的嘴唇忽然贴到了他的双唇，然后是一个细切的声音在耳边响起：“谢谢你，我爱你。” 阿瓦登觉得眼睛有些湿润，他戴上旁观者，推开门，重新步入到那一片令人窒息的世界中去，但他此时已与来时的心境大不相同。此后阿瓦登的精神面貌明显有了改善。他谨慎地享受着这种秘密集会的乐趣，并且乐在其中。每一周或者两周，他们五个人都会在周日秘密地举行说话会的活动，聊天，唱歌或者听杜拉丝讲1984的故事。阿瓦登同阿尔特弥斯又“完全交流”了几次，偶尔他也会跟杜拉丝“交流”。他有了两个身份，一个是现实中和网上的阿瓦登，编号19842015，还有一个是说话会里的王二。他很享受这个名字，觉得这就是自己另外的一个人生。有一次集会，他们谈到了敏感词汇的问题。阿瓦登记得很早的时候——他对这方面的记忆有点模糊——有关部门给出的是一份敏感词汇列表，由网站的内部管理人员秘密参考使用，他对如何演变成现在的局面大惑不解。那一天瓦格纳带了一瓶葡萄酒，兴致很高，于是索性给他们讲了讲“屏蔽”的进化史，身为网管的他经常可以接触到这些资料。 在最开始美国政府只是单纯地屏蔽掉敏感词汇，但很快他们就发现这样的措施根本没有用处。很多人会采取在词组中夹杂符号或者数字的方式来绕开系统检查；于是有关部门不得不将这些近似敏感词汇也一一屏蔽掉。然而众所周知，数字与符号之间的组合方式是近乎无限的，只要你有想象力，就完全可以组合出一个新的词组而且不失掉他的原意。比如说“politic”这个词，就有“politi/c”、“政polit/ic”、“pol/itic”等近乎无限种表达方式。当有关部门意识到这个问题时，他们采取了新的策略。既然无法辨识词组，那么就用单词屏蔽。这一举措在一开始是奏效的，违规交谈的人显著减少，但很快人们就发现可以用同音字或者谐音的方式来继续表达自己的危险思想。即使有关部门封掉全部敏感词汇的同音字，也无济于事，思想活跃的美国人充分发挥了自己的想象力，使用隐喻，借代、类比、引申及其他修辞方法，或者将一个敏感词用数个不敏感的字来代替。人类的思维方式要比电脑开阔许多。电脑屏蔽掉一条路，他们还会有更多的路可以选择。这一场水面以下的角力看起来似乎是美国大众要取得胜利。这时候，一个具有逆向思维精神的人出现了。他的身份不明。有人说他是有关部门的主管；也有人说他是因过度使用敏感词汇而被捕的危险人士。无论他是谁，总之整个局面被扭转过来。他向有关部门建议，不再告诉大众禁止说什么，而是规定他们只能说什么，用什么方式去说。有关部门很快就心领神会，制订了新的规章制度：取消了敏感词汇列表，取而代之的是互联网络健康语言列表，并把这举措推广到了日常生活中的语言屏蔽系统中去。 这一次，大众终于处于下风。以往他们与有关部门尽情地在网络与现实中捉着迷藏，而现在他们却被有关部门扼住了咽喉。这样一来，有关部门可以有效率地掌握住言论，因为整个语言的框架都被彻底控制了。在有限的空间内，大众几乎是无计可施。尽管如此，大众还是不屈不挠地将这场战争——或者说游戏——继续下去，他们挑选健康词汇列表中的合法字眼来表达不合法的意思：两个连续的“稳定”意思就是“反对”，“稳定”加“繁荣昌盛”则暗示“屏蔽”。美国政府不得不对这一动向保持着警惕，并日复一日地将更多的词汇从健康词汇列表里删掉，禁止大众使用。“当然，这场战争会持续下去的。只要世界上还存在着两个不同的字或者词组，那么就可以继续自由交流——你知道莫尔斯电码吧？”瓦格纳说到这里，拿起茶杯一饮而尽，满意地打了一个嗝。“可是，这场战争的代价就是语言的失落。表达能力会越来越贫乏，越来越淡而无味，人们会越来越倾向于沉默，这对有关部门反而是好事。”兰斯洛特摆出一副忧虑的表情，有节奏地用指关节敲击着桌面，“这样一来，岂不就等于是大众的自由意识将语言推向死亡的边缘？真讽刺啊。按照这个趋势，有关部门是不会败的，他们会笑到最后。”“不，不，笑这种情感他们是不会了解的。”瓦格纳淡淡地回答。“我倒是觉得，美国是一直处于恐惧的情感之中呢，生怕人们掌握了太多的词汇，表达出太多的思想，变的难以掌握。”阿尔特弥斯说完摆出一副她在上班期间冷若冰霜的呆板脸孔，学着僵硬的腔调喊了一句：“营造健康的网络环境，美国万岁！”杜拉丝、兰斯洛特与瓦格纳都哈哈大笑，唯一没笑的是王二（阿瓦登）。他对于兰斯洛特刚才的那句话始终耿耿于怀：大众与有关部门的对抗，其最终结局就是语言的消亡。那么他们现在这个小小的说话会，也只不过是在一列开向悬崖的列车里关上窗帘，享受坠毁前最后的宁静罢了。不过他没有说出口，因为这太煞风景了。阿瓦登不希望破坏说话会的气氛，这对他很重要。 从说话会回到家里，阿瓦登躺在行军床上，双手枕着脑袋，陷入了沉思。自从加入说话会以后，他变的比以前更容易陷入思考。有时候他想的是这个社会、这个互联网络或者这座城市中存在的荒谬性；有时候他想的是自己的生活；还有时候他想的是阿尔特弥斯。他不知道是不是在一个压抑的世界里，人的情感会变的格外强烈，他现在陷入对阿尔特弥斯的迷恋无法自拔。阿瓦登一直很羡慕杜拉丝讲的《1984》里面的温斯顿，他和朱利亚有一间两个人独处的小屋，一个只属于他们两个人的小世界。他在与阿尔特弥斯“完全交流”的时候曾经吐露过自己的心声，阿尔特弥斯没有直接回答，而是表示两个人的关系无法再比说话会更近一步——维持现在的状态就已经是个人行为的极限，有关部门可不会一直打瞌睡。“我们只能把感情生活压缩在每周一次的说话会活动里，这已经很奢侈了。”她对他说，同时温柔地抚摩他的胸膛。“只有在说话会里，我们才是阿尔特弥斯和王二。而在其他时间里，你是 19842015，而我是 19387465。”对此，阿瓦登只能发出一声轻轻的叹息，确实他不该奢求更多。除了感情，发生变化的还有互联网络。自从加入说话会以后，阿瓦登逐渐发现互联网表面下潜藏的一些东西。正如瓦格纳在一次活动的时候指出，普罗大众与有关部门的战争从未结束，总会有思想和言论从严厉管制的缝隙中流泻出来。阿瓦登发现，在完全公式化的EMAIL与网络论坛中其实隐藏着不少耐人寻味的细节，就好象那个title一样，存在着各式各样的密码与隐藏寓意。这些东西出自不同人的手里，样式和破译方式都不同，阿瓦登不知道那些密码背后隐藏的是怎样的内容。不过有一点可以确知的是，说话会并不是唯一的一个地下集会，瓦格纳说的对，始终还是有人在试图用“健康”词汇表达“不健康”思想。讽刺的是，给阿瓦登感触最深的，是有关部门的管制。以往他只是模糊地感觉到自己被绑缚起来，现在他能清晰地看清这种束缚与压抑的脉络，以及加诸在自己身上的各种手段。在小小说话会中享受到的自在让他更加深刻地感受到在宽阔现实中的不自在。 “FUXKYOU, YOUSONOFBITCH！” 每一次的聚会，三位男士都会轻蔑地一起高喊这一句粗话。他们清楚这不会给有关部门带来什么不良影响，不过这确实很痛快。这一周，阿瓦登特别地忙碌，他的同事因为不明原因而被屏蔽掉了，这样一来整个项目就全压在了他一个人身上。这项目是为有关部分设计一种软件，用来控制大功率主动式“旁观者”的能源分配控制。软件很复杂，他不得不每天在电脑前工作十几个小时，只有在身体实在支撑不住的时候才停下来随便吃一点东西，喝一口纯净水，困了就躺在旁边的行军床上睡上一觉，爬起来继续工作。屋子里满是浑浊的烟味与袜子脏衣服的酸臭味，阿瓦登就在这种环境下蓬头垢面地敲着键盘，并不时揉揉满布血丝的眼睛。偏偏在这个时候屋子里的暖气坏掉了。洋灰色的暖气片从昨天开始就变的冰凉，不再有热水流动。阿瓦登检查了一下，发现并不是管道问题，而且邻居们也碰到同样的事，看来是供热系统出了问题。这一变故的正面影响是稍微淡化了屋子内的酸臭味，负面影响是整个屋子变的有如冰窖一样。紧闭的窗户和门能挡住寒风，却挡不住寒冷，低温让本来就寒酸的房间更笼罩上一层霜气。无论是那把木椅还是行军床都象是冷酷的冰雕，屋子里唯一还有些热气的就只剩下电脑。阿瓦登不得不披上所有的御寒衣物，蜷缩在床上，把电脑的散热口对准自己。 有关部门宣布“供热”和“暖气”暂时也被列入敏感词汇，于是阿瓦登没办法写信向供热部门询问，只好静待，除了用来敲键盘的指头以外，尽量保持全身一动不动，以节约热量。在停止供暖后的第四天，暖气片里终于传来“哗啦哗啦”的声音，带着热气的水开始流动，屋子里恢复了温暖，“供热”和“暖气”又可以恢复使用了。于是EMAIL与网络论坛上全都是“庆祝有关部门恢复供应暖气，急人民之所急”的帖子，EMAIL新闻组里也全是类似主题。不过这对阿瓦登来说太晚了，他生了病，感冒，而且是重感冒。他面色苍白，全身软弱无力，头疼的象是被一枚达姆弹射入头部，只能躺在床上等医生。医生来到他家里，给他做了两三次点滴，喂了一些叫不上名字的药片，叫他静养。这一场病足足持续了数天，他不得不放弃参加这一星期的说话会，身体状况实在太差了，阿瓦登甚至怀疑自己搞不好会因此而死掉。 阿瓦登躺在床上，心里懊悔不已，说话会是他唯一的乐趣，现在他却没办法参加。他把头蒙在被子里胡思乱想，瓦格纳这一次会带什么特别的东西来呢？兰斯洛特有没有把两个孩子也领过来？还有阿尔特弥斯，他没参加的话，她会和谁“完全交流”呢？瓦格纳还是兰斯洛特？他还想到了杜拉丝，上一次的聚会里，杜拉丝讲到了温斯顿在秘密幽会的屋子里对朱丽亚说“我们已经死了”，朱丽亚附和着说“我们已经死了”，这时候第三个声音说道“你们已经死了。”杜拉丝就讲到这里，就停住了。阿瓦登急切地想知道接下来发生了什么，第三个声音是谁，是党吗？温斯顿和朱丽亚是否会被捕，他们会有什么样的下场？不光是他，阿尔特弥斯也很希望知道后续情节的发展，不过她并没有去追问杜拉丝。 “让这成为一个悬疑，这样接下来的一周我们的生活都会在期待的乐趣中度过。”她对阿瓦登说，然后两个人继续沉溺于intercourse的快乐。“也许他们都会死。”阿尔特弥斯在交流结束后，看着天花板说。“也许那只是奥布林的声音，他去探望他们。”阿瓦登安慰她道，但是他的心里也不确定。阿瓦登的病持续了十天，然后他终于痊愈了。他痊愈后的第一件事就是从床上爬起来，然后去看墙上的日历：这一天恰好是星期日，说话会活动的日子。阿瓦登已经缺席了一周，这已经令他如饥似渴，甚至做梦都在和他们一起喋喋不休地说着话——所幸他并没有说梦话的习惯，所以24小时工作的旁观者并没发出任何警报。 阿瓦登简单地洗了一下脸，用一把有些生锈的剃刀沾着肥皂仔细地刮掉脸上粗硬的胡须，然后咕噜咕噜地刷了刷牙齿，用手和毛巾沾着热水将自己蓬起的乱发压下去。因为生病，有关部门发了一些补贴给他，其中包括两块羊角面包、两瓶姜汁啤酒和一份精制砂糖。他将这些东西都用塑料布仔细包好，揣到宽大的军大衣里，打算带到说话会上去与大家分享。今天的天气和往常一样地冷，阿瓦登把自己裹在大衣里，登上前往效率大楼的公共汽车。一路上车厢里的广播重复着“营造健康的互联网络”以及一些优秀网络用户的先进事迹；车厢前面的电子屏幕不断滚动显示着最新的健康词汇列表，一个旁观者自车顶垂下来睥睨着车内的每一个表情呆滞的人。阿瓦登坐在最后一排，望着窗外不断向后移动的建筑物与枯黄的树木发呆。车子很快就到达了辛普森大楼附近的车站，阿瓦登下了车，把手放到怀里摸了摸塑料布包着的食物，朝着大楼走去。他在半路无意中抬起头，忽然一阵冰冷的寒流刺入他的胸腔，迫使他停住了脚步。 有什么地方不对劲！他看到了效率大楼的第五层阿尔特弥斯家的窗户发生了一些奇怪的变化。以前阿尔特弥斯家面向大街的窗户总是挂着粉红色的窗帘，而现在窗帘则被扯到了两边，窗户大开，用肉眼可以勉强看到窗玻璃和屋子里雪白的墙壁。假如今天有说话会的话，阿尔特弥斯绝对不会把有屏蔽效果的窗帘打开。而且打开窗户这件事也绝不寻常，在这个城市里的室外空气十分浑浊，几乎不会有人会去开窗换气。也就是说，今天并没有说话集会召开，而是发生了另外一些事情。阿瓦登望着那窗户，心情开始变的有些慌乱，他把手从兜里掏出来，叼起一支香烟，把身体靠在一根电线杆旁故做镇静，以免被行人怀疑。究竟说话会发生了什么，为什么这一周停办了呢？要知道，只要还有复数的成员能够出席，说话会就会一直办下去，难道说瓦格纳、兰斯洛特、杜拉丝和阿尔特弥斯同时无法出席？这种概率实在太小了。阿瓦登一边这样想着，一边向四周不安地张望。忽然他看到了一样东西，一个念头霎时占据了他的全部心灵，让他几乎眩晕过去。 “说话会本周不会有了，以后也不会有了。”阿瓦登嘴唇默默地蠕动着，面如死灰。他看到在街道内侧一处不起眼的地方隐藏着一个类似雷达天线的东西，其造型很象是两个背部贴在一起的大碗。阿瓦登心里清楚这是什么东西：这正是他负责软件设计的大功率主动式“旁观者”，这造型他很熟悉。这装置可以主动发射电波去探测人们的声音，并检查其中是否存在敏感词汇。这样的装置居然就安放在阿尔特弥斯家附近，那么就等于说话会完全暴露在了有关部门的监控之下。主动式旁观者的强大刺探电波会轻易刺穿她家中的铅质窗帘，把所有成员的话原封不动地传到有关部门耳朵里。 这是一个划时代的发明，这一技术的突破意味着有关部门可以不再被动地等待警报，可以主动出击去刺探人们在任何时间任何地点说的任何话语。阿瓦登可以想象接下来会发生什么，阿尔特弥斯他们的每一句话都被有关部门记录下来，会有机器统计出到底有多少违禁词汇被他们使用过；然后联邦警察会冲进她的屋子，将正在聚会的成员们都带走，只留下搜查过后空荡荡的房间和窗户。阿瓦登想到这里，心如刀绞，他一点也不为自己的侥幸逃脱而感到幸运。他的胃袋翻腾起来，一种恶心的感觉从胃里直接升到嘴边，让他想吐，却又不能吐——因为“呕”也是个敏感词汇；大病初愈的孱弱身躯无法承受这种打击，象害了风寒一样颤抖起来，几乎站立不住。他不敢继续朝前走去，仓皇地转过身去，登上另外一辆公共汽车，把嘴闭的更紧了。等阿瓦登回到自己家楼下，看到楼房附近另外一架新的主动式旁观者正在兴建中，漆黑的天线在半空舒展开来，仿佛一面巨大的蜘蛛网。看来有关部门已经着手在整个纽约市部署这种新兴高科技产品。 他不敢驻足观看，低着头从那巨大装置旁边走过，一路不停地走回家，然后把自己的脸紧紧地压在枕头里，却不敢哭出声音来，连一句“FUXKYOU, YOUSONOFBITCH”都不能说。从那以后，阿瓦登的生活回到了普通状态——就是说和原来一样沉滞、压抑、欠缺激情，健康向上，缺乏低级趣味。兰斯洛特说过：“战争的结果就是，大众的自由意识会将语言推向死亡的边缘”，现在看来，他的预言是很准确的：说话会的覆灭，导致“说话”、“歌剧”、“完全”、“交流”几个词先后被剔除出了健康词汇列表，成为敏感词汇。另外，虽然阿拉伯数字还能用，但“1984”这一个数字组合也被屏蔽掉了，这让包括阿瓦登在内的程序员在编写程序时不得不谨慎地检查数字是否违规，这额外增加了很多工作量，让他更加疲惫。阿瓦登不是没有担心过，也许在某一天的深夜，他就会忽然接到一封EMAIL，让他留在家里不要动，不要试图在网络做任何动作；接着电话会响起，电子女声会把这一要求重复再重复，直到警察打开他家的大门，把他带去未知的地方，那里有未知的命运等待着他。《1984》后面的情节发展阿瓦登始终不知道，唯一知道的杜拉丝已经彻底失踪了，所以温斯顿和朱丽亚的结局始终是个谜；就好象兰斯洛特、瓦格纳、杜拉丝和阿尔特弥斯的结局一样，也不从得知。其实这两件事对于阿瓦登来说没什么本质性的区别，所以它们也可以看做是同一个谜。 其实他最担心的，是阿尔特弥斯。每次想到这个名字，阿瓦登就难以抑制心中的郁闷。她究竟会怎么样，彻底被屏蔽掉吗？如果是那样，那么她在这世界上遗留下来的唯一痕迹，就是一个程序员记忆里的假名而已了。说话会消失后三个星期，仍旧风平浪静，没有任何人来找过阿瓦登，他也没收到过任何类似内容的EMAIL，阿瓦登一直在想，也许是他们没有吐露出自己的下落，也可能是因为他们根本不知道——他们认识的只是一个叫王二的程序员。这个城市里有数以千计的程序员，而王二是个假名。因此，生活一如既往地平静。不，确切地说，还是有一点不同的，那就是互联网络健康词汇列表：那上面的词组消失的速度比以前要快的快，每小时每分钟都有词与单字飞快地在名单上消失，阿瓦登不得不花上大量时间去更新列表，以跟紧当前形势。与词汇列表更新速度相对的，EMAIL和网络论坛上的东西越来越乏味。因为人们不得不用极有限的词去表达广泛的意思，大家都变得寡言少语。就连那些秘密的暗语和联系方式也少了许多；整个网络就象是前些天阿瓦登家里出了问题的暖气片一样：虽然名义上是给人带来温暖的东西，但却变的冰冷、僵硬，让人如坠冰窟。 这一天，阿瓦登从电脑前抬起头来，他看了看窗外迷茫的灰色天空，胸口一阵抽搐，不由得痛苦地咳了一声。他拿起塑料杯，将杯子里的纯净水一饮而尽，杯子丢进同样是塑料质地的垃圾桶里，发出钝钝的撞击声，他觉得自己的脑子也是一团垃圾，举起手敲了敲，果然发出同样钝钝的撞击声。然后他拿起大衣，戴上墨绿色的护目镜，走出门去。阿瓦登没带便携式的旁观者，那东西已经不需要了，城市里到处都是主动式的旁观者，随时监听是否有违禁词汇的存在。整个纽约现在就象是互联网络一样，被有关部门营造成十分健康。阿瓦登这一次外出是有正当理由的，他决定去取消网络论坛服务，这服务已经用不着了，因为无论EMAIL，新闻组，BBS论坛还是其他什么现在全部都变成了一样的东西。 从日历来说现在应该是春季，但外面还是很冷，高大的灰色建筑矗立在平地上，仿佛绝对零度下的石林。大团大团的风裹着黄沙与废气穿行其间，风沙无处不在，让人置身其中而难以摆脱。阿瓦登把手揣进兜里，脖子缩进领口，畏缩着向网络部的大楼走去。忽然，他停下了脚步，惊讶地站在原地无法动弹。他看到阿尔特弥斯正站在前面的路灯下，穿着黑色的制服。可是她的变化有多么大啊，面容象是老了十岁，满脸都是衰老的皱纹，年轻的活力荡然无存；她听到脚步声，转过头来，两个乌黑的大眼睛显得异常空洞，目光越过阿瓦登延伸到远方，没有一个明晰的焦点。阿瓦登万万没有想到会在这个时间这个地点碰到她，这让他已经沉寂已久的心灵泛起了几点火花，可惜他迟钝的神经已经无法表达出“激动”这一个简单的情感了。两个人互相对视了一阵，他终于木然走到她身边，张了张嘴唇，想对她说些什么。但是他掏出今天新发布的健康词汇列表，发现上面是一片空白——终于连最后一个词组也被有关部门屏蔽了。 于是阿瓦登只好保持着沉默，默默地与面无表情的她擦肩而过，继续向前走去。他的身影逐渐融入同样安静的灰色人群之中，整个城市都显得寂静极了。"},{"title":"博客功能测试（请忽略此文）","date":"2021-05-20T12:28:39.000Z","url":"/2021/05/20/test/","categories":[["undefined",""]],"content":"各种测试 提示信息样例：成功啦o(￣▽￣)ブ 有危险Σ(っ °Д °;)っ 有消息(・∀・(・∀・(・∀・*) 当心哦≧ ﹏ ≦ 折叠内容样例： 折叠框的标题 被折叠的内容 1被折叠的内容 2 展开预先折叠内容样例： 折叠框的标题 被折叠的内容 1被折叠的内容 2 提示面板框样例： 面板框的标题 successs是面板框的类型，可以是： successsdangerinfowarning 图片测试样例： 视频测试样例：关于视频格式的解释： 链接测试样例：More info: example 代码测试样例： quote样例： 1.普通Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque hendrerit lacus ut purus iaculis feugiat. Sed nec tempor elit, quis aliquam neque. Curabitur sed diam eget dolor fermentum semper at eu lorem. 2.书中引用Do not just seek happiness for yourself. Seek happiness for all. Through kindness. Through mercy. David LevithanWide Awake3.twitter引用NEW: DevDocs now comes with syntax highlighting.  @DevDocstwitter.com/devdocs/status/3560951920859627524.网络文章引用Every interaction is both precious and an opportunity to delight. Seth GodinWelcome to Island Marketing 表格测试样例： Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 BP算法 训练集 设 \\begin{array}{l}{\\text { Set } a^{(1)}=x^{(i)}} \\\\ {\\text { Perform forward propagation to compute } a^{(l)} \\text { for } l=2,3, \\ldots, L} \\\\ {\\text { Using } y^{(i)}, \\text { compute } \\delta^{(L)}=a^{(L)}-y^{(i)}} \\\\ {\\text { Compute } \\delta^{(L-1)}, \\delta^{(l+1)}, \\ldots, \\delta^{(2)}} \\\\ {\\Delta_{i j}^{(l)} :=\\Delta_{i j}^{(l)}+a_{j}^{(l)} \\delta_{i}^{(l+1)}}\\end{array} 其中 附录 Python常用命令创建虚拟环境python创建虚拟环境 设置国内源 使用requirement.txt批量安装pip安装 requirments.txt 对项目生成requirement依赖，以便于其他人对项目的forkfreeze-适用于虚拟环境 检查cuda版本（cmd命令） 常用Python代码python消除警告 plt绘图中文乱码 使用pip安装notebook 更改juypter打开默认位置 "}]